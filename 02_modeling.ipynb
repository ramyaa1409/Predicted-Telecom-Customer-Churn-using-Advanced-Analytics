{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Red Kurti, 2024. All rights reserved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELING\n",
    "\n",
    "This notebook is part of the final project for the course STAT206: Statistical Computing at UCR, under the supervision of Professor Alfonso Landeros and TA Noe Vidales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia Version 1.10.0\n",
      "Commit 3120989f39b (2023-12-25 18:01 UTC)\n",
      "Build Info:\n",
      "  Official https://julialang.org/ release\n",
      "Platform Info:\n",
      "  OS: macOS (arm64-apple-darwin22.4.0)\n",
      "  CPU: 10 × Apple M1 Pro\n",
      "  WORD_SIZE: 64\n",
      "  LIBM: libopenlibm\n",
      "  LLVM: libLLVM-15.0.7 (ORCJIT, apple-m1)\n",
      "  Threads: 1 on 8 virtual cores\n",
      "Environment:\n",
      "  JULIA_NUM_THREADS = \n"
     ]
    }
   ],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Library/CloudStorage/OneDrive-Personal/UCR/academics/winter_24/STAT206/project`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mStatus\u001b[22m\u001b[39m `~/Library/CloudStorage/OneDrive-Personal/UCR/academics/winter_24/STAT206/project/Project.toml`\n",
      "  \u001b[90m[324d7699] \u001b[39mCategoricalArrays v0.10.8\n",
      "  \u001b[90m[7806a523] \u001b[39mDecisionTree v0.12.4\n",
      "  \u001b[90m[31c24e10] \u001b[39mDistributions v0.25.107\n",
      "  \u001b[90m[89b67f3b] \u001b[39mExcelFiles v1.0.0\n",
      "  \u001b[90m[da1fdf0e] \u001b[39mFreqTables v0.4.6\n",
      "  \u001b[90m[38e38edf] \u001b[39mGLM v1.9.0\n",
      "  \u001b[90m[09f84164] \u001b[39mHypothesisTests v0.11.0\n",
      "  \u001b[90m[c709b415] \u001b[39mImbalance v0.1.5\n",
      "  \u001b[90m[add582a8] \u001b[39mMLJ v0.20.3\n",
      "  \u001b[90m[a7f614a8] \u001b[39mMLJBase v1.1.2\n",
      "  \u001b[90m[c6f25543] \u001b[39mMLJDecisionTreeInterface v0.4.1\n",
      "  \u001b[90m[caf8df21] \u001b[39mMLJGLMInterface v0.3.7\n",
      "  \u001b[90m[6ee0df7b] \u001b[39mMLJLinearModels v0.10.0\n",
      "  \u001b[90m[d491faf4] \u001b[39mMLJModels v0.16.16\n",
      "  \u001b[90m[5ae90465] \u001b[39mMLJScikitLearnInterface v0.6.1\n",
      "  \u001b[90m[03970b2e] \u001b[39mMLJTuning v0.8.2\n",
      "  \u001b[90m[54119dfa] \u001b[39mMLJXGBoostInterface v0.3.10\n",
      "  \u001b[90m[636a865e] \u001b[39mNearestNeighborModels v0.2.3\n",
      "  \u001b[90m[91a5bcdd] \u001b[39mPlots v1.40.2\n",
      "  \u001b[90m[ce6b1742] \u001b[39mRDatasets v0.7.7\n",
      "  \u001b[90m[8e980c4a] \u001b[39mShapefile v0.12.0\n",
      "  \u001b[90m[860ef19b] \u001b[39mStableRNGs v1.0.1\n",
      "  \u001b[90m[2913bbd2] \u001b[39mStatsBase v0.34.2\n",
      "  \u001b[90m[3eaba693] \u001b[39mStatsModels v0.7.3\n",
      "  \u001b[90m[f3b207a7] \u001b[39mStatsPlots v0.15.7\n",
      "  \u001b[90m[009559a3] \u001b[39mXGBoost v2.5.1\n",
      "  \u001b[90m[9a3f8284] \u001b[39mRandom\n"
     ]
    }
   ],
   "source": [
    "import Pkg\n",
    "Pkg.activate(pwd())\n",
    "Pkg.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#c67812ff\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Color theme\n",
    "primary_color = \"#191919ff\"\n",
    "secondary_color = \"#c67812ff\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import MLJXGBoostInterface ✔"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: For silent loading, specify `verbosity=0`. \n",
      "└ @ Main /Users/redx/.julia/packages/MLJModels/55VdT/src/loading.jl:159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLJXGBoostInterface.XGBoostClassifier"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data\n",
    "using CSV, DataFrames, CategoricalArrays\n",
    "# Statistics\n",
    "using StableRNGs\n",
    "# Visualization\n",
    "using Plots\n",
    "# ML Base\n",
    "using MLJ, MLJModels, MLJBase, MLJTuning\n",
    "# ML Linear\n",
    "using MLJLinearModels\n",
    "# ML Nearest Neighbors\n",
    "using NearestNeighborModels\n",
    "# ML Random Forest\n",
    "using MLJDecisionTreeInterface\n",
    "\n",
    "# Load the LogisticClassifier model from the MLJLinearModels package\n",
    "@load LogisticClassifier pkg = MLJLinearModels verbosity = 0\n",
    "@load KNNClassifier pkg=NearestNeighborModels verbosity=0\n",
    "XGBoostClassifier = @load XGBoostClassifier pkg=XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Standardized data, class imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────────┬────────────┬────────────────┬──────────────┬────────────────────┬─────────────────────┬─────────────┬──────────────┬────────────────┬─────────────────┬───────────────┬───────────────────┬────────────────────┬────────────────────┬──────────────────────────────────┬─────────────────────┬───────────────────────┬───────────────────────────────┬──────────────────────┬───────────────────────┬────────────────────────────────────────┬────────────────────────┬──────────────────┬───────────────────────────────────┬───────────────────┬──────────────────────────┬────────────────────┬────────────────────┬───────────────────────┬────────────────────────┬───────────────────────────────────────────┬─────────────────────────────────────────┬──────────────────────────────────┬──────────────────────────────┬─────────────────┬───────────────┬─────────────┬────────────┐\n",
      "│\u001b[1m latitude   \u001b[0m│\u001b[1m longitude  \u001b[0m│\u001b[1m gender__Female \u001b[0m│\u001b[1m gender__Male \u001b[0m│\u001b[1m senior_citizen__No \u001b[0m│\u001b[1m senior_citizen__Yes \u001b[0m│\u001b[1m partner__No \u001b[0m│\u001b[1m partner__Yes \u001b[0m│\u001b[1m dependents__No \u001b[0m│\u001b[1m dependents__Yes \u001b[0m│\u001b[1m tenure_months \u001b[0m│\u001b[1m phone_service__No \u001b[0m│\u001b[1m phone_service__Yes \u001b[0m│\u001b[1m multiple_lines__No \u001b[0m│\u001b[1m multiple_lines__No phone service \u001b[0m│\u001b[1m multiple_lines__Yes \u001b[0m│\u001b[1m internet_service__DSL \u001b[0m│\u001b[1m internet_service__Fiber optic \u001b[0m│\u001b[1m internet_service__No \u001b[0m│\u001b[1m device_protection__No \u001b[0m│\u001b[1m device_protection__No internet service \u001b[0m│\u001b[1m device_protection__Yes \u001b[0m│\u001b[1m tech_support__No \u001b[0m│\u001b[1m tech_support__No internet service \u001b[0m│\u001b[1m tech_support__Yes \u001b[0m│\u001b[1m contract__Month-to-month \u001b[0m│\u001b[1m contract__One year \u001b[0m│\u001b[1m contract__Two year \u001b[0m│\u001b[1m paperless_billing__No \u001b[0m│\u001b[1m paperless_billing__Yes \u001b[0m│\u001b[1m payment_method__Bank transfer (automatic) \u001b[0m│\u001b[1m payment_method__Credit card (automatic) \u001b[0m│\u001b[1m payment_method__Electronic check \u001b[0m│\u001b[1m payment_method__Mailed check \u001b[0m│\u001b[1m monthly_charges \u001b[0m│\u001b[1m total_charges \u001b[0m│\u001b[1m churn_value \u001b[0m│\u001b[1m cltv       \u001b[0m│\n",
      "│\u001b[90m Float64    \u001b[0m│\u001b[90m Float64    \u001b[0m│\u001b[90m Float64        \u001b[0m│\u001b[90m Float64      \u001b[0m│\u001b[90m Float64            \u001b[0m│\u001b[90m Float64             \u001b[0m│\u001b[90m Float64     \u001b[0m│\u001b[90m Float64      \u001b[0m│\u001b[90m Float64        \u001b[0m│\u001b[90m Float64         \u001b[0m│\u001b[90m Float64       \u001b[0m│\u001b[90m Float64           \u001b[0m│\u001b[90m Float64            \u001b[0m│\u001b[90m Float64            \u001b[0m│\u001b[90m Float64                          \u001b[0m│\u001b[90m Float64             \u001b[0m│\u001b[90m Float64               \u001b[0m│\u001b[90m Float64                       \u001b[0m│\u001b[90m Float64              \u001b[0m│\u001b[90m Float64               \u001b[0m│\u001b[90m Float64                                \u001b[0m│\u001b[90m Float64                \u001b[0m│\u001b[90m Float64          \u001b[0m│\u001b[90m Float64                           \u001b[0m│\u001b[90m Float64           \u001b[0m│\u001b[90m Float64                  \u001b[0m│\u001b[90m Float64            \u001b[0m│\u001b[90m Float64            \u001b[0m│\u001b[90m Float64               \u001b[0m│\u001b[90m Float64                \u001b[0m│\u001b[90m Float64                                   \u001b[0m│\u001b[90m Float64                                 \u001b[0m│\u001b[90m Float64                          \u001b[0m│\u001b[90m Float64                      \u001b[0m│\u001b[90m Float64         \u001b[0m│\u001b[90m Float64       \u001b[0m│\u001b[90m Int64       \u001b[0m│\u001b[90m Float64    \u001b[0m│\n",
      "│\u001b[90m Continuous \u001b[0m│\u001b[90m Continuous \u001b[0m│\u001b[90m Continuous     \u001b[0m│\u001b[90m Continuous   \u001b[0m│\u001b[90m Continuous         \u001b[0m│\u001b[90m Continuous          \u001b[0m│\u001b[90m Continuous  \u001b[0m│\u001b[90m Continuous   \u001b[0m│\u001b[90m Continuous     \u001b[0m│\u001b[90m Continuous      \u001b[0m│\u001b[90m Continuous    \u001b[0m│\u001b[90m Continuous        \u001b[0m│\u001b[90m Continuous         \u001b[0m│\u001b[90m Continuous         \u001b[0m│\u001b[90m Continuous                       \u001b[0m│\u001b[90m Continuous          \u001b[0m│\u001b[90m Continuous            \u001b[0m│\u001b[90m Continuous                    \u001b[0m│\u001b[90m Continuous           \u001b[0m│\u001b[90m Continuous            \u001b[0m│\u001b[90m Continuous                             \u001b[0m│\u001b[90m Continuous             \u001b[0m│\u001b[90m Continuous       \u001b[0m│\u001b[90m Continuous                        \u001b[0m│\u001b[90m Continuous        \u001b[0m│\u001b[90m Continuous               \u001b[0m│\u001b[90m Continuous         \u001b[0m│\u001b[90m Continuous         \u001b[0m│\u001b[90m Continuous            \u001b[0m│\u001b[90m Continuous             \u001b[0m│\u001b[90m Continuous                                \u001b[0m│\u001b[90m Continuous                              \u001b[0m│\u001b[90m Continuous                       \u001b[0m│\u001b[90m Continuous                   \u001b[0m│\u001b[90m Continuous      \u001b[0m│\u001b[90m Continuous    \u001b[0m│\u001b[90m Count       \u001b[0m│\u001b[90m Continuous \u001b[0m│\n",
      "├────────────┼────────────┼────────────────┼──────────────┼────────────────────┼─────────────────────┼─────────────┼──────────────┼────────────────┼─────────────────┼───────────────┼───────────────────┼────────────────────┼────────────────────┼──────────────────────────────────┼─────────────────────┼───────────────────────┼───────────────────────────────┼──────────────────────┼───────────────────────┼────────────────────────────────────────┼────────────────────────┼──────────────────┼───────────────────────────────────┼───────────────────┼──────────────────────────┼────────────────────┼────────────────────┼───────────────────────┼────────────────────────┼───────────────────────────────────────────┼─────────────────────────────────────────┼──────────────────────────────────┼──────────────────────────────┼─────────────────┼───────────────┼─────────────┼────────────┤\n",
      "│ -0.944244  │ 0.707472   │ 0.0            │ 1.0          │ 1.0                │ 0.0                 │ 1.0         │ 0.0          │ 1.0            │ 0.0             │ -1.23942      │ 0.0               │ 1.0                │ 1.0                │ 0.0                              │ 0.0                 │ 1.0                   │ 0.0                           │ 0.0                  │ 1.0                   │ 0.0                                    │ 0.0                    │ 1.0              │ 0.0                               │ 0.0               │ 1.0                      │ 0.0                │ 0.0                │ 0.0                   │ 1.0                    │ 0.0                                       │ 0.0                                     │ 0.0                              │ 1.0                          │ -0.363897       │ -0.959581     │ 1.0         │ -0.983112  │\n",
      "│ -0.905504  │ 0.691418   │ 1.0            │ 0.0          │ 1.0                │ 0.0                 │ 1.0         │ 0.0          │ 0.0            │ 1.0             │ -1.23942      │ 0.0               │ 1.0                │ 1.0                │ 0.0                              │ 0.0                 │ 0.0                   │ 1.0                           │ 0.0                  │ 1.0                   │ 0.0                                    │ 0.0                    │ 1.0              │ 0.0                               │ 0.0               │ 1.0                      │ 0.0                │ 0.0                │ 0.0                   │ 1.0                    │ 0.0                                       │ 0.0                                     │ 1.0                              │ 0.0                          │ 0.196164        │ -0.940391     │ 1.0         │ -1.43811   │\n",
      "│ -0.910092  │ 0.69766    │ 1.0            │ 0.0          │ 1.0                │ 0.0                 │ 1.0         │ 0.0          │ 0.0            │ 1.0             │ -0.99497      │ 0.0               │ 1.0                │ 0.0                │ 0.0                              │ 1.0                 │ 0.0                   │ 1.0                           │ 0.0                  │ 0.0                   │ 0.0                                    │ 1.0                    │ 1.0              │ 0.0                               │ 0.0               │ 1.0                      │ 0.0                │ 0.0                │ 0.0                   │ 1.0                    │ 0.0                                       │ 0.0                                     │ 1.0                              │ 0.0                          │ 1.15841         │ -0.645323     │ 1.0         │ 0.820825   │\n",
      "│ -0.904346  │ 0.687576   │ 1.0            │ 0.0          │ 1.0                │ 0.0                 │ 0.0         │ 1.0          │ 0.0            │ 1.0             │ -0.180148     │ 0.0               │ 1.0                │ 0.0                │ 0.0                              │ 1.0                 │ 0.0                   │ 1.0                           │ 0.0                  │ 0.0                   │ 0.0                                    │ 1.0                    │ 0.0              │ 0.0                               │ 1.0               │ 1.0                      │ 0.0                │ 0.0                │ 0.0                   │ 1.0                    │ 0.0                                       │ 0.0                                     │ 1.0                              │ 0.0                          │ 1.32958         │ 0.336492      │ 1.0         │ 0.508751   │\n",
      "│ -0.913671  │ 0.71048    │ 0.0            │ 1.0          │ 1.0                │ 0.0                 │ 1.0         │ 0.0          │ 0.0            │ 1.0             │ 0.675414      │ 0.0               │ 1.0                │ 0.0                │ 0.0                              │ 1.0                 │ 0.0                   │ 1.0                           │ 0.0                  │ 0.0                   │ 0.0                                    │ 1.0                    │ 1.0              │ 0.0                               │ 0.0               │ 1.0                      │ 0.0                │ 0.0                │ 0.0                   │ 1.0                    │ 1.0                                       │ 0.0                                     │ 0.0                              │ 0.0                          │ 1.29302         │ 1.2145        │ 1.0         │ 0.793761   │\n",
      "└────────────┴────────────┴────────────────┴──────────────┴────────────────────┴─────────────────────┴─────────────┴──────────────┴────────────────┴─────────────────┴───────────────┴───────────────────┴────────────────────┴────────────────────┴──────────────────────────────────┴─────────────────────┴───────────────────────┴───────────────────────────────┴──────────────────────┴───────────────────────┴────────────────────────────────────────┴────────────────────────┴──────────────────┴───────────────────────────────────┴───────────────────┴──────────────────────────┴────────────────────┴────────────────────┴───────────────────────┴────────────────────────┴───────────────────────────────────────────┴─────────────────────────────────────────┴──────────────────────────────────┴──────────────────────────────┴─────────────────┴───────────────┴─────────────┴────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Load the the data csv file from the /data directory\n",
    "telco = DataFrame(CSV.File(\"data/telco_standardized.csv\"))\n",
    "\n",
    "# Peek at the first 5 rows of the dataframe\n",
    "first(telco, 5) |> pretty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Standardized data, class balancing using undersampling of the majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────────┬────────────┬────────────────┬──────────────┬────────────────────┬─────────────────────┬─────────────┬──────────────┬────────────────┬─────────────────┬───────────────┬───────────────────┬────────────────────┬────────────────────┬──────────────────────────────────┬─────────────────────┬───────────────────────┬───────────────────────────────┬──────────────────────┬───────────────────────┬────────────────────────────────────────┬────────────────────────┬──────────────────┬───────────────────────────────────┬───────────────────┬──────────────────────────┬────────────────────┬────────────────────┬───────────────────────┬────────────────────────┬───────────────────────────────────────────┬─────────────────────────────────────────┬──────────────────────────────────┬──────────────────────────────┬─────────────────┬───────────────┬─────────────┬────────────┐\n",
      "│\u001b[1m latitude   \u001b[0m│\u001b[1m longitude  \u001b[0m│\u001b[1m gender__Female \u001b[0m│\u001b[1m gender__Male \u001b[0m│\u001b[1m senior_citizen__No \u001b[0m│\u001b[1m senior_citizen__Yes \u001b[0m│\u001b[1m partner__No \u001b[0m│\u001b[1m partner__Yes \u001b[0m│\u001b[1m dependents__No \u001b[0m│\u001b[1m dependents__Yes \u001b[0m│\u001b[1m tenure_months \u001b[0m│\u001b[1m phone_service__No \u001b[0m│\u001b[1m phone_service__Yes \u001b[0m│\u001b[1m multiple_lines__No \u001b[0m│\u001b[1m multiple_lines__No phone service \u001b[0m│\u001b[1m multiple_lines__Yes \u001b[0m│\u001b[1m internet_service__DSL \u001b[0m│\u001b[1m internet_service__Fiber optic \u001b[0m│\u001b[1m internet_service__No \u001b[0m│\u001b[1m device_protection__No \u001b[0m│\u001b[1m device_protection__No internet service \u001b[0m│\u001b[1m device_protection__Yes \u001b[0m│\u001b[1m tech_support__No \u001b[0m│\u001b[1m tech_support__No internet service \u001b[0m│\u001b[1m tech_support__Yes \u001b[0m│\u001b[1m contract__Month-to-month \u001b[0m│\u001b[1m contract__One year \u001b[0m│\u001b[1m contract__Two year \u001b[0m│\u001b[1m paperless_billing__No \u001b[0m│\u001b[1m paperless_billing__Yes \u001b[0m│\u001b[1m payment_method__Bank transfer (automatic) \u001b[0m│\u001b[1m payment_method__Credit card (automatic) \u001b[0m│\u001b[1m payment_method__Electronic check \u001b[0m│\u001b[1m payment_method__Mailed check \u001b[0m│\u001b[1m monthly_charges \u001b[0m│\u001b[1m total_charges \u001b[0m│\u001b[1m churn_value \u001b[0m│\u001b[1m cltv       \u001b[0m│\n",
      "│\u001b[90m Float64    \u001b[0m│\u001b[90m Float64    \u001b[0m│\u001b[90m Float64        \u001b[0m│\u001b[90m Float64      \u001b[0m│\u001b[90m Float64            \u001b[0m│\u001b[90m Float64             \u001b[0m│\u001b[90m Float64     \u001b[0m│\u001b[90m Float64      \u001b[0m│\u001b[90m Float64        \u001b[0m│\u001b[90m Float64         \u001b[0m│\u001b[90m Float64       \u001b[0m│\u001b[90m Float64           \u001b[0m│\u001b[90m Float64            \u001b[0m│\u001b[90m Float64            \u001b[0m│\u001b[90m Float64                          \u001b[0m│\u001b[90m Float64             \u001b[0m│\u001b[90m Float64               \u001b[0m│\u001b[90m Float64                       \u001b[0m│\u001b[90m Float64              \u001b[0m│\u001b[90m Float64               \u001b[0m│\u001b[90m Float64                                \u001b[0m│\u001b[90m Float64                \u001b[0m│\u001b[90m Float64          \u001b[0m│\u001b[90m Float64                           \u001b[0m│\u001b[90m Float64           \u001b[0m│\u001b[90m Float64                  \u001b[0m│\u001b[90m Float64            \u001b[0m│\u001b[90m Float64            \u001b[0m│\u001b[90m Float64               \u001b[0m│\u001b[90m Float64                \u001b[0m│\u001b[90m Float64                                   \u001b[0m│\u001b[90m Float64                                 \u001b[0m│\u001b[90m Float64                          \u001b[0m│\u001b[90m Float64                      \u001b[0m│\u001b[90m Float64         \u001b[0m│\u001b[90m Float64       \u001b[0m│\u001b[90m Int64       \u001b[0m│\u001b[90m Float64    \u001b[0m│\n",
      "│\u001b[90m Continuous \u001b[0m│\u001b[90m Continuous \u001b[0m│\u001b[90m Continuous     \u001b[0m│\u001b[90m Continuous   \u001b[0m│\u001b[90m Continuous         \u001b[0m│\u001b[90m Continuous          \u001b[0m│\u001b[90m Continuous  \u001b[0m│\u001b[90m Continuous   \u001b[0m│\u001b[90m Continuous     \u001b[0m│\u001b[90m Continuous      \u001b[0m│\u001b[90m Continuous    \u001b[0m│\u001b[90m Continuous        \u001b[0m│\u001b[90m Continuous         \u001b[0m│\u001b[90m Continuous         \u001b[0m│\u001b[90m Continuous                       \u001b[0m│\u001b[90m Continuous          \u001b[0m│\u001b[90m Continuous            \u001b[0m│\u001b[90m Continuous                    \u001b[0m│\u001b[90m Continuous           \u001b[0m│\u001b[90m Continuous            \u001b[0m│\u001b[90m Continuous                             \u001b[0m│\u001b[90m Continuous             \u001b[0m│\u001b[90m Continuous       \u001b[0m│\u001b[90m Continuous                        \u001b[0m│\u001b[90m Continuous        \u001b[0m│\u001b[90m Continuous               \u001b[0m│\u001b[90m Continuous         \u001b[0m│\u001b[90m Continuous         \u001b[0m│\u001b[90m Continuous            \u001b[0m│\u001b[90m Continuous             \u001b[0m│\u001b[90m Continuous                                \u001b[0m│\u001b[90m Continuous                              \u001b[0m│\u001b[90m Continuous                       \u001b[0m│\u001b[90m Continuous                   \u001b[0m│\u001b[90m Continuous      \u001b[0m│\u001b[90m Continuous    \u001b[0m│\u001b[90m Count       \u001b[0m│\u001b[90m Continuous \u001b[0m│\n",
      "├────────────┼────────────┼────────────────┼──────────────┼────────────────────┼─────────────────────┼─────────────┼──────────────┼────────────────┼─────────────────┼───────────────┼───────────────────┼────────────────────┼────────────────────┼──────────────────────────────────┼─────────────────────┼───────────────────────┼───────────────────────────────┼──────────────────────┼───────────────────────┼────────────────────────────────────────┼────────────────────────┼──────────────────┼───────────────────────────────────┼───────────────────┼──────────────────────────┼────────────────────┼────────────────────┼───────────────────────┼────────────────────────┼───────────────────────────────────────────┼─────────────────────────────────────────┼──────────────────────────────────┼──────────────────────────────┼─────────────────┼───────────────┼─────────────┼────────────┤\n",
      "│ -0.749927  │ 0.13258    │ 1.0            │ 0.0          │ 1.0                │ 0.0                 │ 1.0         │ 0.0          │ 1.0            │ 0.0             │ -1.07645      │ 0.0               │ 1.0                │ 0.0                │ 0.0                              │ 1.0                 │ 0.0                   │ 1.0                           │ 0.0                  │ 1.0                   │ 0.0                                    │ 0.0                    │ 1.0              │ 0.0                               │ 0.0               │ 1.0                      │ 0.0                │ 0.0                │ 0.0                   │ 1.0                    │ 1.0                                       │ 0.0                                     │ 0.0                              │ 0.0                          │ 0.955654        │ -0.770656     │ 1.0         │ 1.02803    │\n",
      "│ -0.858596  │ 0.893839   │ 0.0            │ 1.0          │ 1.0                │ 0.0                 │ 1.0         │ 0.0          │ 0.0            │ 1.0             │ 1.36801       │ 0.0               │ 1.0                │ 0.0                │ 0.0                              │ 1.0                 │ 1.0                   │ 0.0                           │ 0.0                  │ 1.0                   │ 0.0                                    │ 0.0                    │ 0.0              │ 0.0                               │ 1.0               │ 0.0                      │ 0.0                │ 1.0                │ 1.0                   │ 0.0                    │ 1.0                                       │ 0.0                                     │ 0.0                              │ 0.0                          │ 0.0299738       │ 0.924487      │ 0.0         │ 0.555266   │\n",
      "│ -0.934305  │ 0.679061   │ 0.0            │ 1.0          │ 1.0                │ 0.0                 │ 1.0         │ 0.0          │ 1.0            │ 0.0             │ -0.913487     │ 0.0               │ 1.0                │ 0.0                │ 0.0                              │ 1.0                 │ 0.0                   │ 1.0                           │ 0.0                  │ 1.0                   │ 0.0                                    │ 0.0                    │ 1.0              │ 0.0                               │ 0.0               │ 1.0                      │ 0.0                │ 0.0                │ 0.0                   │ 1.0                    │ 0.0                                       │ 0.0                                     │ 0.0                              │ 1.0                          │ 0.310836        │ -0.649161     │ 1.0         │ -0.14415   │\n",
      "│ -0.859582  │ 0.395327   │ 1.0            │ 0.0          │ 1.0                │ 0.0                 │ 0.0         │ 1.0          │ 1.0            │ 0.0             │ 1.04208       │ 0.0               │ 1.0                │ 0.0                │ 0.0                              │ 1.0                 │ 0.0                   │ 1.0                           │ 0.0                  │ 0.0                   │ 0.0                                    │ 1.0                    │ 1.0              │ 0.0                               │ 0.0               │ 0.0                      │ 1.0                │ 0.0                │ 0.0                   │ 1.0                    │ 0.0                                       │ 0.0                                     │ 1.0                              │ 0.0                          │ 0.980583        │ 1.46766       │ 0.0         │ 1.49233    │\n",
      "│ -1.33605   │ 1.1752     │ 1.0            │ 0.0          │ 1.0                │ 0.0                 │ 0.0         │ 1.0          │ 0.0            │ 1.0             │ 0.756896      │ 0.0               │ 1.0                │ 0.0                │ 0.0                              │ 1.0                 │ 0.0                   │ 1.0                           │ 0.0                  │ 0.0                   │ 0.0                                    │ 1.0                    │ 1.0              │ 0.0                               │ 0.0               │ 0.0                      │ 1.0                │ 0.0                │ 0.0                   │ 1.0                    │ 0.0                                       │ 0.0                                     │ 1.0                              │ 0.0                          │ 0.842645        │ 1.00211       │ 0.0         │ 1.09146    │\n",
      "└────────────┴────────────┴────────────────┴──────────────┴────────────────────┴─────────────────────┴─────────────┴──────────────┴────────────────┴─────────────────┴───────────────┴───────────────────┴────────────────────┴────────────────────┴──────────────────────────────────┴─────────────────────┴───────────────────────┴───────────────────────────────┴──────────────────────┴───────────────────────┴────────────────────────────────────────┴────────────────────────┴──────────────────┴───────────────────────────────────┴───────────────────┴──────────────────────────┴────────────────────┴────────────────────┴───────────────────────┴────────────────────────┴───────────────────────────────────────────┴─────────────────────────────────────────┴──────────────────────────────────┴──────────────────────────────┴─────────────────┴───────────────┴─────────────┴────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Load the the data csv file from the /data directory\n",
    "telco_balanced_undersampling = DataFrame(CSV.File(\"data/telco_balanced_undersampling.csv\"))\n",
    "\n",
    "# Peek at the first 5 rows of the dataframe\n",
    "first(telco_balanced_undersampling, 5) |> pretty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Standardized data, class balancing using oversampling of the minority class (SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────────┬────────────┬────────────────┬──────────────┬────────────────────┬─────────────────────┬─────────────┬──────────────┬────────────────┬─────────────────┬───────────────┬───────────────────┬────────────────────┬────────────────────┬──────────────────────────────────┬─────────────────────┬───────────────────────┬───────────────────────────────┬──────────────────────┬───────────────────────┬────────────────────────────────────────┬────────────────────────┬──────────────────┬───────────────────────────────────┬───────────────────┬──────────────────────────┬────────────────────┬────────────────────┬───────────────────────┬────────────────────────┬───────────────────────────────────────────┬─────────────────────────────────────────┬──────────────────────────────────┬──────────────────────────────┬─────────────────┬───────────────┬────────────┬─────────────┐\n",
      "│\u001b[1m latitude   \u001b[0m│\u001b[1m longitude  \u001b[0m│\u001b[1m gender__Female \u001b[0m│\u001b[1m gender__Male \u001b[0m│\u001b[1m senior_citizen__No \u001b[0m│\u001b[1m senior_citizen__Yes \u001b[0m│\u001b[1m partner__No \u001b[0m│\u001b[1m partner__Yes \u001b[0m│\u001b[1m dependents__No \u001b[0m│\u001b[1m dependents__Yes \u001b[0m│\u001b[1m tenure_months \u001b[0m│\u001b[1m phone_service__No \u001b[0m│\u001b[1m phone_service__Yes \u001b[0m│\u001b[1m multiple_lines__No \u001b[0m│\u001b[1m multiple_lines__No phone service \u001b[0m│\u001b[1m multiple_lines__Yes \u001b[0m│\u001b[1m internet_service__DSL \u001b[0m│\u001b[1m internet_service__Fiber optic \u001b[0m│\u001b[1m internet_service__No \u001b[0m│\u001b[1m device_protection__No \u001b[0m│\u001b[1m device_protection__No internet service \u001b[0m│\u001b[1m device_protection__Yes \u001b[0m│\u001b[1m tech_support__No \u001b[0m│\u001b[1m tech_support__No internet service \u001b[0m│\u001b[1m tech_support__Yes \u001b[0m│\u001b[1m contract__Month-to-month \u001b[0m│\u001b[1m contract__One year \u001b[0m│\u001b[1m contract__Two year \u001b[0m│\u001b[1m paperless_billing__No \u001b[0m│\u001b[1m paperless_billing__Yes \u001b[0m│\u001b[1m payment_method__Bank transfer (automatic) \u001b[0m│\u001b[1m payment_method__Credit card (automatic) \u001b[0m│\u001b[1m payment_method__Electronic check \u001b[0m│\u001b[1m payment_method__Mailed check \u001b[0m│\u001b[1m monthly_charges \u001b[0m│\u001b[1m total_charges \u001b[0m│\u001b[1m cltv       \u001b[0m│\u001b[1m churn_value \u001b[0m│\n",
      "│\u001b[90m Float64    \u001b[0m│\u001b[90m Float64    \u001b[0m│\u001b[90m Float64        \u001b[0m│\u001b[90m Float64      \u001b[0m│\u001b[90m Float64            \u001b[0m│\u001b[90m Float64             \u001b[0m│\u001b[90m Float64     \u001b[0m│\u001b[90m Float64      \u001b[0m│\u001b[90m Float64        \u001b[0m│\u001b[90m Float64         \u001b[0m│\u001b[90m Float64       \u001b[0m│\u001b[90m Float64           \u001b[0m│\u001b[90m Float64            \u001b[0m│\u001b[90m Float64            \u001b[0m│\u001b[90m Float64                          \u001b[0m│\u001b[90m Float64             \u001b[0m│\u001b[90m Float64               \u001b[0m│\u001b[90m Float64                       \u001b[0m│\u001b[90m Float64              \u001b[0m│\u001b[90m Float64               \u001b[0m│\u001b[90m Float64                                \u001b[0m│\u001b[90m Float64                \u001b[0m│\u001b[90m Float64          \u001b[0m│\u001b[90m Float64                           \u001b[0m│\u001b[90m Float64           \u001b[0m│\u001b[90m Float64                  \u001b[0m│\u001b[90m Float64            \u001b[0m│\u001b[90m Float64            \u001b[0m│\u001b[90m Float64               \u001b[0m│\u001b[90m Float64                \u001b[0m│\u001b[90m Float64                                   \u001b[0m│\u001b[90m Float64                                 \u001b[0m│\u001b[90m Float64                          \u001b[0m│\u001b[90m Float64                      \u001b[0m│\u001b[90m Float64         \u001b[0m│\u001b[90m Float64       \u001b[0m│\u001b[90m Float64    \u001b[0m│\u001b[90m Int64       \u001b[0m│\n",
      "│\u001b[90m Continuous \u001b[0m│\u001b[90m Continuous \u001b[0m│\u001b[90m Continuous     \u001b[0m│\u001b[90m Continuous   \u001b[0m│\u001b[90m Continuous         \u001b[0m│\u001b[90m Continuous          \u001b[0m│\u001b[90m Continuous  \u001b[0m│\u001b[90m Continuous   \u001b[0m│\u001b[90m Continuous     \u001b[0m│\u001b[90m Continuous      \u001b[0m│\u001b[90m Continuous    \u001b[0m│\u001b[90m Continuous        \u001b[0m│\u001b[90m Continuous         \u001b[0m│\u001b[90m Continuous         \u001b[0m│\u001b[90m Continuous                       \u001b[0m│\u001b[90m Continuous          \u001b[0m│\u001b[90m Continuous            \u001b[0m│\u001b[90m Continuous                    \u001b[0m│\u001b[90m Continuous           \u001b[0m│\u001b[90m Continuous            \u001b[0m│\u001b[90m Continuous                             \u001b[0m│\u001b[90m Continuous             \u001b[0m│\u001b[90m Continuous       \u001b[0m│\u001b[90m Continuous                        \u001b[0m│\u001b[90m Continuous        \u001b[0m│\u001b[90m Continuous               \u001b[0m│\u001b[90m Continuous         \u001b[0m│\u001b[90m Continuous         \u001b[0m│\u001b[90m Continuous            \u001b[0m│\u001b[90m Continuous             \u001b[0m│\u001b[90m Continuous                                \u001b[0m│\u001b[90m Continuous                              \u001b[0m│\u001b[90m Continuous                       \u001b[0m│\u001b[90m Continuous                   \u001b[0m│\u001b[90m Continuous      \u001b[0m│\u001b[90m Continuous    \u001b[0m│\u001b[90m Continuous \u001b[0m│\u001b[90m Count       \u001b[0m│\n",
      "├────────────┼────────────┼────────────────┼──────────────┼────────────────────┼─────────────────────┼─────────────┼──────────────┼────────────────┼─────────────────┼───────────────┼───────────────────┼────────────────────┼────────────────────┼──────────────────────────────────┼─────────────────────┼───────────────────────┼───────────────────────────────┼──────────────────────┼───────────────────────┼────────────────────────────────────────┼────────────────────────┼──────────────────┼───────────────────────────────────┼───────────────────┼──────────────────────────┼────────────────────┼────────────────────┼───────────────────────┼────────────────────────┼───────────────────────────────────────────┼─────────────────────────────────────────┼──────────────────────────────────┼──────────────────────────────┼─────────────────┼───────────────┼────────────┼─────────────┤\n",
      "│ -0.944244  │ 0.707472   │ 0.0            │ 1.0          │ 1.0                │ 0.0                 │ 1.0         │ 0.0          │ 1.0            │ 0.0             │ -1.23942      │ 0.0               │ 1.0                │ 1.0                │ 0.0                              │ 0.0                 │ 1.0                   │ 0.0                           │ 0.0                  │ 1.0                   │ 0.0                                    │ 0.0                    │ 1.0              │ 0.0                               │ 0.0               │ 1.0                      │ 0.0                │ 0.0                │ 0.0                   │ 1.0                    │ 0.0                                       │ 0.0                                     │ 0.0                              │ 1.0                          │ -0.363897       │ -0.959581     │ -0.983112  │ 1.0         │\n",
      "│ -0.905504  │ 0.691418   │ 1.0            │ 0.0          │ 1.0                │ 0.0                 │ 1.0         │ 0.0          │ 0.0            │ 1.0             │ -1.23942      │ 0.0               │ 1.0                │ 1.0                │ 0.0                              │ 0.0                 │ 0.0                   │ 1.0                           │ 0.0                  │ 1.0                   │ 0.0                                    │ 0.0                    │ 1.0              │ 0.0                               │ 0.0               │ 1.0                      │ 0.0                │ 0.0                │ 0.0                   │ 1.0                    │ 0.0                                       │ 0.0                                     │ 1.0                              │ 0.0                          │ 0.196164        │ -0.940391     │ -1.43811   │ 1.0         │\n",
      "│ -0.910092  │ 0.69766    │ 1.0            │ 0.0          │ 1.0                │ 0.0                 │ 1.0         │ 0.0          │ 0.0            │ 1.0             │ -0.99497      │ 0.0               │ 1.0                │ 0.0                │ 0.0                              │ 1.0                 │ 0.0                   │ 1.0                           │ 0.0                  │ 0.0                   │ 0.0                                    │ 1.0                    │ 1.0              │ 0.0                               │ 0.0               │ 1.0                      │ 0.0                │ 0.0                │ 0.0                   │ 1.0                    │ 0.0                                       │ 0.0                                     │ 1.0                              │ 0.0                          │ 1.15841         │ -0.645323     │ 0.820825   │ 1.0         │\n",
      "│ -0.904346  │ 0.687576   │ 1.0            │ 0.0          │ 1.0                │ 0.0                 │ 0.0         │ 1.0          │ 0.0            │ 1.0             │ -0.180148     │ 0.0               │ 1.0                │ 0.0                │ 0.0                              │ 1.0                 │ 0.0                   │ 1.0                           │ 0.0                  │ 0.0                   │ 0.0                                    │ 1.0                    │ 0.0              │ 0.0                               │ 1.0               │ 1.0                      │ 0.0                │ 0.0                │ 0.0                   │ 1.0                    │ 0.0                                       │ 0.0                                     │ 1.0                              │ 0.0                          │ 1.32958         │ 0.336492      │ 0.508751   │ 1.0         │\n",
      "│ -0.913671  │ 0.71048    │ 0.0            │ 1.0          │ 1.0                │ 0.0                 │ 1.0         │ 0.0          │ 0.0            │ 1.0             │ 0.675414      │ 0.0               │ 1.0                │ 0.0                │ 0.0                              │ 1.0                 │ 0.0                   │ 1.0                           │ 0.0                  │ 0.0                   │ 0.0                                    │ 1.0                    │ 1.0              │ 0.0                               │ 0.0               │ 1.0                      │ 0.0                │ 0.0                │ 0.0                   │ 1.0                    │ 1.0                                       │ 0.0                                     │ 0.0                              │ 0.0                          │ 1.29302         │ 1.2145        │ 0.793761   │ 1.0         │\n",
      "└────────────┴────────────┴────────────────┴──────────────┴────────────────────┴─────────────────────┴─────────────┴──────────────┴────────────────┴─────────────────┴───────────────┴───────────────────┴────────────────────┴────────────────────┴──────────────────────────────────┴─────────────────────┴───────────────────────┴───────────────────────────────┴──────────────────────┴───────────────────────┴────────────────────────────────────────┴────────────────────────┴──────────────────┴───────────────────────────────────┴───────────────────┴──────────────────────────┴────────────────────┴────────────────────┴───────────────────────┴────────────────────────┴───────────────────────────────────────────┴─────────────────────────────────────────┴──────────────────────────────────┴──────────────────────────────┴─────────────────┴───────────────┴────────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Load the the data csv file from the /data directory\n",
    "telco_balanced_oversampling = DataFrame(CSV.File(\"data/telco_balanced_oversampling.csv\"))\n",
    "\n",
    "# Peek at the first 5 rows of the dataframe\n",
    "first(telco_balanced_oversampling, 5) |> pretty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "split_data (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function that splits the data into training and testing sets, features and target\n",
    "function split_data(df::DataFrame)\n",
    "\n",
    "  # Use random seed for reproducibility\n",
    "  rng = StableRNG(206)\n",
    "\n",
    "  # Split the data into training and testing sets\n",
    "  train, test = partition(df, 0.7, rng = rng, shuffle=true)\n",
    "\n",
    "\n",
    "  # Split the training data into features and target\n",
    "  X_train = train[!, Not(:churn_value)]\n",
    "  y_train = coerce(train.churn_value, Multiclass)\n",
    "\n",
    "  # Split the testing data into features and target\n",
    "  X_test = test[!, Not(:churn_value)]\n",
    "  y_test = coerce(test.churn_value, Multiclass)\n",
    "\n",
    "  # Return all 4 subsets\n",
    "  return X_train, y_train, X_test, y_test\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Evaluation Metric Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metrics (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a function that takes in a model fit and returns the accuracy of the model\n",
    "function metrics(model_name, predictions, y, threshold)\n",
    "\n",
    "  # Make predictions\n",
    "  p_hat = pdf.(predictions, 1)\n",
    "\n",
    "  # Convert to binary\n",
    "  y_hat = p_hat .> threshold\n",
    "\n",
    "  TP = sum((y_hat .== 1) .& (y .== 1))\n",
    "  TN = sum((y_hat .== 0) .& (y .== 0))\n",
    "  FP = sum((y_hat .== 1) .& (y .== 0))\n",
    "  FN = sum((y_hat .== 0) .& (y .== 1))\n",
    "\n",
    "  accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "  precision = TP / (TP + FP)\n",
    "  recall = TP / (TP + FN)\n",
    "  f1 = 2 * precision * recall / (precision + recall)\n",
    "  area_under_curve = auc(predictions, y)\n",
    "\n",
    "  # Create a DataFrame to store the metrics\n",
    "  results = DataFrame(\n",
    "    model = [model_name],\n",
    "    accuracy = [round(accuracy, digits=3)],\n",
    "    precision = [round(precision, digits=3)],\n",
    "    recall = [round(recall, digits=3)],\n",
    "    f1 = [round(f1, digits=3)],\n",
    "    auc = [round(area_under_curve, digits=3)]\n",
    "  )\n",
    "\n",
    "  return results\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>0×6 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">model</th><th style = \"text-align: left;\">accuracy</th><th style = \"text-align: left;\">precision</th><th style = \"text-align: left;\">recall</th><th style = \"text-align: left;\">f1</th><th style = \"text-align: left;\">auc</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"String\" style = \"text-align: left;\">String</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th></tr></thead><tbody></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& model & accuracy & precision & recall & f1 & auc\\\\\n",
       "\t\\hline\n",
       "\t& String & Float64 & Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m0×6 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m model  \u001b[0m\u001b[1m accuracy \u001b[0m\u001b[1m precision \u001b[0m\u001b[1m recall  \u001b[0m\u001b[1m f1      \u001b[0m\u001b[1m auc     \u001b[0m\n",
       "     │\u001b[90m String \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\n",
       "─────┴────────────────────────────────────────────────────────"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a function that takes model metrics and stores in a DataFrame\n",
    "all_model_metrics = DataFrame(\n",
    "  model = String[],\n",
    "  accuracy = Float64[],\n",
    "  precision = Float64[],\n",
    "  recall = Float64[],\n",
    "  f1 = Float64[],\n",
    "  auc = Float64[]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Logistic Regression Models\n",
    "We build logistic regression models to predict the target variable, `y`. We first start with a baseline model, using the standardized data with class imbalance. We then build models using the standardized data with class balancing using undersampling and oversampling. We then address some of the common pitfalls of linear models, and try to improve on predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: The Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trained Machine; caches model-specific representations of data\n",
       "  model: LogisticClassifier(lambda = 2.220446049250313e-16, …)\n",
       "  args: \n",
       "    1:\tSource @975 ⏎ Table{AbstractVector{Continuous}}\n",
       "    2:\tSource @507 ⏎ AbstractVector{Multiclass{2}}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split the data\n",
    "X_train, y_train, X_test, y_test = split_data(telco)\n",
    "\n",
    "# Build the machine\n",
    "model_baseline = fit!(machine(LogisticClassifier(), X_train, y_train), verbosity=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>1×6 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">model</th><th style = \"text-align: left;\">accuracy</th><th style = \"text-align: left;\">precision</th><th style = \"text-align: left;\">recall</th><th style = \"text-align: left;\">f1</th><th style = \"text-align: left;\">auc</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"String\" style = \"text-align: left;\">String</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: left;\">GLM: Baseline</td><td style = \"text-align: right;\">0.817</td><td style = \"text-align: right;\">0.67</td><td style = \"text-align: right;\">0.597</td><td style = \"text-align: right;\">0.632</td><td style = \"text-align: right;\">0.859</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& model & accuracy & precision & recall & f1 & auc\\\\\n",
       "\t\\hline\n",
       "\t& String & Float64 & Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & GLM: Baseline & 0.817 & 0.67 & 0.597 & 0.632 & 0.859 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m1×6 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m model         \u001b[0m\u001b[1m accuracy \u001b[0m\u001b[1m precision \u001b[0m\u001b[1m recall  \u001b[0m\u001b[1m f1      \u001b[0m\u001b[1m auc     \u001b[0m\n",
       "     │\u001b[90m String        \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\n",
       "─────┼───────────────────────────────────────────────────────────────\n",
       "   1 │ GLM: Baseline     0.817       0.67    0.597    0.632    0.859"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = predict(model_baseline, X_test)\n",
    "# Compute the metrics\n",
    "model_baseline_metrics = metrics(\"GLM: Baseline\", predictions, y_test, 0.5)\n",
    "# Store the metrics in the DataFrame\n",
    "push!(all_model_metrics, model_baseline_metrics[1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Threshold Parameter Tuning\n",
    "In classification models, the threshold can be used a tuning parameter to adjust the trade-off between precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip780\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<defs>\n",
       "  <clipPath id=\"clip781\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<defs>\n",
       "  <clipPath id=\"clip782\">\n",
       "    <rect x=\"219\" y=\"47\" width=\"1526\" height=\"1377\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip782)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"468.524,1423.18 468.524,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip782)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"879.528,1423.18 879.528,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip782)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1290.53,1423.18 1290.53,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip782)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1701.54,1423.18 1701.54,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip780)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,1423.18 1744.69,1423.18 \"/>\n",
       "<polyline clip-path=\"url(#clip780)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"468.524,1423.18 468.524,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip780)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"879.528,1423.18 879.528,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip780)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1290.53,1423.18 1290.53,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip780)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1701.54,1423.18 1701.54,1404.28 \"/>\n",
       "<path clip-path=\"url(#clip780)\" d=\"M446.707 1454.1 Q443.096 1454.1 441.267 1457.66 Q439.461 1461.2 439.461 1468.33 Q439.461 1475.44 441.267 1479.01 Q443.096 1482.55 446.707 1482.55 Q450.341 1482.55 452.147 1479.01 Q453.975 1475.44 453.975 1468.33 Q453.975 1461.2 452.147 1457.66 Q450.341 1454.1 446.707 1454.1 M446.707 1450.39 Q452.517 1450.39 455.572 1455 Q458.651 1459.58 458.651 1468.33 Q458.651 1477.06 455.572 1481.67 Q452.517 1486.25 446.707 1486.25 Q440.897 1486.25 437.818 1481.67 Q434.762 1477.06 434.762 1468.33 Q434.762 1459.58 437.818 1455 Q440.897 1450.39 446.707 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M466.869 1479.7 L471.753 1479.7 L471.753 1485.58 L466.869 1485.58 L466.869 1479.7 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M485.966 1481.64 L502.285 1481.64 L502.285 1485.58 L480.341 1485.58 L480.341 1481.64 Q483.003 1478.89 487.586 1474.26 Q492.193 1469.61 493.373 1468.27 Q495.619 1465.74 496.498 1464.01 Q497.401 1462.25 497.401 1460.56 Q497.401 1457.8 495.457 1456.07 Q493.535 1454.33 490.433 1454.33 Q488.234 1454.33 485.781 1455.09 Q483.35 1455.86 480.572 1457.41 L480.572 1452.69 Q483.396 1451.55 485.85 1450.97 Q488.304 1450.39 490.341 1450.39 Q495.711 1450.39 498.906 1453.08 Q502.1 1455.77 502.1 1460.26 Q502.1 1462.39 501.29 1464.31 Q500.503 1466.2 498.396 1468.8 Q497.818 1469.47 494.716 1472.69 Q491.614 1475.88 485.966 1481.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M856.669 1454.1 Q853.058 1454.1 851.23 1457.66 Q849.424 1461.2 849.424 1468.33 Q849.424 1475.44 851.23 1479.01 Q853.058 1482.55 856.669 1482.55 Q860.304 1482.55 862.109 1479.01 Q863.938 1475.44 863.938 1468.33 Q863.938 1461.2 862.109 1457.66 Q860.304 1454.1 856.669 1454.1 M856.669 1450.39 Q862.479 1450.39 865.535 1455 Q868.614 1459.58 868.614 1468.33 Q868.614 1477.06 865.535 1481.67 Q862.479 1486.25 856.669 1486.25 Q850.859 1486.25 847.78 1481.67 Q844.725 1477.06 844.725 1468.33 Q844.725 1459.58 847.78 1455 Q850.859 1450.39 856.669 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M876.831 1479.7 L881.715 1479.7 L881.715 1485.58 L876.831 1485.58 L876.831 1479.7 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M904.748 1455.09 L892.942 1473.54 L904.748 1473.54 L904.748 1455.09 M903.521 1451.02 L909.401 1451.02 L909.401 1473.54 L914.331 1473.54 L914.331 1477.43 L909.401 1477.43 L909.401 1485.58 L904.748 1485.58 L904.748 1477.43 L889.146 1477.43 L889.146 1472.92 L903.521 1451.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M1267.84 1454.1 Q1264.22 1454.1 1262.4 1457.66 Q1260.59 1461.2 1260.59 1468.33 Q1260.59 1475.44 1262.4 1479.01 Q1264.22 1482.55 1267.84 1482.55 Q1271.47 1482.55 1273.28 1479.01 Q1275.1 1475.44 1275.1 1468.33 Q1275.1 1461.2 1273.28 1457.66 Q1271.47 1454.1 1267.84 1454.1 M1267.84 1450.39 Q1273.65 1450.39 1276.7 1455 Q1279.78 1459.58 1279.78 1468.33 Q1279.78 1477.06 1276.7 1481.67 Q1273.65 1486.25 1267.84 1486.25 Q1262.03 1486.25 1258.95 1481.67 Q1255.89 1477.06 1255.89 1468.33 Q1255.89 1459.58 1258.95 1455 Q1262.03 1450.39 1267.84 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M1288 1479.7 L1292.88 1479.7 L1292.88 1485.58 L1288 1485.58 L1288 1479.7 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M1313.65 1466.44 Q1310.5 1466.44 1308.65 1468.59 Q1306.82 1470.74 1306.82 1474.49 Q1306.82 1478.22 1308.65 1480.39 Q1310.5 1482.55 1313.65 1482.55 Q1316.79 1482.55 1318.62 1480.39 Q1320.47 1478.22 1320.47 1474.49 Q1320.47 1470.74 1318.62 1468.59 Q1316.79 1466.44 1313.65 1466.44 M1322.93 1451.78 L1322.93 1456.04 Q1321.17 1455.21 1319.36 1454.77 Q1317.58 1454.33 1315.82 1454.33 Q1311.19 1454.33 1308.74 1457.45 Q1306.31 1460.58 1305.96 1466.9 Q1307.33 1464.89 1309.39 1463.82 Q1311.45 1462.73 1313.92 1462.73 Q1319.13 1462.73 1322.14 1465.9 Q1325.17 1469.05 1325.17 1474.49 Q1325.17 1479.82 1322.03 1483.03 Q1318.88 1486.25 1313.65 1486.25 Q1307.65 1486.25 1304.48 1481.67 Q1301.31 1477.06 1301.31 1468.33 Q1301.31 1460.14 1305.2 1455.28 Q1309.09 1450.39 1315.64 1450.39 Q1317.4 1450.39 1319.18 1450.74 Q1320.98 1451.09 1322.93 1451.78 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M1678.97 1454.1 Q1675.36 1454.1 1673.53 1457.66 Q1671.72 1461.2 1671.72 1468.33 Q1671.72 1475.44 1673.53 1479.01 Q1675.36 1482.55 1678.97 1482.55 Q1682.6 1482.55 1684.41 1479.01 Q1686.24 1475.44 1686.24 1468.33 Q1686.24 1461.2 1684.41 1457.66 Q1682.6 1454.1 1678.97 1454.1 M1678.97 1450.39 Q1684.78 1450.39 1687.83 1455 Q1690.91 1459.58 1690.91 1468.33 Q1690.91 1477.06 1687.83 1481.67 Q1684.78 1486.25 1678.97 1486.25 Q1673.16 1486.25 1670.08 1481.67 Q1667.02 1477.06 1667.02 1468.33 Q1667.02 1459.58 1670.08 1455 Q1673.16 1450.39 1678.97 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M1699.13 1479.7 L1704.01 1479.7 L1704.01 1485.58 L1699.13 1485.58 L1699.13 1479.7 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M1724.2 1469.17 Q1720.86 1469.17 1718.94 1470.95 Q1717.05 1472.73 1717.05 1475.86 Q1717.05 1478.98 1718.94 1480.77 Q1720.86 1482.55 1724.2 1482.55 Q1727.53 1482.55 1729.45 1480.77 Q1731.37 1478.96 1731.37 1475.86 Q1731.37 1472.73 1729.45 1470.95 Q1727.55 1469.17 1724.2 1469.17 M1719.52 1467.18 Q1716.51 1466.44 1714.82 1464.38 Q1713.16 1462.32 1713.16 1459.35 Q1713.16 1455.21 1716.1 1452.8 Q1719.06 1450.39 1724.2 1450.39 Q1729.36 1450.39 1732.3 1452.8 Q1735.24 1455.21 1735.24 1459.35 Q1735.24 1462.32 1733.55 1464.38 Q1731.88 1466.44 1728.9 1467.18 Q1732.28 1467.96 1734.15 1470.26 Q1736.05 1472.55 1736.05 1475.86 Q1736.05 1480.88 1732.97 1483.57 Q1729.92 1486.25 1724.2 1486.25 Q1718.48 1486.25 1715.4 1483.57 Q1712.35 1480.88 1712.35 1475.86 Q1712.35 1472.55 1714.24 1470.26 Q1716.14 1467.96 1719.52 1467.18 M1717.81 1459.79 Q1717.81 1462.48 1719.48 1463.98 Q1721.17 1465.49 1724.2 1465.49 Q1727.21 1465.49 1728.9 1463.98 Q1730.61 1462.48 1730.61 1459.79 Q1730.61 1457.11 1728.9 1455.6 Q1727.21 1454.1 1724.2 1454.1 Q1721.17 1454.1 1719.48 1455.6 Q1717.81 1457.11 1717.81 1459.79 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M824.521 1520.52 L864.72 1520.52 L864.72 1525.93 L847.851 1525.93 L847.851 1568.04 L841.39 1568.04 L841.39 1525.93 L824.521 1525.93 L824.521 1520.52 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M900.305 1546.53 L900.305 1568.04 L894.448 1568.04 L894.448 1546.72 Q894.448 1541.66 892.475 1539.14 Q890.501 1536.63 886.555 1536.63 Q881.812 1536.63 879.075 1539.65 Q876.338 1542.68 876.338 1547.9 L876.338 1568.04 L870.449 1568.04 L870.449 1518.52 L876.338 1518.52 L876.338 1537.93 Q878.438 1534.72 881.271 1533.13 Q884.136 1531.54 887.86 1531.54 Q894.003 1531.54 897.154 1535.36 Q900.305 1539.14 900.305 1546.53 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M932.642 1537.87 Q931.656 1537.3 930.478 1537.04 Q929.332 1536.76 927.932 1536.76 Q922.967 1536.76 920.293 1540 Q917.651 1543.22 917.651 1549.27 L917.651 1568.04 L911.763 1568.04 L911.763 1532.4 L917.651 1532.4 L917.651 1537.93 Q919.497 1534.69 922.457 1533.13 Q925.417 1531.54 929.651 1531.54 Q930.255 1531.54 930.987 1531.63 Q931.719 1531.7 932.611 1531.85 L932.642 1537.87 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M967.845 1548.76 L967.845 1551.62 L940.918 1551.62 Q941.3 1557.67 944.546 1560.85 Q947.825 1564 953.649 1564 Q957.023 1564 960.174 1563.17 Q963.357 1562.35 966.476 1560.69 L966.476 1566.23 Q963.325 1567.57 960.015 1568.27 Q956.705 1568.97 953.299 1568.97 Q944.769 1568.97 939.772 1564 Q934.807 1559.04 934.807 1550.57 Q934.807 1541.82 939.517 1536.69 Q944.26 1531.54 952.281 1531.54 Q959.474 1531.54 963.643 1536.18 Q967.845 1540.8 967.845 1548.76 M961.988 1547.04 Q961.925 1542.23 959.283 1539.37 Q956.673 1536.5 952.344 1536.5 Q947.443 1536.5 944.483 1539.27 Q941.554 1542.04 941.109 1547.07 L961.988 1547.04 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M1000.18 1533.45 L1000.18 1538.98 Q997.7 1537.71 995.026 1537.07 Q992.353 1536.44 989.488 1536.44 Q985.128 1536.44 982.932 1537.77 Q980.767 1539.11 980.767 1541.79 Q980.767 1543.82 982.327 1545 Q983.886 1546.15 988.597 1547.2 L990.602 1547.64 Q996.841 1548.98 999.451 1551.43 Q1002.09 1553.85 1002.09 1558.21 Q1002.09 1563.17 998.146 1566.07 Q994.231 1568.97 987.356 1568.97 Q984.491 1568.97 981.372 1568.39 Q978.285 1567.85 974.847 1566.74 L974.847 1560.69 Q978.094 1562.38 981.245 1563.24 Q984.396 1564.07 987.483 1564.07 Q991.621 1564.07 993.849 1562.66 Q996.077 1561.23 996.077 1558.65 Q996.077 1556.27 994.453 1554.99 Q992.862 1553.72 987.419 1552.54 L985.382 1552.07 Q979.94 1550.92 977.521 1548.56 Q975.102 1546.18 975.102 1542.04 Q975.102 1537.01 978.666 1534.27 Q982.231 1531.54 988.788 1531.54 Q992.034 1531.54 994.899 1532.01 Q997.764 1532.49 1000.18 1533.45 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M1041.05 1546.53 L1041.05 1568.04 L1035.19 1568.04 L1035.19 1546.72 Q1035.19 1541.66 1033.22 1539.14 Q1031.25 1536.63 1027.3 1536.63 Q1022.56 1536.63 1019.82 1539.65 Q1017.08 1542.68 1017.08 1547.9 L1017.08 1568.04 L1011.2 1568.04 L1011.2 1518.52 L1017.08 1518.52 L1017.08 1537.93 Q1019.18 1534.72 1022.02 1533.13 Q1024.88 1531.54 1028.61 1531.54 Q1034.75 1531.54 1037.9 1535.36 Q1041.05 1539.14 1041.05 1546.53 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M1066.55 1536.5 Q1061.83 1536.5 1059.1 1540.19 Q1056.36 1543.85 1056.36 1550.25 Q1056.36 1556.65 1059.07 1560.34 Q1061.8 1564 1066.55 1564 Q1071.22 1564 1073.96 1560.31 Q1076.7 1556.62 1076.7 1550.25 Q1076.7 1543.92 1073.96 1540.23 Q1071.22 1536.5 1066.55 1536.5 M1066.55 1531.54 Q1074.18 1531.54 1078.54 1536.5 Q1082.9 1541.47 1082.9 1550.25 Q1082.9 1559 1078.54 1564 Q1074.18 1568.97 1066.55 1568.97 Q1058.87 1568.97 1054.51 1564 Q1050.19 1559 1050.19 1550.25 Q1050.19 1541.47 1054.51 1536.5 Q1058.87 1531.54 1066.55 1531.54 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M1092.61 1518.52 L1098.47 1518.52 L1098.47 1568.04 L1092.61 1568.04 L1092.61 1518.52 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M1134.18 1537.81 L1134.18 1518.52 L1140.04 1518.52 L1140.04 1568.04 L1134.18 1568.04 L1134.18 1562.7 Q1132.33 1565.88 1129.5 1567.44 Q1126.7 1568.97 1122.75 1568.97 Q1116.29 1568.97 1112.22 1563.81 Q1108.18 1558.65 1108.18 1550.25 Q1108.18 1541.85 1112.22 1536.69 Q1116.29 1531.54 1122.75 1531.54 Q1126.7 1531.54 1129.5 1533.1 Q1132.33 1534.62 1134.18 1537.81 M1114.22 1550.25 Q1114.22 1556.71 1116.87 1560.4 Q1119.54 1564.07 1124.19 1564.07 Q1128.83 1564.07 1131.51 1560.4 Q1134.18 1556.71 1134.18 1550.25 Q1134.18 1543.79 1131.51 1540.13 Q1128.83 1536.44 1124.19 1536.44 Q1119.54 1536.44 1116.87 1540.13 Q1114.22 1543.79 1114.22 1550.25 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip782)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"219.866,1176.67 1744.69,1176.67 \"/>\n",
       "<polyline clip-path=\"url(#clip782)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"219.866,884.313 1744.69,884.313 \"/>\n",
       "<polyline clip-path=\"url(#clip782)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"219.866,591.958 1744.69,591.958 \"/>\n",
       "<polyline clip-path=\"url(#clip782)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"219.866,299.604 1744.69,299.604 \"/>\n",
       "<polyline clip-path=\"url(#clip780)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,1423.18 219.866,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip780)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,1176.67 233.376,1176.67 \"/>\n",
       "<polyline clip-path=\"url(#clip780)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,884.313 233.376,884.313 \"/>\n",
       "<polyline clip-path=\"url(#clip780)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,591.958 233.376,591.958 \"/>\n",
       "<polyline clip-path=\"url(#clip780)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,299.604 233.376,299.604 \"/>\n",
       "<path clip-path=\"url(#clip780)\" d=\"M128.288 1162.47 Q124.677 1162.47 122.848 1166.03 Q121.043 1169.57 121.043 1176.7 Q121.043 1183.81 122.848 1187.37 Q124.677 1190.91 128.288 1190.91 Q131.922 1190.91 133.728 1187.37 Q135.556 1183.81 135.556 1176.7 Q135.556 1169.57 133.728 1166.03 Q131.922 1162.47 128.288 1162.47 M128.288 1158.76 Q134.098 1158.76 137.154 1163.37 Q140.232 1167.95 140.232 1176.7 Q140.232 1185.43 137.154 1190.03 Q134.098 1194.62 128.288 1194.62 Q122.478 1194.62 119.399 1190.03 Q116.343 1185.43 116.343 1176.7 Q116.343 1167.95 119.399 1163.37 Q122.478 1158.76 128.288 1158.76 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M148.45 1188.07 L153.334 1188.07 L153.334 1193.95 L148.45 1193.95 L148.45 1188.07 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M167.547 1190.01 L183.866 1190.01 L183.866 1193.95 L161.922 1193.95 L161.922 1190.01 Q164.584 1187.26 169.167 1182.63 Q173.774 1177.97 174.954 1176.63 Q177.2 1174.11 178.079 1172.37 Q178.982 1170.61 178.982 1168.92 Q178.982 1166.17 177.038 1164.43 Q175.116 1162.7 172.014 1162.7 Q169.815 1162.7 167.362 1163.46 Q164.931 1164.22 162.153 1165.78 L162.153 1161.05 Q164.977 1159.92 167.431 1159.34 Q169.885 1158.76 171.922 1158.76 Q177.292 1158.76 180.487 1161.45 Q183.681 1164.13 183.681 1168.62 Q183.681 1170.75 182.871 1172.67 Q182.084 1174.57 179.977 1177.16 Q179.399 1177.84 176.297 1181.05 Q173.195 1184.25 167.547 1190.01 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M126.205 870.111 Q122.593 870.111 120.765 873.676 Q118.959 877.218 118.959 884.347 Q118.959 891.454 120.765 895.019 Q122.593 898.56 126.205 898.56 Q129.839 898.56 131.644 895.019 Q133.473 891.454 133.473 884.347 Q133.473 877.218 131.644 873.676 Q129.839 870.111 126.205 870.111 M126.205 866.408 Q132.015 866.408 135.07 871.014 Q138.149 875.597 138.149 884.347 Q138.149 893.074 135.07 897.681 Q132.015 902.264 126.205 902.264 Q120.394 902.264 117.316 897.681 Q114.26 893.074 114.26 884.347 Q114.26 875.597 117.316 871.014 Q120.394 866.408 126.205 866.408 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M146.366 895.713 L151.251 895.713 L151.251 901.593 L146.366 901.593 L146.366 895.713 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M174.283 871.107 L162.477 889.556 L174.283 889.556 L174.283 871.107 M173.056 867.033 L178.936 867.033 L178.936 889.556 L183.866 889.556 L183.866 893.444 L178.936 893.444 L178.936 901.593 L174.283 901.593 L174.283 893.444 L158.681 893.444 L158.681 888.931 L173.056 867.033 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M126.529 577.757 Q122.918 577.757 121.089 581.322 Q119.283 584.864 119.283 591.993 Q119.283 599.1 121.089 602.664 Q122.918 606.206 126.529 606.206 Q130.163 606.206 131.968 602.664 Q133.797 599.1 133.797 591.993 Q133.797 584.864 131.968 581.322 Q130.163 577.757 126.529 577.757 M126.529 574.053 Q132.339 574.053 135.394 578.66 Q138.473 583.243 138.473 591.993 Q138.473 600.72 135.394 605.326 Q132.339 609.91 126.529 609.91 Q120.718 609.91 117.64 605.326 Q114.584 600.72 114.584 591.993 Q114.584 583.243 117.64 578.66 Q120.718 574.053 126.529 574.053 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M146.691 603.359 L151.575 603.359 L151.575 609.238 L146.691 609.238 L146.691 603.359 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M172.339 590.095 Q169.19 590.095 167.339 592.248 Q165.51 594.401 165.51 598.151 Q165.51 601.877 167.339 604.053 Q169.19 606.206 172.339 606.206 Q175.487 606.206 177.315 604.053 Q179.167 601.877 179.167 598.151 Q179.167 594.401 177.315 592.248 Q175.487 590.095 172.339 590.095 M181.621 575.442 L181.621 579.702 Q179.862 578.868 178.056 578.428 Q176.274 577.989 174.514 577.989 Q169.885 577.989 167.431 581.114 Q165.001 584.239 164.653 590.558 Q166.019 588.544 168.079 587.479 Q170.139 586.391 172.616 586.391 Q177.825 586.391 180.834 589.563 Q183.866 592.711 183.866 598.151 Q183.866 603.475 180.718 606.692 Q177.57 609.91 172.339 609.91 Q166.343 609.91 163.172 605.326 Q160.001 600.72 160.001 591.993 Q160.001 583.799 163.89 578.938 Q167.778 574.053 174.329 574.053 Q176.089 574.053 177.871 574.401 Q179.676 574.748 181.621 575.442 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M126.783 285.403 Q123.172 285.403 121.343 288.968 Q119.538 292.509 119.538 299.639 Q119.538 306.745 121.343 310.31 Q123.172 313.852 126.783 313.852 Q130.417 313.852 132.223 310.31 Q134.052 306.745 134.052 299.639 Q134.052 292.509 132.223 288.968 Q130.417 285.403 126.783 285.403 M126.783 281.699 Q132.593 281.699 135.649 286.306 Q138.728 290.889 138.728 299.639 Q138.728 308.366 135.649 312.972 Q132.593 317.556 126.783 317.556 Q120.973 317.556 117.894 312.972 Q114.839 308.366 114.839 299.639 Q114.839 290.889 117.894 286.306 Q120.973 281.699 126.783 281.699 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M146.945 311.005 L151.829 311.005 L151.829 316.884 L146.945 316.884 L146.945 311.005 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M172.014 300.472 Q168.681 300.472 166.76 302.255 Q164.862 304.037 164.862 307.162 Q164.862 310.287 166.76 312.069 Q168.681 313.852 172.014 313.852 Q175.348 313.852 177.269 312.069 Q179.19 310.264 179.19 307.162 Q179.19 304.037 177.269 302.255 Q175.371 300.472 172.014 300.472 M167.339 298.482 Q164.329 297.741 162.64 295.681 Q160.973 293.62 160.973 290.658 Q160.973 286.514 163.913 284.107 Q166.876 281.699 172.014 281.699 Q177.176 281.699 180.116 284.107 Q183.056 286.514 183.056 290.658 Q183.056 293.62 181.366 295.681 Q179.7 297.741 176.714 298.482 Q180.093 299.269 181.968 301.56 Q183.866 303.852 183.866 307.162 Q183.866 312.185 180.788 314.87 Q177.732 317.556 172.014 317.556 Q166.297 317.556 163.218 314.87 Q160.163 312.185 160.163 307.162 Q160.163 303.852 162.061 301.56 Q163.959 299.269 167.339 298.482 M165.626 291.097 Q165.626 293.783 167.292 295.287 Q168.982 296.792 172.014 296.792 Q175.024 296.792 176.714 295.287 Q178.426 293.783 178.426 291.097 Q178.426 288.412 176.714 286.908 Q175.024 285.403 172.014 285.403 Q168.982 285.403 167.292 286.908 Q165.626 288.412 165.626 291.097 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M21.7677 1047.23 L39.6235 1047.23 L39.6235 1039.14 Q39.6235 1034.65 37.3 1032.2 Q34.9765 1029.75 30.6797 1029.75 Q26.4147 1029.75 24.0912 1032.2 Q21.7677 1034.65 21.7677 1039.14 L21.7677 1047.23 M16.4842 1053.66 L16.4842 1039.14 Q16.4842 1031.15 20.1126 1027.08 Q23.7092 1022.97 30.6797 1022.97 Q37.7138 1022.97 41.3104 1027.08 Q44.907 1031.15 44.907 1039.14 L44.907 1047.23 L64.0042 1047.23 L64.0042 1053.66 L16.4842 1053.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M33.8307 995.092 Q33.2578 996.079 33.0032 997.256 Q32.7167 998.402 32.7167 999.803 Q32.7167 1004.77 35.9632 1007.44 Q39.1779 1010.08 45.2253 1010.08 L64.0042 1010.08 L64.0042 1015.97 L28.3562 1015.97 L28.3562 1010.08 L33.8944 1010.08 Q30.6479 1008.24 29.0883 1005.28 Q27.4968 1002.32 27.4968 998.084 Q27.4968 997.479 27.5923 996.747 Q27.656 996.015 27.8151 995.124 L33.8307 995.092 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M44.7161 959.89 L47.5806 959.89 L47.5806 986.817 Q53.6281 986.435 56.8109 983.188 Q59.9619 979.91 59.9619 974.085 Q59.9619 970.711 59.1344 967.56 Q58.3069 964.377 56.6518 961.258 L62.1899 961.258 Q63.5267 964.409 64.227 967.719 Q64.9272 971.03 64.9272 974.435 Q64.9272 982.965 59.9619 987.962 Q54.9967 992.928 46.5303 992.928 Q37.7774 992.928 32.6531 988.217 Q27.4968 983.475 27.4968 975.454 Q27.4968 968.26 32.1438 964.091 Q36.7589 959.89 44.7161 959.89 M42.9973 965.746 Q38.1912 965.81 35.3266 968.451 Q32.4621 971.061 32.4621 975.39 Q32.4621 980.292 35.2312 983.252 Q38.0002 986.18 43.0292 986.626 L42.9973 965.746 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M29.7248 924.624 L35.1993 924.624 Q33.8307 927.106 33.1623 929.621 Q32.4621 932.103 32.4621 934.65 Q32.4621 940.347 36.0905 943.498 Q39.6872 946.649 46.212 946.649 Q52.7369 946.649 56.3653 943.498 Q59.9619 940.347 59.9619 934.65 Q59.9619 932.103 59.2935 929.621 Q58.5933 927.106 57.2247 924.624 L62.6355 924.624 Q63.7814 927.074 64.3543 929.716 Q64.9272 932.326 64.9272 935.286 Q64.9272 943.339 59.8664 948.081 Q54.8057 952.824 46.212 952.824 Q37.491 952.824 32.4939 948.049 Q27.4968 943.243 27.4968 934.904 Q27.4968 932.199 28.0697 929.621 Q28.6108 927.043 29.7248 924.624 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M28.3562 914.438 L28.3562 908.582 L64.0042 908.582 L64.0042 914.438 L28.3562 914.438 M14.479 914.438 L14.479 908.582 L21.895 908.582 L21.895 914.438 L14.479 914.438 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M29.4065 873.602 L34.9447 873.602 Q33.6716 876.085 33.035 878.759 Q32.3984 881.432 32.3984 884.297 Q32.3984 888.657 33.7352 890.853 Q35.072 893.018 37.7456 893.018 Q39.7826 893.018 40.9603 891.458 Q42.1061 889.899 43.1565 885.188 L43.6021 883.183 Q44.9389 876.944 47.3897 874.334 Q49.8086 871.693 54.1691 871.693 Q59.1344 871.693 62.0308 875.639 Q64.9272 879.554 64.9272 886.429 Q64.9272 889.294 64.3543 892.413 Q63.8132 895.5 62.6992 898.938 L56.6518 898.938 Q58.3387 895.691 59.198 892.54 Q60.0256 889.389 60.0256 886.302 Q60.0256 882.164 58.6251 879.936 Q57.1929 877.708 54.6147 877.708 Q52.2276 877.708 50.9545 879.332 Q49.6813 880.923 48.5037 886.366 L48.0262 888.403 Q46.8804 893.845 44.5251 896.264 Q42.138 898.683 38.0002 898.683 Q32.9713 898.683 30.2341 895.119 Q27.4968 891.554 27.4968 884.997 Q27.4968 881.751 27.9743 878.886 Q28.4517 876.021 29.4065 873.602 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M28.3562 862.367 L28.3562 856.511 L64.0042 856.511 L64.0042 862.367 L28.3562 862.367 M14.479 862.367 L14.479 856.511 L21.895 856.511 L21.895 862.367 L14.479 862.367 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M32.4621 830.443 Q32.4621 835.154 36.1542 837.891 Q39.8145 840.628 46.212 840.628 Q52.6095 840.628 56.3017 837.923 Q59.9619 835.185 59.9619 830.443 Q59.9619 825.764 56.2698 823.027 Q52.5777 820.29 46.212 820.29 Q39.8781 820.29 36.186 823.027 Q32.4621 825.764 32.4621 830.443 M27.4968 830.443 Q27.4968 822.804 32.4621 818.444 Q37.4273 814.083 46.212 814.083 Q54.9649 814.083 59.9619 818.444 Q64.9272 822.804 64.9272 830.443 Q64.9272 838.114 59.9619 842.474 Q54.9649 846.803 46.212 846.803 Q37.4273 846.803 32.4621 842.474 Q27.4968 838.114 27.4968 830.443 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M42.4881 774.743 L64.0042 774.743 L64.0042 780.599 L42.679 780.599 Q37.6183 780.599 35.1038 782.573 Q32.5894 784.546 32.5894 788.493 Q32.5894 793.235 35.6131 795.973 Q38.6368 798.71 43.8567 798.71 L64.0042 798.71 L64.0042 804.598 L28.3562 804.598 L28.3562 798.71 L33.8944 798.71 Q30.6797 796.609 29.0883 793.776 Q27.4968 790.912 27.4968 787.188 Q27.4968 781.045 31.3163 777.894 Q35.1038 774.743 42.4881 774.743 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M46.0847 726.141 Q46.0847 733.239 47.7079 735.976 Q49.3312 738.713 53.2461 738.713 Q56.3653 738.713 58.2114 736.676 Q60.0256 734.607 60.0256 731.074 Q60.0256 726.204 56.5881 723.276 Q53.1188 720.316 47.3897 720.316 L46.0847 720.316 L46.0847 726.141 M43.6657 714.46 L64.0042 714.46 L64.0042 720.316 L58.5933 720.316 Q61.8398 722.321 63.3994 725.313 Q64.9272 728.305 64.9272 732.634 Q64.9272 738.108 61.8716 741.355 Q58.7843 744.57 53.6281 744.57 Q47.6125 744.57 44.5569 740.559 Q41.5014 736.517 41.5014 728.528 L41.5014 720.316 L40.9285 720.316 Q36.8862 720.316 34.6901 722.99 Q32.4621 725.632 32.4621 730.438 Q32.4621 733.493 33.1941 736.39 Q33.9262 739.286 35.3903 741.96 L29.9795 741.96 Q28.7381 738.745 28.1334 735.721 Q27.4968 732.698 27.4968 729.833 Q27.4968 722.099 31.5072 718.279 Q35.5176 714.46 43.6657 714.46 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M42.4881 672.764 L64.0042 672.764 L64.0042 678.621 L42.679 678.621 Q37.6183 678.621 35.1038 680.594 Q32.5894 682.568 32.5894 686.514 Q32.5894 691.257 35.6131 693.994 Q38.6368 696.731 43.8567 696.731 L64.0042 696.731 L64.0042 702.62 L28.3562 702.62 L28.3562 696.731 L33.8944 696.731 Q30.6797 694.631 29.0883 691.798 Q27.4968 688.933 27.4968 685.209 Q27.4968 679.066 31.3163 675.915 Q35.1038 672.764 42.4881 672.764 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M33.7671 637.626 L14.479 637.626 L14.479 631.769 L64.0042 631.769 L64.0042 637.626 L58.657 637.626 Q61.8398 639.472 63.3994 642.304 Q64.9272 645.105 64.9272 649.052 Q64.9272 655.513 59.771 659.587 Q54.6147 663.63 46.212 663.63 Q37.8093 663.63 32.6531 659.587 Q27.4968 655.513 27.4968 649.052 Q27.4968 645.105 29.0564 642.304 Q30.5842 639.472 33.7671 637.626 M46.212 657.582 Q52.6732 657.582 56.3653 654.94 Q60.0256 652.267 60.0256 647.62 Q60.0256 642.973 56.3653 640.299 Q52.6732 637.626 46.212 637.626 Q39.7508 637.626 36.0905 640.299 Q32.3984 642.973 32.3984 647.62 Q32.3984 652.267 36.0905 654.94 Q39.7508 657.582 46.212 657.582 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M41.7242 576.197 Q42.4244 574.128 44.7161 572.186 Q47.0077 570.213 51.0181 568.239 L64.0042 561.715 L64.0042 568.621 L51.8138 574.701 Q47.0395 577.056 45.48 579.284 Q43.9204 581.48 43.9204 585.3 L43.9204 592.302 L64.0042 592.302 L64.0042 598.731 L16.4842 598.731 L16.4842 584.217 Q16.4842 576.069 19.8898 572.059 Q23.2955 568.049 30.1704 568.049 Q34.6582 568.049 37.6183 570.149 Q40.5784 572.218 41.7242 576.197 M21.7677 592.302 L38.6368 592.302 L38.6368 584.217 Q38.6368 579.57 36.5043 577.215 Q34.34 574.828 30.1704 574.828 Q26.0009 574.828 23.9002 577.215 Q21.7677 579.57 21.7677 584.217 L21.7677 592.302 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M44.7161 526.13 L47.5806 526.13 L47.5806 553.057 Q53.6281 552.675 56.8109 549.429 Q59.9619 546.15 59.9619 540.326 Q59.9619 536.952 59.1344 533.801 Q58.3069 530.618 56.6518 527.499 L62.1899 527.499 Q63.5267 530.65 64.227 533.96 Q64.9272 537.27 64.9272 540.676 Q64.9272 549.206 59.9619 554.203 Q54.9967 559.168 46.5303 559.168 Q37.7774 559.168 32.6531 554.458 Q27.4968 549.715 27.4968 541.694 Q27.4968 534.501 32.1438 530.332 Q36.7589 526.13 44.7161 526.13 M42.9973 531.987 Q38.1912 532.05 35.3266 534.692 Q32.4621 537.302 32.4621 541.631 Q32.4621 546.532 35.2312 549.492 Q38.0002 552.421 43.0292 552.866 L42.9973 531.987 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M29.7248 490.864 L35.1993 490.864 Q33.8307 493.347 33.1623 495.861 Q32.4621 498.344 32.4621 500.89 Q32.4621 506.588 36.0905 509.739 Q39.6872 512.89 46.212 512.89 Q52.7369 512.89 56.3653 509.739 Q59.9619 506.588 59.9619 500.89 Q59.9619 498.344 59.2935 495.861 Q58.5933 493.347 57.2247 490.864 L62.6355 490.864 Q63.7814 493.315 64.3543 495.957 Q64.9272 498.567 64.9272 501.527 Q64.9272 509.579 59.8664 514.322 Q54.8057 519.064 46.212 519.064 Q37.491 519.064 32.4939 514.29 Q27.4968 509.484 27.4968 501.145 Q27.4968 498.44 28.0697 495.861 Q28.6108 493.283 29.7248 490.864 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M46.0847 464.478 Q46.0847 471.576 47.7079 474.313 Q49.3312 477.051 53.2461 477.051 Q56.3653 477.051 58.2114 475.014 Q60.0256 472.945 60.0256 469.412 Q60.0256 464.542 56.5881 461.614 Q53.1188 458.654 47.3897 458.654 L46.0847 458.654 L46.0847 464.478 M43.6657 452.797 L64.0042 452.797 L64.0042 458.654 L58.5933 458.654 Q61.8398 460.659 63.3994 463.651 Q64.9272 466.643 64.9272 470.971 Q64.9272 476.446 61.8716 479.693 Q58.7843 482.907 53.6281 482.907 Q47.6125 482.907 44.5569 478.897 Q41.5014 474.855 41.5014 466.866 L41.5014 458.654 L40.9285 458.654 Q36.8862 458.654 34.6901 461.327 Q32.4621 463.969 32.4621 468.775 Q32.4621 471.831 33.1941 474.727 Q33.9262 477.624 35.3903 480.297 L29.9795 480.297 Q28.7381 477.083 28.1334 474.059 Q27.4968 471.035 27.4968 468.171 Q27.4968 460.436 31.5072 456.617 Q35.5176 452.797 43.6657 452.797 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M14.479 440.734 L14.479 434.878 L64.0042 434.878 L64.0042 440.734 L14.479 440.734 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M14.479 422.624 L14.479 416.767 L64.0042 416.767 L64.0042 422.624 L14.479 422.624 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip782)\" style=\"stroke:#191919; stroke-linecap:round; stroke-linejoin:round; stroke-width:12; stroke-opacity:1; fill:none\" points=\"263.022,871.157 468.524,760.062 674.026,675.279 879.528,589.035 1085.03,489.634 1290.53,437.011 1496.03,349.304 1701.54,93.4945 \"/>\n",
       "<polyline clip-path=\"url(#clip782)\" style=\"stroke:#c67812; stroke-linecap:round; stroke-linejoin:round; stroke-width:12; stroke-opacity:1; fill:none\" points=\"263.022,86.1857 468.524,170.968 674.026,311.298 879.528,447.243 1085.03,596.344 1290.53,827.304 1496.03,1088.96 1701.54,1384.24 \"/>\n",
       "<polyline clip-path=\"url(#clip782)\" style=\"stroke:#a9a9a9; stroke-linecap:round; stroke-linejoin:round; stroke-width:12; stroke-opacity:1; fill:none\" stroke-dasharray=\"48, 30\" points=\"612.375,2799.12 612.375,-1328.69 \"/>\n",
       "<path clip-path=\"url(#clip780)\" d=\"M1815.79 858.836 L2352.76 858.836 L2352.76 651.476 L1815.79 651.476  Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"0\"/>\n",
       "<polyline clip-path=\"url(#clip780)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1815.79,858.836 2352.76,858.836 2352.76,651.476 1815.79,651.476 1815.79,858.836 \"/>\n",
       "<polyline clip-path=\"url(#clip780)\" style=\"stroke:#191919; stroke-linecap:round; stroke-linejoin:round; stroke-width:12; stroke-opacity:1; fill:none\" points=\"1839.49,703.316 1981.68,703.316 \"/>\n",
       "<path clip-path=\"url(#clip780)\" d=\"M2010.05 689.879 L2010.05 702.865 L2015.93 702.865 Q2019.2 702.865 2020.98 701.175 Q2022.76 699.485 2022.76 696.361 Q2022.76 693.259 2020.98 691.569 Q2019.2 689.879 2015.93 689.879 L2010.05 689.879 M2005.38 686.036 L2015.93 686.036 Q2021.74 686.036 2024.71 688.675 Q2027.69 691.291 2027.69 696.361 Q2027.69 701.476 2024.71 704.092 Q2021.74 706.708 2015.93 706.708 L2010.05 706.708 L2010.05 720.596 L2005.38 720.596 L2005.38 686.036 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2047.97 698.652 Q2047.25 698.236 2046.4 698.05 Q2045.56 697.842 2044.54 697.842 Q2040.93 697.842 2038.99 700.203 Q2037.07 702.541 2037.07 706.939 L2037.07 720.596 L2032.79 720.596 L2032.79 694.671 L2037.07 694.671 L2037.07 698.698 Q2038.41 696.337 2040.56 695.203 Q2042.72 694.046 2045.79 694.046 Q2046.23 694.046 2046.77 694.115 Q2047.3 694.161 2047.95 694.277 L2047.97 698.652 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2073.57 706.569 L2073.57 708.652 L2053.99 708.652 Q2054.27 713.05 2056.63 715.365 Q2059.01 717.657 2063.25 717.657 Q2065.7 717.657 2067.99 717.055 Q2070.31 716.453 2072.58 715.249 L2072.58 719.277 Q2070.29 720.249 2067.88 720.759 Q2065.47 721.268 2062.99 721.268 Q2056.79 721.268 2053.16 717.657 Q2049.54 714.046 2049.54 707.888 Q2049.54 701.523 2052.97 697.796 Q2056.42 694.046 2062.25 694.046 Q2067.48 694.046 2070.52 697.425 Q2073.57 700.782 2073.57 706.569 M2069.31 705.319 Q2069.27 701.823 2067.35 699.74 Q2065.45 697.657 2062.3 697.657 Q2058.73 697.657 2056.58 699.671 Q2054.45 701.685 2054.13 705.342 L2069.31 705.319 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2099.22 695.666 L2099.22 699.648 Q2097.41 698.652 2095.59 698.166 Q2093.78 697.657 2091.93 697.657 Q2087.79 697.657 2085.49 700.296 Q2083.2 702.911 2083.2 707.657 Q2083.2 712.402 2085.49 715.041 Q2087.79 717.657 2091.93 717.657 Q2093.78 717.657 2095.59 717.171 Q2097.41 716.661 2099.22 715.666 L2099.22 719.601 Q2097.44 720.434 2095.52 720.851 Q2093.62 721.268 2091.47 721.268 Q2085.61 721.268 2082.16 717.587 Q2078.71 713.907 2078.71 707.657 Q2078.71 701.314 2082.18 697.68 Q2085.68 694.046 2091.74 694.046 Q2093.71 694.046 2095.59 694.462 Q2097.46 694.856 2099.22 695.666 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2106.63 694.671 L2110.89 694.671 L2110.89 720.596 L2106.63 720.596 L2106.63 694.671 M2106.63 684.578 L2110.89 684.578 L2110.89 689.972 L2106.63 689.972 L2106.63 684.578 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2136.33 695.435 L2136.33 699.462 Q2134.52 698.536 2132.58 698.073 Q2130.63 697.611 2128.55 697.611 Q2125.38 697.611 2123.78 698.583 Q2122.21 699.555 2122.21 701.499 Q2122.21 702.981 2123.34 703.837 Q2124.47 704.671 2127.9 705.435 L2129.36 705.759 Q2133.9 706.731 2135.79 708.513 Q2137.72 710.272 2137.72 713.444 Q2137.72 717.055 2134.85 719.161 Q2132 721.268 2127 721.268 Q2124.91 721.268 2122.65 720.851 Q2120.4 720.458 2117.9 719.647 L2117.9 715.249 Q2120.26 716.476 2122.55 717.101 Q2124.85 717.703 2127.09 717.703 Q2130.1 717.703 2131.72 716.684 Q2133.34 715.643 2133.34 713.768 Q2133.34 712.032 2132.16 711.106 Q2131 710.18 2127.04 709.323 L2125.56 708.976 Q2121.6 708.143 2119.85 706.43 Q2118.09 704.694 2118.09 701.685 Q2118.09 698.027 2120.68 696.036 Q2123.27 694.046 2128.04 694.046 Q2130.4 694.046 2132.48 694.393 Q2134.57 694.74 2136.33 695.435 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2144.5 694.671 L2148.76 694.671 L2148.76 720.596 L2144.5 720.596 L2144.5 694.671 M2144.5 684.578 L2148.76 684.578 L2148.76 689.972 L2144.5 689.972 L2144.5 684.578 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2167.72 697.657 Q2164.29 697.657 2162.3 700.342 Q2160.31 703.004 2160.31 707.657 Q2160.31 712.31 2162.28 714.995 Q2164.27 717.657 2167.72 717.657 Q2171.12 717.657 2173.11 714.972 Q2175.1 712.286 2175.1 707.657 Q2175.1 703.05 2173.11 700.365 Q2171.12 697.657 2167.72 697.657 M2167.72 694.046 Q2173.27 694.046 2176.44 697.657 Q2179.61 701.268 2179.61 707.657 Q2179.61 714.022 2176.44 717.657 Q2173.27 721.268 2167.72 721.268 Q2162.14 721.268 2158.97 717.657 Q2155.82 714.022 2155.82 707.657 Q2155.82 701.268 2158.97 697.657 Q2162.14 694.046 2167.72 694.046 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2208.22 704.948 L2208.22 720.596 L2203.97 720.596 L2203.97 705.087 Q2203.97 701.407 2202.53 699.578 Q2201.09 697.749 2198.22 697.749 Q2194.78 697.749 2192.78 699.948 Q2190.79 702.148 2190.79 705.944 L2190.79 720.596 L2186.51 720.596 L2186.51 694.671 L2190.79 694.671 L2190.79 698.698 Q2192.32 696.361 2194.38 695.203 Q2196.47 694.046 2199.17 694.046 Q2203.64 694.046 2205.93 696.823 Q2208.22 699.578 2208.22 704.948 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip780)\" style=\"stroke:#c67812; stroke-linecap:round; stroke-linejoin:round; stroke-width:12; stroke-opacity:1; fill:none\" points=\"1839.49,755.156 1981.68,755.156 \"/>\n",
       "<path clip-path=\"url(#clip780)\" d=\"M2021.77 756.233 Q2023.27 756.742 2024.68 758.409 Q2026.12 760.075 2027.55 762.992 L2032.3 772.436 L2027.28 772.436 L2022.86 763.571 Q2021.14 760.099 2019.52 758.964 Q2017.92 757.83 2015.15 757.83 L2010.05 757.83 L2010.05 772.436 L2005.38 772.436 L2005.38 737.876 L2015.93 737.876 Q2021.86 737.876 2024.78 740.353 Q2027.69 742.83 2027.69 747.83 Q2027.69 751.094 2026.17 753.247 Q2024.66 755.4 2021.77 756.233 M2010.05 741.719 L2010.05 753.988 L2015.93 753.988 Q2019.31 753.988 2021.03 752.437 Q2022.76 750.863 2022.76 747.83 Q2022.76 744.798 2021.03 743.27 Q2019.31 741.719 2015.93 741.719 L2010.05 741.719 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2058.18 758.409 L2058.18 760.492 L2038.6 760.492 Q2038.87 764.89 2041.23 767.205 Q2043.62 769.497 2047.85 769.497 Q2050.31 769.497 2052.6 768.895 Q2054.92 768.293 2057.18 767.089 L2057.18 771.117 Q2054.89 772.089 2052.48 772.599 Q2050.08 773.108 2047.6 773.108 Q2041.4 773.108 2037.76 769.497 Q2034.15 765.886 2034.15 759.728 Q2034.15 753.363 2037.58 749.636 Q2041.03 745.886 2046.86 745.886 Q2052.09 745.886 2055.12 749.265 Q2058.18 752.622 2058.18 758.409 M2053.92 757.159 Q2053.87 753.663 2051.95 751.58 Q2050.05 749.497 2046.91 749.497 Q2043.34 749.497 2041.19 751.511 Q2039.06 753.525 2038.73 757.182 L2053.92 757.159 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2083.83 747.506 L2083.83 751.488 Q2082.02 750.492 2080.19 750.006 Q2078.39 749.497 2076.54 749.497 Q2072.39 749.497 2070.1 752.136 Q2067.81 754.751 2067.81 759.497 Q2067.81 764.242 2070.1 766.881 Q2072.39 769.497 2076.54 769.497 Q2078.39 769.497 2080.19 769.011 Q2082.02 768.501 2083.83 767.506 L2083.83 771.441 Q2082.04 772.274 2080.12 772.691 Q2078.23 773.108 2076.07 773.108 Q2070.22 773.108 2066.77 769.427 Q2063.32 765.747 2063.32 759.497 Q2063.32 753.154 2066.79 749.52 Q2070.29 745.886 2076.35 745.886 Q2078.32 745.886 2080.19 746.302 Q2082.07 746.696 2083.83 747.506 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2103.02 759.404 Q2097.85 759.404 2095.86 760.585 Q2093.87 761.765 2093.87 764.612 Q2093.87 766.881 2095.35 768.224 Q2096.86 769.543 2099.43 769.543 Q2102.97 769.543 2105.1 767.043 Q2107.25 764.52 2107.25 760.353 L2107.25 759.404 L2103.02 759.404 M2111.51 757.645 L2111.51 772.436 L2107.25 772.436 L2107.25 768.501 Q2105.79 770.862 2103.62 771.997 Q2101.44 773.108 2098.29 773.108 Q2094.31 773.108 2091.95 770.886 Q2089.61 768.64 2089.61 764.89 Q2089.61 760.515 2092.53 758.293 Q2095.47 756.071 2101.28 756.071 L2107.25 756.071 L2107.25 755.654 Q2107.25 752.714 2105.31 751.117 Q2103.39 749.497 2099.89 749.497 Q2097.67 749.497 2095.56 750.029 Q2093.46 750.562 2091.51 751.626 L2091.51 747.691 Q2093.85 746.788 2096.05 746.349 Q2098.25 745.886 2100.33 745.886 Q2105.96 745.886 2108.73 748.802 Q2111.51 751.719 2111.51 757.645 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2120.29 736.418 L2124.54 736.418 L2124.54 772.436 L2120.29 772.436 L2120.29 736.418 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2133.46 736.418 L2137.72 736.418 L2137.72 772.436 L2133.46 772.436 L2133.46 736.418 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip780)\" style=\"stroke:#a9a9a9; stroke-linecap:round; stroke-linejoin:round; stroke-width:12; stroke-opacity:1; fill:none\" stroke-dasharray=\"48, 30\" points=\"1839.49,806.996 1981.68,806.996 \"/>\n",
       "<path clip-path=\"url(#clip780)\" d=\"M2033.25 792.379 L2033.25 797.309 Q2030.89 795.11 2028.2 794.022 Q2025.54 792.934 2022.53 792.934 Q2016.61 792.934 2013.46 796.568 Q2010.31 800.179 2010.31 807.031 Q2010.31 813.86 2013.46 817.494 Q2016.61 821.105 2022.53 821.105 Q2025.54 821.105 2028.2 820.017 Q2030.89 818.929 2033.25 816.73 L2033.25 821.614 Q2030.79 823.281 2028.04 824.114 Q2025.31 824.948 2022.25 824.948 Q2014.41 824.948 2009.89 820.156 Q2005.38 815.341 2005.38 807.031 Q2005.38 798.698 2009.89 793.906 Q2014.41 789.091 2022.25 789.091 Q2025.36 789.091 2028.09 789.925 Q2030.84 790.735 2033.25 792.379 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2040.29 788.258 L2044.54 788.258 L2044.54 824.276 L2040.29 824.276 L2040.29 788.258 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2065.24 811.244 Q2060.08 811.244 2058.09 812.425 Q2056.1 813.605 2056.1 816.452 Q2056.1 818.721 2057.58 820.064 Q2059.08 821.383 2061.65 821.383 Q2065.19 821.383 2067.32 818.883 Q2069.48 816.36 2069.48 812.193 L2069.48 811.244 L2065.24 811.244 M2073.73 809.485 L2073.73 824.276 L2069.48 824.276 L2069.48 820.341 Q2068.02 822.702 2065.84 823.837 Q2063.67 824.948 2060.52 824.948 Q2056.54 824.948 2054.17 822.726 Q2051.84 820.48 2051.84 816.73 Q2051.84 812.355 2054.75 810.133 Q2057.69 807.911 2063.5 807.911 L2069.48 807.911 L2069.48 807.494 Q2069.48 804.554 2067.53 802.957 Q2065.61 801.337 2062.11 801.337 Q2059.89 801.337 2057.79 801.869 Q2055.68 802.402 2053.73 803.466 L2053.73 799.531 Q2056.07 798.628 2058.27 798.189 Q2060.47 797.726 2062.55 797.726 Q2068.18 797.726 2070.96 800.642 Q2073.73 803.559 2073.73 809.485 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2099.04 799.115 L2099.04 803.142 Q2097.23 802.216 2095.29 801.753 Q2093.34 801.291 2091.26 801.291 Q2088.09 801.291 2086.49 802.263 Q2084.91 803.235 2084.91 805.179 Q2084.91 806.661 2086.05 807.517 Q2087.18 808.351 2090.61 809.115 L2092.07 809.439 Q2096.6 810.411 2098.5 812.193 Q2100.42 813.952 2100.42 817.124 Q2100.42 820.735 2097.55 822.841 Q2094.71 824.948 2089.71 824.948 Q2087.62 824.948 2085.35 824.531 Q2083.11 824.138 2080.61 823.327 L2080.61 818.929 Q2082.97 820.156 2085.26 820.781 Q2087.55 821.383 2089.8 821.383 Q2092.81 821.383 2094.43 820.364 Q2096.05 819.323 2096.05 817.448 Q2096.05 815.712 2094.87 814.786 Q2093.71 813.86 2089.75 813.003 L2088.27 812.656 Q2084.31 811.823 2082.55 810.11 Q2080.79 808.374 2080.79 805.365 Q2080.79 801.707 2083.39 799.716 Q2085.98 797.726 2090.75 797.726 Q2093.11 797.726 2095.19 798.073 Q2097.28 798.42 2099.04 799.115 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2123.73 799.115 L2123.73 803.142 Q2121.93 802.216 2119.98 801.753 Q2118.04 801.291 2115.96 801.291 Q2112.79 801.291 2111.19 802.263 Q2109.61 803.235 2109.61 805.179 Q2109.61 806.661 2110.75 807.517 Q2111.88 808.351 2115.31 809.115 L2116.77 809.439 Q2121.3 810.411 2123.2 812.193 Q2125.12 813.952 2125.12 817.124 Q2125.12 820.735 2122.25 822.841 Q2119.41 824.948 2114.41 824.948 Q2112.32 824.948 2110.05 824.531 Q2107.81 824.138 2105.31 823.327 L2105.31 818.929 Q2107.67 820.156 2109.96 820.781 Q2112.25 821.383 2114.5 821.383 Q2117.51 821.383 2119.13 820.364 Q2120.75 819.323 2120.75 817.448 Q2120.75 815.712 2119.57 814.786 Q2118.41 813.86 2114.45 813.003 L2112.97 812.656 Q2109.01 811.823 2107.25 810.11 Q2105.49 808.374 2105.49 805.365 Q2105.49 801.707 2108.09 799.716 Q2110.68 797.726 2115.45 797.726 Q2117.81 797.726 2119.89 798.073 Q2121.97 798.42 2123.73 799.115 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2151.84 807.772 L2151.84 820.434 L2159.34 820.434 Q2163.11 820.434 2164.91 818.883 Q2166.74 817.309 2166.74 814.091 Q2166.74 810.851 2164.91 809.323 Q2163.11 807.772 2159.34 807.772 L2151.84 807.772 M2151.84 793.559 L2151.84 803.976 L2158.76 803.976 Q2162.18 803.976 2163.85 802.703 Q2165.54 801.406 2165.54 798.767 Q2165.54 796.152 2163.85 794.855 Q2162.18 793.559 2158.76 793.559 L2151.84 793.559 M2147.16 789.716 L2159.1 789.716 Q2164.45 789.716 2167.35 791.939 Q2170.24 794.161 2170.24 798.258 Q2170.24 801.429 2168.76 803.304 Q2167.28 805.179 2164.41 805.642 Q2167.85 806.383 2169.75 808.744 Q2171.67 811.082 2171.67 814.601 Q2171.67 819.23 2168.53 821.753 Q2165.38 824.276 2159.57 824.276 L2147.16 824.276 L2147.16 789.716 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2191.28 811.244 Q2186.12 811.244 2184.13 812.425 Q2182.14 813.605 2182.14 816.452 Q2182.14 818.721 2183.62 820.064 Q2185.12 821.383 2187.69 821.383 Q2191.23 821.383 2193.36 818.883 Q2195.52 816.36 2195.52 812.193 L2195.52 811.244 L2191.28 811.244 M2199.78 809.485 L2199.78 824.276 L2195.52 824.276 L2195.52 820.341 Q2194.06 822.702 2191.88 823.837 Q2189.71 824.948 2186.56 824.948 Q2182.58 824.948 2180.22 822.726 Q2177.88 820.48 2177.88 816.73 Q2177.88 812.355 2180.79 810.133 Q2183.73 807.911 2189.54 807.911 L2195.52 807.911 L2195.52 807.494 Q2195.52 804.554 2193.57 802.957 Q2191.65 801.337 2188.16 801.337 Q2185.93 801.337 2183.83 801.869 Q2181.72 802.402 2179.78 803.466 L2179.78 799.531 Q2182.11 798.628 2184.31 798.189 Q2186.51 797.726 2188.59 797.726 Q2194.22 797.726 2197 800.642 Q2199.78 803.559 2199.78 809.485 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2208.55 788.258 L2212.81 788.258 L2212.81 824.276 L2208.55 824.276 L2208.55 788.258 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2233.5 811.244 Q2228.34 811.244 2226.35 812.425 Q2224.36 813.605 2224.36 816.452 Q2224.36 818.721 2225.84 820.064 Q2227.34 821.383 2229.91 821.383 Q2233.46 821.383 2235.59 818.883 Q2237.74 816.36 2237.74 812.193 L2237.74 811.244 L2233.5 811.244 M2242 809.485 L2242 824.276 L2237.74 824.276 L2237.74 820.341 Q2236.28 822.702 2234.1 823.837 Q2231.93 824.948 2228.78 824.948 Q2224.8 824.948 2222.44 822.726 Q2220.1 820.48 2220.1 816.73 Q2220.1 812.355 2223.02 810.133 Q2225.96 807.911 2231.77 807.911 L2237.74 807.911 L2237.74 807.494 Q2237.74 804.554 2235.79 802.957 Q2233.87 801.337 2230.38 801.337 Q2228.15 801.337 2226.05 801.869 Q2223.94 802.402 2222 803.466 L2222 799.531 Q2224.34 798.628 2226.53 798.189 Q2228.73 797.726 2230.82 797.726 Q2236.44 797.726 2239.22 800.642 Q2242 803.559 2242 809.485 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2272.32 808.628 L2272.32 824.276 L2268.06 824.276 L2268.06 808.767 Q2268.06 805.087 2266.63 803.258 Q2265.19 801.429 2262.32 801.429 Q2258.87 801.429 2256.88 803.628 Q2254.89 805.828 2254.89 809.624 L2254.89 824.276 L2250.61 824.276 L2250.61 798.351 L2254.89 798.351 L2254.89 802.378 Q2256.42 800.041 2258.48 798.883 Q2260.56 797.726 2263.27 797.726 Q2267.74 797.726 2270.03 800.503 Q2272.32 803.258 2272.32 808.628 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2299.47 799.346 L2299.47 803.328 Q2297.67 802.332 2295.84 801.846 Q2294.03 801.337 2292.18 801.337 Q2288.04 801.337 2285.75 803.976 Q2283.46 806.591 2283.46 811.337 Q2283.46 816.082 2285.75 818.721 Q2288.04 821.337 2292.18 821.337 Q2294.03 821.337 2295.84 820.851 Q2297.67 820.341 2299.47 819.346 L2299.47 823.281 Q2297.69 824.114 2295.77 824.531 Q2293.87 824.948 2291.72 824.948 Q2285.86 824.948 2282.41 821.267 Q2278.96 817.587 2278.96 811.337 Q2278.96 804.994 2282.44 801.36 Q2285.93 797.726 2292 797.726 Q2293.96 797.726 2295.84 798.142 Q2297.71 798.536 2299.47 799.346 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2329.06 810.249 L2329.06 812.332 L2309.47 812.332 Q2309.75 816.73 2312.11 819.045 Q2314.5 821.337 2318.73 821.337 Q2321.19 821.337 2323.48 820.735 Q2325.79 820.133 2328.06 818.929 L2328.06 822.957 Q2325.77 823.929 2323.36 824.439 Q2320.96 824.948 2318.48 824.948 Q2312.27 824.948 2308.64 821.337 Q2305.03 817.726 2305.03 811.568 Q2305.03 805.203 2308.46 801.476 Q2311.9 797.726 2317.74 797.726 Q2322.97 797.726 2326 801.105 Q2329.06 804.462 2329.06 810.249 M2324.8 808.999 Q2324.75 805.503 2322.83 803.42 Q2320.93 801.337 2317.78 801.337 Q2314.22 801.337 2312.07 803.351 Q2309.94 805.365 2309.61 809.022 L2324.8 808.999 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /></svg>\n"
      ],
      "text/html": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip830\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<defs>\n",
       "  <clipPath id=\"clip831\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<defs>\n",
       "  <clipPath id=\"clip832\">\n",
       "    <rect x=\"219\" y=\"47\" width=\"1526\" height=\"1377\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip832)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"468.524,1423.18 468.524,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip832)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"879.528,1423.18 879.528,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip832)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1290.53,1423.18 1290.53,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip832)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1701.54,1423.18 1701.54,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,1423.18 1744.69,1423.18 \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"468.524,1423.18 468.524,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"879.528,1423.18 879.528,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1290.53,1423.18 1290.53,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1701.54,1423.18 1701.54,1404.28 \"/>\n",
       "<path clip-path=\"url(#clip830)\" d=\"M446.707 1454.1 Q443.096 1454.1 441.267 1457.66 Q439.461 1461.2 439.461 1468.33 Q439.461 1475.44 441.267 1479.01 Q443.096 1482.55 446.707 1482.55 Q450.341 1482.55 452.147 1479.01 Q453.975 1475.44 453.975 1468.33 Q453.975 1461.2 452.147 1457.66 Q450.341 1454.1 446.707 1454.1 M446.707 1450.39 Q452.517 1450.39 455.572 1455 Q458.651 1459.58 458.651 1468.33 Q458.651 1477.06 455.572 1481.67 Q452.517 1486.25 446.707 1486.25 Q440.897 1486.25 437.818 1481.67 Q434.762 1477.06 434.762 1468.33 Q434.762 1459.58 437.818 1455 Q440.897 1450.39 446.707 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M466.869 1479.7 L471.753 1479.7 L471.753 1485.58 L466.869 1485.58 L466.869 1479.7 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M485.966 1481.64 L502.285 1481.64 L502.285 1485.58 L480.341 1485.58 L480.341 1481.64 Q483.003 1478.89 487.586 1474.26 Q492.193 1469.61 493.373 1468.27 Q495.619 1465.74 496.498 1464.01 Q497.401 1462.25 497.401 1460.56 Q497.401 1457.8 495.457 1456.07 Q493.535 1454.33 490.433 1454.33 Q488.234 1454.33 485.781 1455.09 Q483.35 1455.86 480.572 1457.41 L480.572 1452.69 Q483.396 1451.55 485.85 1450.97 Q488.304 1450.39 490.341 1450.39 Q495.711 1450.39 498.906 1453.08 Q502.1 1455.77 502.1 1460.26 Q502.1 1462.39 501.29 1464.31 Q500.503 1466.2 498.396 1468.8 Q497.818 1469.47 494.716 1472.69 Q491.614 1475.88 485.966 1481.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M856.669 1454.1 Q853.058 1454.1 851.23 1457.66 Q849.424 1461.2 849.424 1468.33 Q849.424 1475.44 851.23 1479.01 Q853.058 1482.55 856.669 1482.55 Q860.304 1482.55 862.109 1479.01 Q863.938 1475.44 863.938 1468.33 Q863.938 1461.2 862.109 1457.66 Q860.304 1454.1 856.669 1454.1 M856.669 1450.39 Q862.479 1450.39 865.535 1455 Q868.614 1459.58 868.614 1468.33 Q868.614 1477.06 865.535 1481.67 Q862.479 1486.25 856.669 1486.25 Q850.859 1486.25 847.78 1481.67 Q844.725 1477.06 844.725 1468.33 Q844.725 1459.58 847.78 1455 Q850.859 1450.39 856.669 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M876.831 1479.7 L881.715 1479.7 L881.715 1485.58 L876.831 1485.58 L876.831 1479.7 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M904.748 1455.09 L892.942 1473.54 L904.748 1473.54 L904.748 1455.09 M903.521 1451.02 L909.401 1451.02 L909.401 1473.54 L914.331 1473.54 L914.331 1477.43 L909.401 1477.43 L909.401 1485.58 L904.748 1485.58 L904.748 1477.43 L889.146 1477.43 L889.146 1472.92 L903.521 1451.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M1267.84 1454.1 Q1264.22 1454.1 1262.4 1457.66 Q1260.59 1461.2 1260.59 1468.33 Q1260.59 1475.44 1262.4 1479.01 Q1264.22 1482.55 1267.84 1482.55 Q1271.47 1482.55 1273.28 1479.01 Q1275.1 1475.44 1275.1 1468.33 Q1275.1 1461.2 1273.28 1457.66 Q1271.47 1454.1 1267.84 1454.1 M1267.84 1450.39 Q1273.65 1450.39 1276.7 1455 Q1279.78 1459.58 1279.78 1468.33 Q1279.78 1477.06 1276.7 1481.67 Q1273.65 1486.25 1267.84 1486.25 Q1262.03 1486.25 1258.95 1481.67 Q1255.89 1477.06 1255.89 1468.33 Q1255.89 1459.58 1258.95 1455 Q1262.03 1450.39 1267.84 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M1288 1479.7 L1292.88 1479.7 L1292.88 1485.58 L1288 1485.58 L1288 1479.7 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M1313.65 1466.44 Q1310.5 1466.44 1308.65 1468.59 Q1306.82 1470.74 1306.82 1474.49 Q1306.82 1478.22 1308.65 1480.39 Q1310.5 1482.55 1313.65 1482.55 Q1316.79 1482.55 1318.62 1480.39 Q1320.47 1478.22 1320.47 1474.49 Q1320.47 1470.74 1318.62 1468.59 Q1316.79 1466.44 1313.65 1466.44 M1322.93 1451.78 L1322.93 1456.04 Q1321.17 1455.21 1319.36 1454.77 Q1317.58 1454.33 1315.82 1454.33 Q1311.19 1454.33 1308.74 1457.45 Q1306.31 1460.58 1305.96 1466.9 Q1307.33 1464.89 1309.39 1463.82 Q1311.45 1462.73 1313.92 1462.73 Q1319.13 1462.73 1322.14 1465.9 Q1325.17 1469.05 1325.17 1474.49 Q1325.17 1479.82 1322.03 1483.03 Q1318.88 1486.25 1313.65 1486.25 Q1307.65 1486.25 1304.48 1481.67 Q1301.31 1477.06 1301.31 1468.33 Q1301.31 1460.14 1305.2 1455.28 Q1309.09 1450.39 1315.64 1450.39 Q1317.4 1450.39 1319.18 1450.74 Q1320.98 1451.09 1322.93 1451.78 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M1678.97 1454.1 Q1675.36 1454.1 1673.53 1457.66 Q1671.72 1461.2 1671.72 1468.33 Q1671.72 1475.44 1673.53 1479.01 Q1675.36 1482.55 1678.97 1482.55 Q1682.6 1482.55 1684.41 1479.01 Q1686.24 1475.44 1686.24 1468.33 Q1686.24 1461.2 1684.41 1457.66 Q1682.6 1454.1 1678.97 1454.1 M1678.97 1450.39 Q1684.78 1450.39 1687.83 1455 Q1690.91 1459.58 1690.91 1468.33 Q1690.91 1477.06 1687.83 1481.67 Q1684.78 1486.25 1678.97 1486.25 Q1673.16 1486.25 1670.08 1481.67 Q1667.02 1477.06 1667.02 1468.33 Q1667.02 1459.58 1670.08 1455 Q1673.16 1450.39 1678.97 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M1699.13 1479.7 L1704.01 1479.7 L1704.01 1485.58 L1699.13 1485.58 L1699.13 1479.7 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M1724.2 1469.17 Q1720.86 1469.17 1718.94 1470.95 Q1717.05 1472.73 1717.05 1475.86 Q1717.05 1478.98 1718.94 1480.77 Q1720.86 1482.55 1724.2 1482.55 Q1727.53 1482.55 1729.45 1480.77 Q1731.37 1478.96 1731.37 1475.86 Q1731.37 1472.73 1729.45 1470.95 Q1727.55 1469.17 1724.2 1469.17 M1719.52 1467.18 Q1716.51 1466.44 1714.82 1464.38 Q1713.16 1462.32 1713.16 1459.35 Q1713.16 1455.21 1716.1 1452.8 Q1719.06 1450.39 1724.2 1450.39 Q1729.36 1450.39 1732.3 1452.8 Q1735.24 1455.21 1735.24 1459.35 Q1735.24 1462.32 1733.55 1464.38 Q1731.88 1466.44 1728.9 1467.18 Q1732.28 1467.96 1734.15 1470.26 Q1736.05 1472.55 1736.05 1475.86 Q1736.05 1480.88 1732.97 1483.57 Q1729.92 1486.25 1724.2 1486.25 Q1718.48 1486.25 1715.4 1483.57 Q1712.35 1480.88 1712.35 1475.86 Q1712.35 1472.55 1714.24 1470.26 Q1716.14 1467.96 1719.52 1467.18 M1717.81 1459.79 Q1717.81 1462.48 1719.48 1463.98 Q1721.17 1465.49 1724.2 1465.49 Q1727.21 1465.49 1728.9 1463.98 Q1730.61 1462.48 1730.61 1459.79 Q1730.61 1457.11 1728.9 1455.6 Q1727.21 1454.1 1724.2 1454.1 Q1721.17 1454.1 1719.48 1455.6 Q1717.81 1457.11 1717.81 1459.79 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M824.521 1520.52 L864.72 1520.52 L864.72 1525.93 L847.851 1525.93 L847.851 1568.04 L841.39 1568.04 L841.39 1525.93 L824.521 1525.93 L824.521 1520.52 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M900.305 1546.53 L900.305 1568.04 L894.448 1568.04 L894.448 1546.72 Q894.448 1541.66 892.475 1539.14 Q890.501 1536.63 886.555 1536.63 Q881.812 1536.63 879.075 1539.65 Q876.338 1542.68 876.338 1547.9 L876.338 1568.04 L870.449 1568.04 L870.449 1518.52 L876.338 1518.52 L876.338 1537.93 Q878.438 1534.72 881.271 1533.13 Q884.136 1531.54 887.86 1531.54 Q894.003 1531.54 897.154 1535.36 Q900.305 1539.14 900.305 1546.53 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M932.642 1537.87 Q931.656 1537.3 930.478 1537.04 Q929.332 1536.76 927.932 1536.76 Q922.967 1536.76 920.293 1540 Q917.651 1543.22 917.651 1549.27 L917.651 1568.04 L911.763 1568.04 L911.763 1532.4 L917.651 1532.4 L917.651 1537.93 Q919.497 1534.69 922.457 1533.13 Q925.417 1531.54 929.651 1531.54 Q930.255 1531.54 930.987 1531.63 Q931.719 1531.7 932.611 1531.85 L932.642 1537.87 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M967.845 1548.76 L967.845 1551.62 L940.918 1551.62 Q941.3 1557.67 944.546 1560.85 Q947.825 1564 953.649 1564 Q957.023 1564 960.174 1563.17 Q963.357 1562.35 966.476 1560.69 L966.476 1566.23 Q963.325 1567.57 960.015 1568.27 Q956.705 1568.97 953.299 1568.97 Q944.769 1568.97 939.772 1564 Q934.807 1559.04 934.807 1550.57 Q934.807 1541.82 939.517 1536.69 Q944.26 1531.54 952.281 1531.54 Q959.474 1531.54 963.643 1536.18 Q967.845 1540.8 967.845 1548.76 M961.988 1547.04 Q961.925 1542.23 959.283 1539.37 Q956.673 1536.5 952.344 1536.5 Q947.443 1536.5 944.483 1539.27 Q941.554 1542.04 941.109 1547.07 L961.988 1547.04 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M1000.18 1533.45 L1000.18 1538.98 Q997.7 1537.71 995.026 1537.07 Q992.353 1536.44 989.488 1536.44 Q985.128 1536.44 982.932 1537.77 Q980.767 1539.11 980.767 1541.79 Q980.767 1543.82 982.327 1545 Q983.886 1546.15 988.597 1547.2 L990.602 1547.64 Q996.841 1548.98 999.451 1551.43 Q1002.09 1553.85 1002.09 1558.21 Q1002.09 1563.17 998.146 1566.07 Q994.231 1568.97 987.356 1568.97 Q984.491 1568.97 981.372 1568.39 Q978.285 1567.85 974.847 1566.74 L974.847 1560.69 Q978.094 1562.38 981.245 1563.24 Q984.396 1564.07 987.483 1564.07 Q991.621 1564.07 993.849 1562.66 Q996.077 1561.23 996.077 1558.65 Q996.077 1556.27 994.453 1554.99 Q992.862 1553.72 987.419 1552.54 L985.382 1552.07 Q979.94 1550.92 977.521 1548.56 Q975.102 1546.18 975.102 1542.04 Q975.102 1537.01 978.666 1534.27 Q982.231 1531.54 988.788 1531.54 Q992.034 1531.54 994.899 1532.01 Q997.764 1532.49 1000.18 1533.45 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M1041.05 1546.53 L1041.05 1568.04 L1035.19 1568.04 L1035.19 1546.72 Q1035.19 1541.66 1033.22 1539.14 Q1031.25 1536.63 1027.3 1536.63 Q1022.56 1536.63 1019.82 1539.65 Q1017.08 1542.68 1017.08 1547.9 L1017.08 1568.04 L1011.2 1568.04 L1011.2 1518.52 L1017.08 1518.52 L1017.08 1537.93 Q1019.18 1534.72 1022.02 1533.13 Q1024.88 1531.54 1028.61 1531.54 Q1034.75 1531.54 1037.9 1535.36 Q1041.05 1539.14 1041.05 1546.53 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M1066.55 1536.5 Q1061.83 1536.5 1059.1 1540.19 Q1056.36 1543.85 1056.36 1550.25 Q1056.36 1556.65 1059.07 1560.34 Q1061.8 1564 1066.55 1564 Q1071.22 1564 1073.96 1560.31 Q1076.7 1556.62 1076.7 1550.25 Q1076.7 1543.92 1073.96 1540.23 Q1071.22 1536.5 1066.55 1536.5 M1066.55 1531.54 Q1074.18 1531.54 1078.54 1536.5 Q1082.9 1541.47 1082.9 1550.25 Q1082.9 1559 1078.54 1564 Q1074.18 1568.97 1066.55 1568.97 Q1058.87 1568.97 1054.51 1564 Q1050.19 1559 1050.19 1550.25 Q1050.19 1541.47 1054.51 1536.5 Q1058.87 1531.54 1066.55 1531.54 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M1092.61 1518.52 L1098.47 1518.52 L1098.47 1568.04 L1092.61 1568.04 L1092.61 1518.52 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M1134.18 1537.81 L1134.18 1518.52 L1140.04 1518.52 L1140.04 1568.04 L1134.18 1568.04 L1134.18 1562.7 Q1132.33 1565.88 1129.5 1567.44 Q1126.7 1568.97 1122.75 1568.97 Q1116.29 1568.97 1112.22 1563.81 Q1108.18 1558.65 1108.18 1550.25 Q1108.18 1541.85 1112.22 1536.69 Q1116.29 1531.54 1122.75 1531.54 Q1126.7 1531.54 1129.5 1533.1 Q1132.33 1534.62 1134.18 1537.81 M1114.22 1550.25 Q1114.22 1556.71 1116.87 1560.4 Q1119.54 1564.07 1124.19 1564.07 Q1128.83 1564.07 1131.51 1560.4 Q1134.18 1556.71 1134.18 1550.25 Q1134.18 1543.79 1131.51 1540.13 Q1128.83 1536.44 1124.19 1536.44 Q1119.54 1536.44 1116.87 1540.13 Q1114.22 1543.79 1114.22 1550.25 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip832)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"219.866,1176.67 1744.69,1176.67 \"/>\n",
       "<polyline clip-path=\"url(#clip832)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"219.866,884.313 1744.69,884.313 \"/>\n",
       "<polyline clip-path=\"url(#clip832)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"219.866,591.958 1744.69,591.958 \"/>\n",
       "<polyline clip-path=\"url(#clip832)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"219.866,299.604 1744.69,299.604 \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,1423.18 219.866,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,1176.67 233.376,1176.67 \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,884.313 233.376,884.313 \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,591.958 233.376,591.958 \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,299.604 233.376,299.604 \"/>\n",
       "<path clip-path=\"url(#clip830)\" d=\"M128.288 1162.47 Q124.677 1162.47 122.848 1166.03 Q121.043 1169.57 121.043 1176.7 Q121.043 1183.81 122.848 1187.37 Q124.677 1190.91 128.288 1190.91 Q131.922 1190.91 133.728 1187.37 Q135.556 1183.81 135.556 1176.7 Q135.556 1169.57 133.728 1166.03 Q131.922 1162.47 128.288 1162.47 M128.288 1158.76 Q134.098 1158.76 137.154 1163.37 Q140.232 1167.95 140.232 1176.7 Q140.232 1185.43 137.154 1190.03 Q134.098 1194.62 128.288 1194.62 Q122.478 1194.62 119.399 1190.03 Q116.343 1185.43 116.343 1176.7 Q116.343 1167.95 119.399 1163.37 Q122.478 1158.76 128.288 1158.76 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M148.45 1188.07 L153.334 1188.07 L153.334 1193.95 L148.45 1193.95 L148.45 1188.07 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M167.547 1190.01 L183.866 1190.01 L183.866 1193.95 L161.922 1193.95 L161.922 1190.01 Q164.584 1187.26 169.167 1182.63 Q173.774 1177.97 174.954 1176.63 Q177.2 1174.11 178.079 1172.37 Q178.982 1170.61 178.982 1168.92 Q178.982 1166.17 177.038 1164.43 Q175.116 1162.7 172.014 1162.7 Q169.815 1162.7 167.362 1163.46 Q164.931 1164.22 162.153 1165.78 L162.153 1161.05 Q164.977 1159.92 167.431 1159.34 Q169.885 1158.76 171.922 1158.76 Q177.292 1158.76 180.487 1161.45 Q183.681 1164.13 183.681 1168.62 Q183.681 1170.75 182.871 1172.67 Q182.084 1174.57 179.977 1177.16 Q179.399 1177.84 176.297 1181.05 Q173.195 1184.25 167.547 1190.01 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M126.205 870.111 Q122.593 870.111 120.765 873.676 Q118.959 877.218 118.959 884.347 Q118.959 891.454 120.765 895.019 Q122.593 898.56 126.205 898.56 Q129.839 898.56 131.644 895.019 Q133.473 891.454 133.473 884.347 Q133.473 877.218 131.644 873.676 Q129.839 870.111 126.205 870.111 M126.205 866.408 Q132.015 866.408 135.07 871.014 Q138.149 875.597 138.149 884.347 Q138.149 893.074 135.07 897.681 Q132.015 902.264 126.205 902.264 Q120.394 902.264 117.316 897.681 Q114.26 893.074 114.26 884.347 Q114.26 875.597 117.316 871.014 Q120.394 866.408 126.205 866.408 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M146.366 895.713 L151.251 895.713 L151.251 901.593 L146.366 901.593 L146.366 895.713 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M174.283 871.107 L162.477 889.556 L174.283 889.556 L174.283 871.107 M173.056 867.033 L178.936 867.033 L178.936 889.556 L183.866 889.556 L183.866 893.444 L178.936 893.444 L178.936 901.593 L174.283 901.593 L174.283 893.444 L158.681 893.444 L158.681 888.931 L173.056 867.033 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M126.529 577.757 Q122.918 577.757 121.089 581.322 Q119.283 584.864 119.283 591.993 Q119.283 599.1 121.089 602.664 Q122.918 606.206 126.529 606.206 Q130.163 606.206 131.968 602.664 Q133.797 599.1 133.797 591.993 Q133.797 584.864 131.968 581.322 Q130.163 577.757 126.529 577.757 M126.529 574.053 Q132.339 574.053 135.394 578.66 Q138.473 583.243 138.473 591.993 Q138.473 600.72 135.394 605.326 Q132.339 609.91 126.529 609.91 Q120.718 609.91 117.64 605.326 Q114.584 600.72 114.584 591.993 Q114.584 583.243 117.64 578.66 Q120.718 574.053 126.529 574.053 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M146.691 603.359 L151.575 603.359 L151.575 609.238 L146.691 609.238 L146.691 603.359 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M172.339 590.095 Q169.19 590.095 167.339 592.248 Q165.51 594.401 165.51 598.151 Q165.51 601.877 167.339 604.053 Q169.19 606.206 172.339 606.206 Q175.487 606.206 177.315 604.053 Q179.167 601.877 179.167 598.151 Q179.167 594.401 177.315 592.248 Q175.487 590.095 172.339 590.095 M181.621 575.442 L181.621 579.702 Q179.862 578.868 178.056 578.428 Q176.274 577.989 174.514 577.989 Q169.885 577.989 167.431 581.114 Q165.001 584.239 164.653 590.558 Q166.019 588.544 168.079 587.479 Q170.139 586.391 172.616 586.391 Q177.825 586.391 180.834 589.563 Q183.866 592.711 183.866 598.151 Q183.866 603.475 180.718 606.692 Q177.57 609.91 172.339 609.91 Q166.343 609.91 163.172 605.326 Q160.001 600.72 160.001 591.993 Q160.001 583.799 163.89 578.938 Q167.778 574.053 174.329 574.053 Q176.089 574.053 177.871 574.401 Q179.676 574.748 181.621 575.442 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M126.783 285.403 Q123.172 285.403 121.343 288.968 Q119.538 292.509 119.538 299.639 Q119.538 306.745 121.343 310.31 Q123.172 313.852 126.783 313.852 Q130.417 313.852 132.223 310.31 Q134.052 306.745 134.052 299.639 Q134.052 292.509 132.223 288.968 Q130.417 285.403 126.783 285.403 M126.783 281.699 Q132.593 281.699 135.649 286.306 Q138.728 290.889 138.728 299.639 Q138.728 308.366 135.649 312.972 Q132.593 317.556 126.783 317.556 Q120.973 317.556 117.894 312.972 Q114.839 308.366 114.839 299.639 Q114.839 290.889 117.894 286.306 Q120.973 281.699 126.783 281.699 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M146.945 311.005 L151.829 311.005 L151.829 316.884 L146.945 316.884 L146.945 311.005 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M172.014 300.472 Q168.681 300.472 166.76 302.255 Q164.862 304.037 164.862 307.162 Q164.862 310.287 166.76 312.069 Q168.681 313.852 172.014 313.852 Q175.348 313.852 177.269 312.069 Q179.19 310.264 179.19 307.162 Q179.19 304.037 177.269 302.255 Q175.371 300.472 172.014 300.472 M167.339 298.482 Q164.329 297.741 162.64 295.681 Q160.973 293.62 160.973 290.658 Q160.973 286.514 163.913 284.107 Q166.876 281.699 172.014 281.699 Q177.176 281.699 180.116 284.107 Q183.056 286.514 183.056 290.658 Q183.056 293.62 181.366 295.681 Q179.7 297.741 176.714 298.482 Q180.093 299.269 181.968 301.56 Q183.866 303.852 183.866 307.162 Q183.866 312.185 180.788 314.87 Q177.732 317.556 172.014 317.556 Q166.297 317.556 163.218 314.87 Q160.163 312.185 160.163 307.162 Q160.163 303.852 162.061 301.56 Q163.959 299.269 167.339 298.482 M165.626 291.097 Q165.626 293.783 167.292 295.287 Q168.982 296.792 172.014 296.792 Q175.024 296.792 176.714 295.287 Q178.426 293.783 178.426 291.097 Q178.426 288.412 176.714 286.908 Q175.024 285.403 172.014 285.403 Q168.982 285.403 167.292 286.908 Q165.626 288.412 165.626 291.097 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M21.7677 1047.23 L39.6235 1047.23 L39.6235 1039.14 Q39.6235 1034.65 37.3 1032.2 Q34.9765 1029.75 30.6797 1029.75 Q26.4147 1029.75 24.0912 1032.2 Q21.7677 1034.65 21.7677 1039.14 L21.7677 1047.23 M16.4842 1053.66 L16.4842 1039.14 Q16.4842 1031.15 20.1126 1027.08 Q23.7092 1022.97 30.6797 1022.97 Q37.7138 1022.97 41.3104 1027.08 Q44.907 1031.15 44.907 1039.14 L44.907 1047.23 L64.0042 1047.23 L64.0042 1053.66 L16.4842 1053.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M33.8307 995.092 Q33.2578 996.079 33.0032 997.256 Q32.7167 998.402 32.7167 999.803 Q32.7167 1004.77 35.9632 1007.44 Q39.1779 1010.08 45.2253 1010.08 L64.0042 1010.08 L64.0042 1015.97 L28.3562 1015.97 L28.3562 1010.08 L33.8944 1010.08 Q30.6479 1008.24 29.0883 1005.28 Q27.4968 1002.32 27.4968 998.084 Q27.4968 997.479 27.5923 996.747 Q27.656 996.015 27.8151 995.124 L33.8307 995.092 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M44.7161 959.89 L47.5806 959.89 L47.5806 986.817 Q53.6281 986.435 56.8109 983.188 Q59.9619 979.91 59.9619 974.085 Q59.9619 970.711 59.1344 967.56 Q58.3069 964.377 56.6518 961.258 L62.1899 961.258 Q63.5267 964.409 64.227 967.719 Q64.9272 971.03 64.9272 974.435 Q64.9272 982.965 59.9619 987.962 Q54.9967 992.928 46.5303 992.928 Q37.7774 992.928 32.6531 988.217 Q27.4968 983.475 27.4968 975.454 Q27.4968 968.26 32.1438 964.091 Q36.7589 959.89 44.7161 959.89 M42.9973 965.746 Q38.1912 965.81 35.3266 968.451 Q32.4621 971.061 32.4621 975.39 Q32.4621 980.292 35.2312 983.252 Q38.0002 986.18 43.0292 986.626 L42.9973 965.746 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M29.7248 924.624 L35.1993 924.624 Q33.8307 927.106 33.1623 929.621 Q32.4621 932.103 32.4621 934.65 Q32.4621 940.347 36.0905 943.498 Q39.6872 946.649 46.212 946.649 Q52.7369 946.649 56.3653 943.498 Q59.9619 940.347 59.9619 934.65 Q59.9619 932.103 59.2935 929.621 Q58.5933 927.106 57.2247 924.624 L62.6355 924.624 Q63.7814 927.074 64.3543 929.716 Q64.9272 932.326 64.9272 935.286 Q64.9272 943.339 59.8664 948.081 Q54.8057 952.824 46.212 952.824 Q37.491 952.824 32.4939 948.049 Q27.4968 943.243 27.4968 934.904 Q27.4968 932.199 28.0697 929.621 Q28.6108 927.043 29.7248 924.624 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M28.3562 914.438 L28.3562 908.582 L64.0042 908.582 L64.0042 914.438 L28.3562 914.438 M14.479 914.438 L14.479 908.582 L21.895 908.582 L21.895 914.438 L14.479 914.438 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M29.4065 873.602 L34.9447 873.602 Q33.6716 876.085 33.035 878.759 Q32.3984 881.432 32.3984 884.297 Q32.3984 888.657 33.7352 890.853 Q35.072 893.018 37.7456 893.018 Q39.7826 893.018 40.9603 891.458 Q42.1061 889.899 43.1565 885.188 L43.6021 883.183 Q44.9389 876.944 47.3897 874.334 Q49.8086 871.693 54.1691 871.693 Q59.1344 871.693 62.0308 875.639 Q64.9272 879.554 64.9272 886.429 Q64.9272 889.294 64.3543 892.413 Q63.8132 895.5 62.6992 898.938 L56.6518 898.938 Q58.3387 895.691 59.198 892.54 Q60.0256 889.389 60.0256 886.302 Q60.0256 882.164 58.6251 879.936 Q57.1929 877.708 54.6147 877.708 Q52.2276 877.708 50.9545 879.332 Q49.6813 880.923 48.5037 886.366 L48.0262 888.403 Q46.8804 893.845 44.5251 896.264 Q42.138 898.683 38.0002 898.683 Q32.9713 898.683 30.2341 895.119 Q27.4968 891.554 27.4968 884.997 Q27.4968 881.751 27.9743 878.886 Q28.4517 876.021 29.4065 873.602 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M28.3562 862.367 L28.3562 856.511 L64.0042 856.511 L64.0042 862.367 L28.3562 862.367 M14.479 862.367 L14.479 856.511 L21.895 856.511 L21.895 862.367 L14.479 862.367 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M32.4621 830.443 Q32.4621 835.154 36.1542 837.891 Q39.8145 840.628 46.212 840.628 Q52.6095 840.628 56.3017 837.923 Q59.9619 835.185 59.9619 830.443 Q59.9619 825.764 56.2698 823.027 Q52.5777 820.29 46.212 820.29 Q39.8781 820.29 36.186 823.027 Q32.4621 825.764 32.4621 830.443 M27.4968 830.443 Q27.4968 822.804 32.4621 818.444 Q37.4273 814.083 46.212 814.083 Q54.9649 814.083 59.9619 818.444 Q64.9272 822.804 64.9272 830.443 Q64.9272 838.114 59.9619 842.474 Q54.9649 846.803 46.212 846.803 Q37.4273 846.803 32.4621 842.474 Q27.4968 838.114 27.4968 830.443 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M42.4881 774.743 L64.0042 774.743 L64.0042 780.599 L42.679 780.599 Q37.6183 780.599 35.1038 782.573 Q32.5894 784.546 32.5894 788.493 Q32.5894 793.235 35.6131 795.973 Q38.6368 798.71 43.8567 798.71 L64.0042 798.71 L64.0042 804.598 L28.3562 804.598 L28.3562 798.71 L33.8944 798.71 Q30.6797 796.609 29.0883 793.776 Q27.4968 790.912 27.4968 787.188 Q27.4968 781.045 31.3163 777.894 Q35.1038 774.743 42.4881 774.743 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M46.0847 726.141 Q46.0847 733.239 47.7079 735.976 Q49.3312 738.713 53.2461 738.713 Q56.3653 738.713 58.2114 736.676 Q60.0256 734.607 60.0256 731.074 Q60.0256 726.204 56.5881 723.276 Q53.1188 720.316 47.3897 720.316 L46.0847 720.316 L46.0847 726.141 M43.6657 714.46 L64.0042 714.46 L64.0042 720.316 L58.5933 720.316 Q61.8398 722.321 63.3994 725.313 Q64.9272 728.305 64.9272 732.634 Q64.9272 738.108 61.8716 741.355 Q58.7843 744.57 53.6281 744.57 Q47.6125 744.57 44.5569 740.559 Q41.5014 736.517 41.5014 728.528 L41.5014 720.316 L40.9285 720.316 Q36.8862 720.316 34.6901 722.99 Q32.4621 725.632 32.4621 730.438 Q32.4621 733.493 33.1941 736.39 Q33.9262 739.286 35.3903 741.96 L29.9795 741.96 Q28.7381 738.745 28.1334 735.721 Q27.4968 732.698 27.4968 729.833 Q27.4968 722.099 31.5072 718.279 Q35.5176 714.46 43.6657 714.46 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M42.4881 672.764 L64.0042 672.764 L64.0042 678.621 L42.679 678.621 Q37.6183 678.621 35.1038 680.594 Q32.5894 682.568 32.5894 686.514 Q32.5894 691.257 35.6131 693.994 Q38.6368 696.731 43.8567 696.731 L64.0042 696.731 L64.0042 702.62 L28.3562 702.62 L28.3562 696.731 L33.8944 696.731 Q30.6797 694.631 29.0883 691.798 Q27.4968 688.933 27.4968 685.209 Q27.4968 679.066 31.3163 675.915 Q35.1038 672.764 42.4881 672.764 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M33.7671 637.626 L14.479 637.626 L14.479 631.769 L64.0042 631.769 L64.0042 637.626 L58.657 637.626 Q61.8398 639.472 63.3994 642.304 Q64.9272 645.105 64.9272 649.052 Q64.9272 655.513 59.771 659.587 Q54.6147 663.63 46.212 663.63 Q37.8093 663.63 32.6531 659.587 Q27.4968 655.513 27.4968 649.052 Q27.4968 645.105 29.0564 642.304 Q30.5842 639.472 33.7671 637.626 M46.212 657.582 Q52.6732 657.582 56.3653 654.94 Q60.0256 652.267 60.0256 647.62 Q60.0256 642.973 56.3653 640.299 Q52.6732 637.626 46.212 637.626 Q39.7508 637.626 36.0905 640.299 Q32.3984 642.973 32.3984 647.62 Q32.3984 652.267 36.0905 654.94 Q39.7508 657.582 46.212 657.582 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M41.7242 576.197 Q42.4244 574.128 44.7161 572.186 Q47.0077 570.213 51.0181 568.239 L64.0042 561.715 L64.0042 568.621 L51.8138 574.701 Q47.0395 577.056 45.48 579.284 Q43.9204 581.48 43.9204 585.3 L43.9204 592.302 L64.0042 592.302 L64.0042 598.731 L16.4842 598.731 L16.4842 584.217 Q16.4842 576.069 19.8898 572.059 Q23.2955 568.049 30.1704 568.049 Q34.6582 568.049 37.6183 570.149 Q40.5784 572.218 41.7242 576.197 M21.7677 592.302 L38.6368 592.302 L38.6368 584.217 Q38.6368 579.57 36.5043 577.215 Q34.34 574.828 30.1704 574.828 Q26.0009 574.828 23.9002 577.215 Q21.7677 579.57 21.7677 584.217 L21.7677 592.302 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M44.7161 526.13 L47.5806 526.13 L47.5806 553.057 Q53.6281 552.675 56.8109 549.429 Q59.9619 546.15 59.9619 540.326 Q59.9619 536.952 59.1344 533.801 Q58.3069 530.618 56.6518 527.499 L62.1899 527.499 Q63.5267 530.65 64.227 533.96 Q64.9272 537.27 64.9272 540.676 Q64.9272 549.206 59.9619 554.203 Q54.9967 559.168 46.5303 559.168 Q37.7774 559.168 32.6531 554.458 Q27.4968 549.715 27.4968 541.694 Q27.4968 534.501 32.1438 530.332 Q36.7589 526.13 44.7161 526.13 M42.9973 531.987 Q38.1912 532.05 35.3266 534.692 Q32.4621 537.302 32.4621 541.631 Q32.4621 546.532 35.2312 549.492 Q38.0002 552.421 43.0292 552.866 L42.9973 531.987 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M29.7248 490.864 L35.1993 490.864 Q33.8307 493.347 33.1623 495.861 Q32.4621 498.344 32.4621 500.89 Q32.4621 506.588 36.0905 509.739 Q39.6872 512.89 46.212 512.89 Q52.7369 512.89 56.3653 509.739 Q59.9619 506.588 59.9619 500.89 Q59.9619 498.344 59.2935 495.861 Q58.5933 493.347 57.2247 490.864 L62.6355 490.864 Q63.7814 493.315 64.3543 495.957 Q64.9272 498.567 64.9272 501.527 Q64.9272 509.579 59.8664 514.322 Q54.8057 519.064 46.212 519.064 Q37.491 519.064 32.4939 514.29 Q27.4968 509.484 27.4968 501.145 Q27.4968 498.44 28.0697 495.861 Q28.6108 493.283 29.7248 490.864 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M46.0847 464.478 Q46.0847 471.576 47.7079 474.313 Q49.3312 477.051 53.2461 477.051 Q56.3653 477.051 58.2114 475.014 Q60.0256 472.945 60.0256 469.412 Q60.0256 464.542 56.5881 461.614 Q53.1188 458.654 47.3897 458.654 L46.0847 458.654 L46.0847 464.478 M43.6657 452.797 L64.0042 452.797 L64.0042 458.654 L58.5933 458.654 Q61.8398 460.659 63.3994 463.651 Q64.9272 466.643 64.9272 470.971 Q64.9272 476.446 61.8716 479.693 Q58.7843 482.907 53.6281 482.907 Q47.6125 482.907 44.5569 478.897 Q41.5014 474.855 41.5014 466.866 L41.5014 458.654 L40.9285 458.654 Q36.8862 458.654 34.6901 461.327 Q32.4621 463.969 32.4621 468.775 Q32.4621 471.831 33.1941 474.727 Q33.9262 477.624 35.3903 480.297 L29.9795 480.297 Q28.7381 477.083 28.1334 474.059 Q27.4968 471.035 27.4968 468.171 Q27.4968 460.436 31.5072 456.617 Q35.5176 452.797 43.6657 452.797 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M14.479 440.734 L14.479 434.878 L64.0042 434.878 L64.0042 440.734 L14.479 440.734 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M14.479 422.624 L14.479 416.767 L64.0042 416.767 L64.0042 422.624 L14.479 422.624 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip832)\" style=\"stroke:#191919; stroke-linecap:round; stroke-linejoin:round; stroke-width:12; stroke-opacity:1; fill:none\" points=\"263.022,871.157 468.524,760.062 674.026,675.279 879.528,589.035 1085.03,489.634 1290.53,437.011 1496.03,349.304 1701.54,93.4945 \"/>\n",
       "<polyline clip-path=\"url(#clip832)\" style=\"stroke:#c67812; stroke-linecap:round; stroke-linejoin:round; stroke-width:12; stroke-opacity:1; fill:none\" points=\"263.022,86.1857 468.524,170.968 674.026,311.298 879.528,447.243 1085.03,596.344 1290.53,827.304 1496.03,1088.96 1701.54,1384.24 \"/>\n",
       "<polyline clip-path=\"url(#clip832)\" style=\"stroke:#a9a9a9; stroke-linecap:round; stroke-linejoin:round; stroke-width:12; stroke-opacity:1; fill:none\" stroke-dasharray=\"48, 30\" points=\"612.375,2799.12 612.375,-1328.69 \"/>\n",
       "<path clip-path=\"url(#clip830)\" d=\"M1815.79 858.836 L2352.76 858.836 L2352.76 651.476 L1815.79 651.476  Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"0\"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1815.79,858.836 2352.76,858.836 2352.76,651.476 1815.79,651.476 1815.79,858.836 \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#191919; stroke-linecap:round; stroke-linejoin:round; stroke-width:12; stroke-opacity:1; fill:none\" points=\"1839.49,703.316 1981.68,703.316 \"/>\n",
       "<path clip-path=\"url(#clip830)\" d=\"M2010.05 689.879 L2010.05 702.865 L2015.93 702.865 Q2019.2 702.865 2020.98 701.175 Q2022.76 699.485 2022.76 696.361 Q2022.76 693.259 2020.98 691.569 Q2019.2 689.879 2015.93 689.879 L2010.05 689.879 M2005.38 686.036 L2015.93 686.036 Q2021.74 686.036 2024.71 688.675 Q2027.69 691.291 2027.69 696.361 Q2027.69 701.476 2024.71 704.092 Q2021.74 706.708 2015.93 706.708 L2010.05 706.708 L2010.05 720.596 L2005.38 720.596 L2005.38 686.036 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M2047.97 698.652 Q2047.25 698.236 2046.4 698.05 Q2045.56 697.842 2044.54 697.842 Q2040.93 697.842 2038.99 700.203 Q2037.07 702.541 2037.07 706.939 L2037.07 720.596 L2032.79 720.596 L2032.79 694.671 L2037.07 694.671 L2037.07 698.698 Q2038.41 696.337 2040.56 695.203 Q2042.72 694.046 2045.79 694.046 Q2046.23 694.046 2046.77 694.115 Q2047.3 694.161 2047.95 694.277 L2047.97 698.652 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M2073.57 706.569 L2073.57 708.652 L2053.99 708.652 Q2054.27 713.05 2056.63 715.365 Q2059.01 717.657 2063.25 717.657 Q2065.7 717.657 2067.99 717.055 Q2070.31 716.453 2072.58 715.249 L2072.58 719.277 Q2070.29 720.249 2067.88 720.759 Q2065.47 721.268 2062.99 721.268 Q2056.79 721.268 2053.16 717.657 Q2049.54 714.046 2049.54 707.888 Q2049.54 701.523 2052.97 697.796 Q2056.42 694.046 2062.25 694.046 Q2067.48 694.046 2070.52 697.425 Q2073.57 700.782 2073.57 706.569 M2069.31 705.319 Q2069.27 701.823 2067.35 699.74 Q2065.45 697.657 2062.3 697.657 Q2058.73 697.657 2056.58 699.671 Q2054.45 701.685 2054.13 705.342 L2069.31 705.319 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M2099.22 695.666 L2099.22 699.648 Q2097.41 698.652 2095.59 698.166 Q2093.78 697.657 2091.93 697.657 Q2087.79 697.657 2085.49 700.296 Q2083.2 702.911 2083.2 707.657 Q2083.2 712.402 2085.49 715.041 Q2087.79 717.657 2091.93 717.657 Q2093.78 717.657 2095.59 717.171 Q2097.41 716.661 2099.22 715.666 L2099.22 719.601 Q2097.44 720.434 2095.52 720.851 Q2093.62 721.268 2091.47 721.268 Q2085.61 721.268 2082.16 717.587 Q2078.71 713.907 2078.71 707.657 Q2078.71 701.314 2082.18 697.68 Q2085.68 694.046 2091.74 694.046 Q2093.71 694.046 2095.59 694.462 Q2097.46 694.856 2099.22 695.666 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M2106.63 694.671 L2110.89 694.671 L2110.89 720.596 L2106.63 720.596 L2106.63 694.671 M2106.63 684.578 L2110.89 684.578 L2110.89 689.972 L2106.63 689.972 L2106.63 684.578 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M2136.33 695.435 L2136.33 699.462 Q2134.52 698.536 2132.58 698.073 Q2130.63 697.611 2128.55 697.611 Q2125.38 697.611 2123.78 698.583 Q2122.21 699.555 2122.21 701.499 Q2122.21 702.981 2123.34 703.837 Q2124.47 704.671 2127.9 705.435 L2129.36 705.759 Q2133.9 706.731 2135.79 708.513 Q2137.72 710.272 2137.72 713.444 Q2137.72 717.055 2134.85 719.161 Q2132 721.268 2127 721.268 Q2124.91 721.268 2122.65 720.851 Q2120.4 720.458 2117.9 719.647 L2117.9 715.249 Q2120.26 716.476 2122.55 717.101 Q2124.85 717.703 2127.09 717.703 Q2130.1 717.703 2131.72 716.684 Q2133.34 715.643 2133.34 713.768 Q2133.34 712.032 2132.16 711.106 Q2131 710.18 2127.04 709.323 L2125.56 708.976 Q2121.6 708.143 2119.85 706.43 Q2118.09 704.694 2118.09 701.685 Q2118.09 698.027 2120.68 696.036 Q2123.27 694.046 2128.04 694.046 Q2130.4 694.046 2132.48 694.393 Q2134.57 694.74 2136.33 695.435 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M2144.5 694.671 L2148.76 694.671 L2148.76 720.596 L2144.5 720.596 L2144.5 694.671 M2144.5 684.578 L2148.76 684.578 L2148.76 689.972 L2144.5 689.972 L2144.5 684.578 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M2167.72 697.657 Q2164.29 697.657 2162.3 700.342 Q2160.31 703.004 2160.31 707.657 Q2160.31 712.31 2162.28 714.995 Q2164.27 717.657 2167.72 717.657 Q2171.12 717.657 2173.11 714.972 Q2175.1 712.286 2175.1 707.657 Q2175.1 703.05 2173.11 700.365 Q2171.12 697.657 2167.72 697.657 M2167.72 694.046 Q2173.27 694.046 2176.44 697.657 Q2179.61 701.268 2179.61 707.657 Q2179.61 714.022 2176.44 717.657 Q2173.27 721.268 2167.72 721.268 Q2162.14 721.268 2158.97 717.657 Q2155.82 714.022 2155.82 707.657 Q2155.82 701.268 2158.97 697.657 Q2162.14 694.046 2167.72 694.046 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M2208.22 704.948 L2208.22 720.596 L2203.97 720.596 L2203.97 705.087 Q2203.97 701.407 2202.53 699.578 Q2201.09 697.749 2198.22 697.749 Q2194.78 697.749 2192.78 699.948 Q2190.79 702.148 2190.79 705.944 L2190.79 720.596 L2186.51 720.596 L2186.51 694.671 L2190.79 694.671 L2190.79 698.698 Q2192.32 696.361 2194.38 695.203 Q2196.47 694.046 2199.17 694.046 Q2203.64 694.046 2205.93 696.823 Q2208.22 699.578 2208.22 704.948 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip830)\" style=\"stroke:#c67812; stroke-linecap:round; stroke-linejoin:round; stroke-width:12; stroke-opacity:1; fill:none\" points=\"1839.49,755.156 1981.68,755.156 \"/>\n",
       "<path clip-path=\"url(#clip830)\" d=\"M2021.77 756.233 Q2023.27 756.742 2024.68 758.409 Q2026.12 760.075 2027.55 762.992 L2032.3 772.436 L2027.28 772.436 L2022.86 763.571 Q2021.14 760.099 2019.52 758.964 Q2017.92 757.83 2015.15 757.83 L2010.05 757.83 L2010.05 772.436 L2005.38 772.436 L2005.38 737.876 L2015.93 737.876 Q2021.86 737.876 2024.78 740.353 Q2027.69 742.83 2027.69 747.83 Q2027.69 751.094 2026.17 753.247 Q2024.66 755.4 2021.77 756.233 M2010.05 741.719 L2010.05 753.988 L2015.93 753.988 Q2019.31 753.988 2021.03 752.437 Q2022.76 750.863 2022.76 747.83 Q2022.76 744.798 2021.03 743.27 Q2019.31 741.719 2015.93 741.719 L2010.05 741.719 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M2058.18 758.409 L2058.18 760.492 L2038.6 760.492 Q2038.87 764.89 2041.23 767.205 Q2043.62 769.497 2047.85 769.497 Q2050.31 769.497 2052.6 768.895 Q2054.92 768.293 2057.18 767.089 L2057.18 771.117 Q2054.89 772.089 2052.48 772.599 Q2050.08 773.108 2047.6 773.108 Q2041.4 773.108 2037.76 769.497 Q2034.15 765.886 2034.15 759.728 Q2034.15 753.363 2037.58 749.636 Q2041.03 745.886 2046.86 745.886 Q2052.09 745.886 2055.12 749.265 Q2058.18 752.622 2058.18 758.409 M2053.92 757.159 Q2053.87 753.663 2051.95 751.58 Q2050.05 749.497 2046.91 749.497 Q2043.34 749.497 2041.19 751.511 Q2039.06 753.525 2038.73 757.182 L2053.92 757.159 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M2083.83 747.506 L2083.83 751.488 Q2082.02 750.492 2080.19 750.006 Q2078.39 749.497 2076.54 749.497 Q2072.39 749.497 2070.1 752.136 Q2067.81 754.751 2067.81 759.497 Q2067.81 764.242 2070.1 766.881 Q2072.39 769.497 2076.54 769.497 Q2078.39 769.497 2080.19 769.011 Q2082.02 768.501 2083.83 767.506 L2083.83 771.441 Q2082.04 772.274 2080.12 772.691 Q2078.23 773.108 2076.07 773.108 Q2070.22 773.108 2066.77 769.427 Q2063.32 765.747 2063.32 759.497 Q2063.32 753.154 2066.79 749.52 Q2070.29 745.886 2076.35 745.886 Q2078.32 745.886 2080.19 746.302 Q2082.07 746.696 2083.83 747.506 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M2103.02 759.404 Q2097.85 759.404 2095.86 760.585 Q2093.87 761.765 2093.87 764.612 Q2093.87 766.881 2095.35 768.224 Q2096.86 769.543 2099.43 769.543 Q2102.97 769.543 2105.1 767.043 Q2107.25 764.52 2107.25 760.353 L2107.25 759.404 L2103.02 759.404 M2111.51 757.645 L2111.51 772.436 L2107.25 772.436 L2107.25 768.501 Q2105.79 770.862 2103.62 771.997 Q2101.44 773.108 2098.29 773.108 Q2094.31 773.108 2091.95 770.886 Q2089.61 768.64 2089.61 764.89 Q2089.61 760.515 2092.53 758.293 Q2095.47 756.071 2101.28 756.071 L2107.25 756.071 L2107.25 755.654 Q2107.25 752.714 2105.31 751.117 Q2103.39 749.497 2099.89 749.497 Q2097.67 749.497 2095.56 750.029 Q2093.46 750.562 2091.51 751.626 L2091.51 747.691 Q2093.85 746.788 2096.05 746.349 Q2098.25 745.886 2100.33 745.886 Q2105.96 745.886 2108.73 748.802 Q2111.51 751.719 2111.51 757.645 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M2120.29 736.418 L2124.54 736.418 L2124.54 772.436 L2120.29 772.436 L2120.29 736.418 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M2133.46 736.418 L2137.72 736.418 L2137.72 772.436 L2133.46 772.436 L2133.46 736.418 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip830)\" style=\"stroke:#a9a9a9; stroke-linecap:round; stroke-linejoin:round; stroke-width:12; stroke-opacity:1; fill:none\" stroke-dasharray=\"48, 30\" points=\"1839.49,806.996 1981.68,806.996 \"/>\n",
       "<path clip-path=\"url(#clip830)\" d=\"M2033.25 792.379 L2033.25 797.309 Q2030.89 795.11 2028.2 794.022 Q2025.54 792.934 2022.53 792.934 Q2016.61 792.934 2013.46 796.568 Q2010.31 800.179 2010.31 807.031 Q2010.31 813.86 2013.46 817.494 Q2016.61 821.105 2022.53 821.105 Q2025.54 821.105 2028.2 820.017 Q2030.89 818.929 2033.25 816.73 L2033.25 821.614 Q2030.79 823.281 2028.04 824.114 Q2025.31 824.948 2022.25 824.948 Q2014.41 824.948 2009.89 820.156 Q2005.38 815.341 2005.38 807.031 Q2005.38 798.698 2009.89 793.906 Q2014.41 789.091 2022.25 789.091 Q2025.36 789.091 2028.09 789.925 Q2030.84 790.735 2033.25 792.379 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M2040.29 788.258 L2044.54 788.258 L2044.54 824.276 L2040.29 824.276 L2040.29 788.258 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M2065.24 811.244 Q2060.08 811.244 2058.09 812.425 Q2056.1 813.605 2056.1 816.452 Q2056.1 818.721 2057.58 820.064 Q2059.08 821.383 2061.65 821.383 Q2065.19 821.383 2067.32 818.883 Q2069.48 816.36 2069.48 812.193 L2069.48 811.244 L2065.24 811.244 M2073.73 809.485 L2073.73 824.276 L2069.48 824.276 L2069.48 820.341 Q2068.02 822.702 2065.84 823.837 Q2063.67 824.948 2060.52 824.948 Q2056.54 824.948 2054.17 822.726 Q2051.84 820.48 2051.84 816.73 Q2051.84 812.355 2054.75 810.133 Q2057.69 807.911 2063.5 807.911 L2069.48 807.911 L2069.48 807.494 Q2069.48 804.554 2067.53 802.957 Q2065.61 801.337 2062.11 801.337 Q2059.89 801.337 2057.79 801.869 Q2055.68 802.402 2053.73 803.466 L2053.73 799.531 Q2056.07 798.628 2058.27 798.189 Q2060.47 797.726 2062.55 797.726 Q2068.18 797.726 2070.96 800.642 Q2073.73 803.559 2073.73 809.485 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M2099.04 799.115 L2099.04 803.142 Q2097.23 802.216 2095.29 801.753 Q2093.34 801.291 2091.26 801.291 Q2088.09 801.291 2086.49 802.263 Q2084.91 803.235 2084.91 805.179 Q2084.91 806.661 2086.05 807.517 Q2087.18 808.351 2090.61 809.115 L2092.07 809.439 Q2096.6 810.411 2098.5 812.193 Q2100.42 813.952 2100.42 817.124 Q2100.42 820.735 2097.55 822.841 Q2094.71 824.948 2089.71 824.948 Q2087.62 824.948 2085.35 824.531 Q2083.11 824.138 2080.61 823.327 L2080.61 818.929 Q2082.97 820.156 2085.26 820.781 Q2087.55 821.383 2089.8 821.383 Q2092.81 821.383 2094.43 820.364 Q2096.05 819.323 2096.05 817.448 Q2096.05 815.712 2094.87 814.786 Q2093.71 813.86 2089.75 813.003 L2088.27 812.656 Q2084.31 811.823 2082.55 810.11 Q2080.79 808.374 2080.79 805.365 Q2080.79 801.707 2083.39 799.716 Q2085.98 797.726 2090.75 797.726 Q2093.11 797.726 2095.19 798.073 Q2097.28 798.42 2099.04 799.115 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M2123.73 799.115 L2123.73 803.142 Q2121.93 802.216 2119.98 801.753 Q2118.04 801.291 2115.96 801.291 Q2112.79 801.291 2111.19 802.263 Q2109.61 803.235 2109.61 805.179 Q2109.61 806.661 2110.75 807.517 Q2111.88 808.351 2115.31 809.115 L2116.77 809.439 Q2121.3 810.411 2123.2 812.193 Q2125.12 813.952 2125.12 817.124 Q2125.12 820.735 2122.25 822.841 Q2119.41 824.948 2114.41 824.948 Q2112.32 824.948 2110.05 824.531 Q2107.81 824.138 2105.31 823.327 L2105.31 818.929 Q2107.67 820.156 2109.96 820.781 Q2112.25 821.383 2114.5 821.383 Q2117.51 821.383 2119.13 820.364 Q2120.75 819.323 2120.75 817.448 Q2120.75 815.712 2119.57 814.786 Q2118.41 813.86 2114.45 813.003 L2112.97 812.656 Q2109.01 811.823 2107.25 810.11 Q2105.49 808.374 2105.49 805.365 Q2105.49 801.707 2108.09 799.716 Q2110.68 797.726 2115.45 797.726 Q2117.81 797.726 2119.89 798.073 Q2121.97 798.42 2123.73 799.115 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M2151.84 807.772 L2151.84 820.434 L2159.34 820.434 Q2163.11 820.434 2164.91 818.883 Q2166.74 817.309 2166.74 814.091 Q2166.74 810.851 2164.91 809.323 Q2163.11 807.772 2159.34 807.772 L2151.84 807.772 M2151.84 793.559 L2151.84 803.976 L2158.76 803.976 Q2162.18 803.976 2163.85 802.703 Q2165.54 801.406 2165.54 798.767 Q2165.54 796.152 2163.85 794.855 Q2162.18 793.559 2158.76 793.559 L2151.84 793.559 M2147.16 789.716 L2159.1 789.716 Q2164.45 789.716 2167.35 791.939 Q2170.24 794.161 2170.24 798.258 Q2170.24 801.429 2168.76 803.304 Q2167.28 805.179 2164.41 805.642 Q2167.85 806.383 2169.75 808.744 Q2171.67 811.082 2171.67 814.601 Q2171.67 819.23 2168.53 821.753 Q2165.38 824.276 2159.57 824.276 L2147.16 824.276 L2147.16 789.716 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M2191.28 811.244 Q2186.12 811.244 2184.13 812.425 Q2182.14 813.605 2182.14 816.452 Q2182.14 818.721 2183.62 820.064 Q2185.12 821.383 2187.69 821.383 Q2191.23 821.383 2193.36 818.883 Q2195.52 816.36 2195.52 812.193 L2195.52 811.244 L2191.28 811.244 M2199.78 809.485 L2199.78 824.276 L2195.52 824.276 L2195.52 820.341 Q2194.06 822.702 2191.88 823.837 Q2189.71 824.948 2186.56 824.948 Q2182.58 824.948 2180.22 822.726 Q2177.88 820.48 2177.88 816.73 Q2177.88 812.355 2180.79 810.133 Q2183.73 807.911 2189.54 807.911 L2195.52 807.911 L2195.52 807.494 Q2195.52 804.554 2193.57 802.957 Q2191.65 801.337 2188.16 801.337 Q2185.93 801.337 2183.83 801.869 Q2181.72 802.402 2179.78 803.466 L2179.78 799.531 Q2182.11 798.628 2184.31 798.189 Q2186.51 797.726 2188.59 797.726 Q2194.22 797.726 2197 800.642 Q2199.78 803.559 2199.78 809.485 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M2208.55 788.258 L2212.81 788.258 L2212.81 824.276 L2208.55 824.276 L2208.55 788.258 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M2233.5 811.244 Q2228.34 811.244 2226.35 812.425 Q2224.36 813.605 2224.36 816.452 Q2224.36 818.721 2225.84 820.064 Q2227.34 821.383 2229.91 821.383 Q2233.46 821.383 2235.59 818.883 Q2237.74 816.36 2237.74 812.193 L2237.74 811.244 L2233.5 811.244 M2242 809.485 L2242 824.276 L2237.74 824.276 L2237.74 820.341 Q2236.28 822.702 2234.1 823.837 Q2231.93 824.948 2228.78 824.948 Q2224.8 824.948 2222.44 822.726 Q2220.1 820.48 2220.1 816.73 Q2220.1 812.355 2223.02 810.133 Q2225.96 807.911 2231.77 807.911 L2237.74 807.911 L2237.74 807.494 Q2237.74 804.554 2235.79 802.957 Q2233.87 801.337 2230.38 801.337 Q2228.15 801.337 2226.05 801.869 Q2223.94 802.402 2222 803.466 L2222 799.531 Q2224.34 798.628 2226.53 798.189 Q2228.73 797.726 2230.82 797.726 Q2236.44 797.726 2239.22 800.642 Q2242 803.559 2242 809.485 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M2272.32 808.628 L2272.32 824.276 L2268.06 824.276 L2268.06 808.767 Q2268.06 805.087 2266.63 803.258 Q2265.19 801.429 2262.32 801.429 Q2258.87 801.429 2256.88 803.628 Q2254.89 805.828 2254.89 809.624 L2254.89 824.276 L2250.61 824.276 L2250.61 798.351 L2254.89 798.351 L2254.89 802.378 Q2256.42 800.041 2258.48 798.883 Q2260.56 797.726 2263.27 797.726 Q2267.74 797.726 2270.03 800.503 Q2272.32 803.258 2272.32 808.628 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M2299.47 799.346 L2299.47 803.328 Q2297.67 802.332 2295.84 801.846 Q2294.03 801.337 2292.18 801.337 Q2288.04 801.337 2285.75 803.976 Q2283.46 806.591 2283.46 811.337 Q2283.46 816.082 2285.75 818.721 Q2288.04 821.337 2292.18 821.337 Q2294.03 821.337 2295.84 820.851 Q2297.67 820.341 2299.47 819.346 L2299.47 823.281 Q2297.69 824.114 2295.77 824.531 Q2293.87 824.948 2291.72 824.948 Q2285.86 824.948 2282.41 821.267 Q2278.96 817.587 2278.96 811.337 Q2278.96 804.994 2282.44 801.36 Q2285.93 797.726 2292 797.726 Q2293.96 797.726 2295.84 798.142 Q2297.71 798.536 2299.47 799.346 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M2329.06 810.249 L2329.06 812.332 L2309.47 812.332 Q2309.75 816.73 2312.11 819.045 Q2314.5 821.337 2318.73 821.337 Q2321.19 821.337 2323.48 820.735 Q2325.79 820.133 2328.06 818.929 L2328.06 822.957 Q2325.77 823.929 2323.36 824.439 Q2320.96 824.948 2318.48 824.948 Q2312.27 824.948 2308.64 821.337 Q2305.03 817.726 2305.03 811.568 Q2305.03 805.203 2308.46 801.476 Q2311.9 797.726 2317.74 797.726 Q2322.97 797.726 2326 801.105 Q2329.06 804.462 2329.06 810.249 M2324.8 808.999 Q2324.75 805.503 2322.83 803.42 Q2320.93 801.337 2317.78 801.337 Q2314.22 801.337 2312.07 803.351 Q2309.94 805.365 2309.61 809.022 L2324.8 808.999 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "threshold_optimization = DataFrame(\n",
    "  threshold = Float64[],\n",
    "  precision = Float64[],\n",
    "  recall = Float64[],\n",
    "  f1 = Float64[],\n",
    ")\n",
    "\n",
    "for threshold in 0.1:0.1:0.8\n",
    "  model_baseline_metrics = metrics(\"dummy\", predictions, y_test, threshold)\n",
    "  push!(threshold_optimization, (threshold, model_baseline_metrics.precision[1], model_baseline_metrics.recall[1], model_baseline_metrics.f1[1]))\n",
    "end\n",
    "\n",
    "# Plot the precision based on the threshold\n",
    "threshold_optimization_plot = plot(\n",
    "  xlabel=\"Threshold\",\n",
    "  ylabel=\"Precision and Recall\",\n",
    "  # title=\"Precision-Recall Tradeoff for Different Thresholds\",\n",
    "  legend=:outerright,\n",
    "  bg=false\n",
    ")\n",
    "\n",
    "# Add precision to the plot based on the threshold\n",
    "threshold_optimization_plot = plot!(\n",
    "  threshold_optimization.threshold,\n",
    "  threshold_optimization.precision,\n",
    "  label=\"Precision\",\n",
    "  color=primary_color,\n",
    "  linewidth=3,\n",
    ")\n",
    "\n",
    "# Add recall to the plot based on the threshold\n",
    "threshold_optimization_plot = plot!(\n",
    "  threshold_optimization.threshold,\n",
    "  threshold_optimization.recall,\n",
    "  label=\"Recall\",\n",
    "  color=secondary_color,\n",
    "  linewidth=3,\n",
    ")\n",
    "\n",
    "# Add a vertical line at class balance\n",
    "threshold_optimization_plot = vline!(\n",
    "  [0.27],\n",
    "  label=\"Class Balance\",\n",
    "  color=:darkgrey,\n",
    "  linestyle=:dash,\n",
    "  linewidth=3,\n",
    ")\n",
    "\n",
    "# Display the plot\n",
    "threshold_optimization_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/Users/redx/Library/CloudStorage/OneDrive-Personal/UCR/academics/winter_24/STAT206/project/images/threshold_optimization_plot.png\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the plot as .png\n",
    "savefig(threshold_optimization_plot, \"images/threshold_optimization_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>8×4 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">threshold</th><th style = \"text-align: left;\">precision</th><th style = \"text-align: left;\">recall</th><th style = \"text-align: left;\">f1</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">0.1</td><td style = \"text-align: right;\">0.409</td><td style = \"text-align: right;\">0.946</td><td style = \"text-align: right;\">0.571</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: right;\">0.2</td><td style = \"text-align: right;\">0.485</td><td style = \"text-align: right;\">0.888</td><td style = \"text-align: right;\">0.628</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: right;\">0.3</td><td style = \"text-align: right;\">0.543</td><td style = \"text-align: right;\">0.792</td><td style = \"text-align: right;\">0.644</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: right;\">0.4</td><td style = \"text-align: right;\">0.602</td><td style = \"text-align: right;\">0.699</td><td style = \"text-align: right;\">0.647</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: right;\">0.5</td><td style = \"text-align: right;\">0.67</td><td style = \"text-align: right;\">0.597</td><td style = \"text-align: right;\">0.632</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: right;\">0.6</td><td style = \"text-align: right;\">0.706</td><td style = \"text-align: right;\">0.439</td><td style = \"text-align: right;\">0.541</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: right;\">0.7</td><td style = \"text-align: right;\">0.766</td><td style = \"text-align: right;\">0.26</td><td style = \"text-align: right;\">0.388</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: right;\">0.8</td><td style = \"text-align: right;\">0.941</td><td style = \"text-align: right;\">0.058</td><td style = \"text-align: right;\">0.109</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccc}\n",
       "\t& threshold & precision & recall & f1\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & 0.1 & 0.409 & 0.946 & 0.571 \\\\\n",
       "\t2 & 0.2 & 0.485 & 0.888 & 0.628 \\\\\n",
       "\t3 & 0.3 & 0.543 & 0.792 & 0.644 \\\\\n",
       "\t4 & 0.4 & 0.602 & 0.699 & 0.647 \\\\\n",
       "\t5 & 0.5 & 0.67 & 0.597 & 0.632 \\\\\n",
       "\t6 & 0.6 & 0.706 & 0.439 & 0.541 \\\\\n",
       "\t7 & 0.7 & 0.766 & 0.26 & 0.388 \\\\\n",
       "\t8 & 0.8 & 0.941 & 0.058 & 0.109 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m8×4 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m threshold \u001b[0m\u001b[1m precision \u001b[0m\u001b[1m recall  \u001b[0m\u001b[1m f1      \u001b[0m\n",
       "     │\u001b[90m Float64   \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\n",
       "─────┼────────────────────────────────────────\n",
       "   1 │       0.1      0.409    0.946    0.571\n",
       "   2 │       0.2      0.485    0.888    0.628\n",
       "   3 │       0.3      0.543    0.792    0.644\n",
       "   4 │       0.4      0.602    0.699    0.647\n",
       "   5 │       0.5      0.67     0.597    0.632\n",
       "   6 │       0.6      0.706    0.439    0.541\n",
       "   7 │       0.7      0.766    0.26     0.388\n",
       "   8 │       0.8      0.941    0.058    0.109"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "threshold_optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find which threshold gives the best f1 score\n",
    "optimal_threshold = threshold_optimization[findfirst(threshold_optimization.f1 .== maximum(threshold_optimization.f1)), :threshold][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Optimal Threshold Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>2×6 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">model</th><th style = \"text-align: left;\">accuracy</th><th style = \"text-align: left;\">precision</th><th style = \"text-align: left;\">recall</th><th style = \"text-align: left;\">f1</th><th style = \"text-align: left;\">auc</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"String\" style = \"text-align: left;\">String</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: left;\">GLM: Baseline</td><td style = \"text-align: right;\">0.817</td><td style = \"text-align: right;\">0.67</td><td style = \"text-align: right;\">0.597</td><td style = \"text-align: right;\">0.632</td><td style = \"text-align: right;\">0.859</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: left;\">GLM: Optimal Threshold</td><td style = \"text-align: right;\">0.8</td><td style = \"text-align: right;\">0.602</td><td style = \"text-align: right;\">0.699</td><td style = \"text-align: right;\">0.647</td><td style = \"text-align: right;\">0.859</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& model & accuracy & precision & recall & f1 & auc\\\\\n",
       "\t\\hline\n",
       "\t& String & Float64 & Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & GLM: Baseline & 0.817 & 0.67 & 0.597 & 0.632 & 0.859 \\\\\n",
       "\t2 & GLM: Optimal Threshold & 0.8 & 0.602 & 0.699 & 0.647 & 0.859 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m2×6 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m model                  \u001b[0m\u001b[1m accuracy \u001b[0m\u001b[1m precision \u001b[0m\u001b[1m recall  \u001b[0m\u001b[1m f1      \u001b[0m\u001b[1m auc     \u001b[0m\n",
       "     │\u001b[90m String                 \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\n",
       "─────┼────────────────────────────────────────────────────────────────────────\n",
       "   1 │ GLM: Baseline              0.817      0.67     0.597    0.632    0.859\n",
       "   2 │ GLM: Optimal Threshold     0.8        0.602    0.699    0.647    0.859"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute the metrics\n",
    "model_baseline_optimal_threshold_metrics = metrics(\n",
    "  \"GLM: Optimal Threshold\",\n",
    "  predictions,\n",
    "  y_test,\n",
    "  optimal_threshold\n",
    ")\n",
    "\n",
    "# Store the metrics in the DataFrame\n",
    "push!(all_model_metrics, model_baseline_optimal_threshold_metrics[1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Addressing Class Imbalance by Undersampling Majority Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trained Machine; caches model-specific representations of data\n",
       "  model: LogisticClassifier(lambda = 2.220446049250313e-16, …)\n",
       "  args: \n",
       "    1:\tSource @351 ⏎ Table{AbstractVector{Continuous}}\n",
       "    2:\tSource @030 ⏎ AbstractVector{Multiclass{2}}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split the data\n",
    "X_train, y_train, X_test, y_test = split_data(telco_balanced_undersampling)\n",
    "\n",
    "# Build the machine\n",
    "model_balanced_undersampling = fit!(machine(LogisticClassifier(), X_train, y_train), verbosity=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>3×6 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">model</th><th style = \"text-align: left;\">accuracy</th><th style = \"text-align: left;\">precision</th><th style = \"text-align: left;\">recall</th><th style = \"text-align: left;\">f1</th><th style = \"text-align: left;\">auc</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"String\" style = \"text-align: left;\">String</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: left;\">GLM: Baseline</td><td style = \"text-align: right;\">0.817</td><td style = \"text-align: right;\">0.67</td><td style = \"text-align: right;\">0.597</td><td style = \"text-align: right;\">0.632</td><td style = \"text-align: right;\">0.859</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: left;\">GLM: Optimal Threshold</td><td style = \"text-align: right;\">0.8</td><td style = \"text-align: right;\">0.602</td><td style = \"text-align: right;\">0.699</td><td style = \"text-align: right;\">0.647</td><td style = \"text-align: right;\">0.859</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: left;\">GLM: Undersampling</td><td style = \"text-align: right;\">0.767</td><td style = \"text-align: right;\">0.752</td><td style = \"text-align: right;\">0.809</td><td style = \"text-align: right;\">0.78</td><td style = \"text-align: right;\">0.847</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& model & accuracy & precision & recall & f1 & auc\\\\\n",
       "\t\\hline\n",
       "\t& String & Float64 & Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & GLM: Baseline & 0.817 & 0.67 & 0.597 & 0.632 & 0.859 \\\\\n",
       "\t2 & GLM: Optimal Threshold & 0.8 & 0.602 & 0.699 & 0.647 & 0.859 \\\\\n",
       "\t3 & GLM: Undersampling & 0.767 & 0.752 & 0.809 & 0.78 & 0.847 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m3×6 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m model                  \u001b[0m\u001b[1m accuracy \u001b[0m\u001b[1m precision \u001b[0m\u001b[1m recall  \u001b[0m\u001b[1m f1      \u001b[0m\u001b[1m auc     \u001b[0m\n",
       "     │\u001b[90m String                 \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\n",
       "─────┼────────────────────────────────────────────────────────────────────────\n",
       "   1 │ GLM: Baseline              0.817      0.67     0.597    0.632    0.859\n",
       "   2 │ GLM: Optimal Threshold     0.8        0.602    0.699    0.647    0.859\n",
       "   3 │ GLM: Undersampling         0.767      0.752    0.809    0.78     0.847"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = predict(model_balanced_undersampling, X_test)\n",
    "# Compute the metrics\n",
    "baseline_undersampling_metrics = metrics(\n",
    "  \"GLM: Undersampling\",\n",
    "  predictions,\n",
    "  y_test,\n",
    "  0.5\n",
    ")\n",
    "# Store the metrics in the DataFrame\n",
    "push!(all_model_metrics, baseline_undersampling_metrics[1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Addressing Class Imbalance by Oversampling Minority Class (SMOTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trained Machine; caches model-specific representations of data\n",
       "  model: LogisticClassifier(lambda = 2.220446049250313e-16, …)\n",
       "  args: \n",
       "    1:\tSource @675 ⏎ Table{AbstractVector{Continuous}}\n",
       "    2:\tSource @047 ⏎ AbstractVector{Multiclass{2}}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split the data\n",
    "X_train, y_train, X_test, y_test = split_data(telco_balanced_oversampling)\n",
    "\n",
    "# Build the machine\n",
    "model_balanced_oversampling = fit!(machine(LogisticClassifier(), X_train, y_train), verbosity=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>4×6 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">model</th><th style = \"text-align: left;\">accuracy</th><th style = \"text-align: left;\">precision</th><th style = \"text-align: left;\">recall</th><th style = \"text-align: left;\">f1</th><th style = \"text-align: left;\">auc</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"String\" style = \"text-align: left;\">String</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: left;\">GLM: Baseline</td><td style = \"text-align: right;\">0.817</td><td style = \"text-align: right;\">0.67</td><td style = \"text-align: right;\">0.597</td><td style = \"text-align: right;\">0.632</td><td style = \"text-align: right;\">0.859</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: left;\">GLM: Optimal Threshold</td><td style = \"text-align: right;\">0.8</td><td style = \"text-align: right;\">0.602</td><td style = \"text-align: right;\">0.699</td><td style = \"text-align: right;\">0.647</td><td style = \"text-align: right;\">0.859</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: left;\">GLM: Undersampling</td><td style = \"text-align: right;\">0.767</td><td style = \"text-align: right;\">0.752</td><td style = \"text-align: right;\">0.809</td><td style = \"text-align: right;\">0.78</td><td style = \"text-align: right;\">0.847</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: left;\">GLM: Oversampling</td><td style = \"text-align: right;\">0.787</td><td style = \"text-align: right;\">0.773</td><td style = \"text-align: right;\">0.827</td><td style = \"text-align: right;\">0.799</td><td style = \"text-align: right;\">0.87</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& model & accuracy & precision & recall & f1 & auc\\\\\n",
       "\t\\hline\n",
       "\t& String & Float64 & Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & GLM: Baseline & 0.817 & 0.67 & 0.597 & 0.632 & 0.859 \\\\\n",
       "\t2 & GLM: Optimal Threshold & 0.8 & 0.602 & 0.699 & 0.647 & 0.859 \\\\\n",
       "\t3 & GLM: Undersampling & 0.767 & 0.752 & 0.809 & 0.78 & 0.847 \\\\\n",
       "\t4 & GLM: Oversampling & 0.787 & 0.773 & 0.827 & 0.799 & 0.87 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m4×6 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m model                  \u001b[0m\u001b[1m accuracy \u001b[0m\u001b[1m precision \u001b[0m\u001b[1m recall  \u001b[0m\u001b[1m f1      \u001b[0m\u001b[1m auc     \u001b[0m\n",
       "     │\u001b[90m String                 \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\n",
       "─────┼────────────────────────────────────────────────────────────────────────\n",
       "   1 │ GLM: Baseline              0.817      0.67     0.597    0.632    0.859\n",
       "   2 │ GLM: Optimal Threshold     0.8        0.602    0.699    0.647    0.859\n",
       "   3 │ GLM: Undersampling         0.767      0.752    0.809    0.78     0.847\n",
       "   4 │ GLM: Oversampling          0.787      0.773    0.827    0.799    0.87"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = predict(model_balanced_oversampling, X_test)\n",
    "# Compute the metrics\n",
    "model_oversampling_metrics = metrics(\n",
    "  \"GLM: Oversampling\",\n",
    "  predictions,\n",
    "  y_test,\n",
    "  0.5\n",
    ")\n",
    "# Store the metrics in the DataFrame\n",
    "push!(all_model_metrics, model_oversampling_metrics[1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: Regularization with Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the Model with Optimal Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trained Machine; caches model-specific representations of data\n",
       "  model: LogisticClassifier(lambda = 1.0, …)\n",
       "  args: \n",
       "    1:\tSource @499 ⏎ Table{AbstractVector{Continuous}}\n",
       "    2:\tSource @179 ⏎ AbstractVector{Multiclass{2}}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split the data\n",
    "X_train, y_train, X_test, y_test = split_data(telco_balanced_oversampling)\n",
    "\n",
    "model_regularization = LogisticClassifier(penalty=:l1, lambda=1.0)\n",
    "\n",
    "model_regularization_fit = fit!(machine(model_regularization, X_train, y_train), verbosity = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning the Hyperparameter `lambda` helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuned_params (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function tuned_params(X_train::DataFrame, y_train::CategoricalArray)\n",
    "\n",
    "  # Define the parameter range for lambda\n",
    "  range_lambda = range(model_regularization, :lambda, lower=0.01, upper=100.0, scale=:log)\n",
    "\n",
    "  # Setup a tuning strategy, e.g., Grid Search\n",
    "  tuning = Grid(resolution=10)  # Adjust resolution as needed\n",
    "\n",
    "  # Define the tuning experiment\n",
    "  tuned_model = TunedModel(\n",
    "    model=model_regularization,\n",
    "    tuning=tuning,\n",
    "    resampling=CV(),\n",
    "    ranges=range_lambda,\n",
    "    measure=log_loss\n",
    "  )\n",
    "\n",
    "  # Fit the tuned model\n",
    "  tuned_model_fit = fit!(machine(tuned_model, X_train, y_train), verbosity=0)\n",
    "\n",
    "  # Get the best lambda\n",
    "  params = fitted_params(tuned_model_fit)\n",
    "\n",
    "  return params\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning the Hyperparameter `lambda`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────────┬────────────┬────────────────┬──────────────┬────────────────────┬─────────────────────┬─────────────┬──────────────┬────────────────┬─────────────────┬───────────────┬───────────────────┬────────────────────┬────────────────────┬──────────────────────────────────┬─────────────────────┬───────────────────────┬───────────────────────────────┬──────────────────────┬───────────────────────┬────────────────────────────────────────┬────────────────────────┬──────────────────┬───────────────────────────────────┬───────────────────┬──────────────────────────┬────────────────────┬────────────────────┬───────────────────────┬────────────────────────┬───────────────────────────────────────────┬─────────────────────────────────────────┬──────────────────────────────────┬──────────────────────────────┬─────────────────┬───────────────┬────────────┐\n",
      "│\u001b[1m latitude   \u001b[0m│\u001b[1m longitude  \u001b[0m│\u001b[1m gender__Female \u001b[0m│\u001b[1m gender__Male \u001b[0m│\u001b[1m senior_citizen__No \u001b[0m│\u001b[1m senior_citizen__Yes \u001b[0m│\u001b[1m partner__No \u001b[0m│\u001b[1m partner__Yes \u001b[0m│\u001b[1m dependents__No \u001b[0m│\u001b[1m dependents__Yes \u001b[0m│\u001b[1m tenure_months \u001b[0m│\u001b[1m phone_service__No \u001b[0m│\u001b[1m phone_service__Yes \u001b[0m│\u001b[1m multiple_lines__No \u001b[0m│\u001b[1m multiple_lines__No phone service \u001b[0m│\u001b[1m multiple_lines__Yes \u001b[0m│\u001b[1m internet_service__DSL \u001b[0m│\u001b[1m internet_service__Fiber optic \u001b[0m│\u001b[1m internet_service__No \u001b[0m│\u001b[1m device_protection__No \u001b[0m│\u001b[1m device_protection__No internet service \u001b[0m│\u001b[1m device_protection__Yes \u001b[0m│\u001b[1m tech_support__No \u001b[0m│\u001b[1m tech_support__No internet service \u001b[0m│\u001b[1m tech_support__Yes \u001b[0m│\u001b[1m contract__Month-to-month \u001b[0m│\u001b[1m contract__One year \u001b[0m│\u001b[1m contract__Two year \u001b[0m│\u001b[1m paperless_billing__No \u001b[0m│\u001b[1m paperless_billing__Yes \u001b[0m│\u001b[1m payment_method__Bank transfer (automatic) \u001b[0m│\u001b[1m payment_method__Credit card (automatic) \u001b[0m│\u001b[1m payment_method__Electronic check \u001b[0m│\u001b[1m payment_method__Mailed check \u001b[0m│\u001b[1m monthly_charges \u001b[0m│\u001b[1m total_charges \u001b[0m│\u001b[1m cltv       \u001b[0m│\n",
      "│\u001b[90m Float64    \u001b[0m│\u001b[90m Float64    \u001b[0m│\u001b[90m Float64        \u001b[0m│\u001b[90m Float64      \u001b[0m│\u001b[90m Float64            \u001b[0m│\u001b[90m Float64             \u001b[0m│\u001b[90m Float64     \u001b[0m│\u001b[90m Float64      \u001b[0m│\u001b[90m Float64        \u001b[0m│\u001b[90m Float64         \u001b[0m│\u001b[90m Float64       \u001b[0m│\u001b[90m Float64           \u001b[0m│\u001b[90m Float64            \u001b[0m│\u001b[90m Float64            \u001b[0m│\u001b[90m Float64                          \u001b[0m│\u001b[90m Float64             \u001b[0m│\u001b[90m Float64               \u001b[0m│\u001b[90m Float64                       \u001b[0m│\u001b[90m Float64              \u001b[0m│\u001b[90m Float64               \u001b[0m│\u001b[90m Float64                                \u001b[0m│\u001b[90m Float64                \u001b[0m│\u001b[90m Float64          \u001b[0m│\u001b[90m Float64                           \u001b[0m│\u001b[90m Float64           \u001b[0m│\u001b[90m Float64                  \u001b[0m│\u001b[90m Float64            \u001b[0m│\u001b[90m Float64            \u001b[0m│\u001b[90m Float64               \u001b[0m│\u001b[90m Float64                \u001b[0m│\u001b[90m Float64                                   \u001b[0m│\u001b[90m Float64                                 \u001b[0m│\u001b[90m Float64                          \u001b[0m│\u001b[90m Float64                      \u001b[0m│\u001b[90m Float64         \u001b[0m│\u001b[90m Float64       \u001b[0m│\u001b[90m Float64    \u001b[0m│\n",
      "│\u001b[90m Continuous \u001b[0m│\u001b[90m Continuous \u001b[0m│\u001b[90m Continuous     \u001b[0m│\u001b[90m Continuous   \u001b[0m│\u001b[90m Continuous         \u001b[0m│\u001b[90m Continuous          \u001b[0m│\u001b[90m Continuous  \u001b[0m│\u001b[90m Continuous   \u001b[0m│\u001b[90m Continuous     \u001b[0m│\u001b[90m Continuous      \u001b[0m│\u001b[90m Continuous    \u001b[0m│\u001b[90m Continuous        \u001b[0m│\u001b[90m Continuous         \u001b[0m│\u001b[90m Continuous         \u001b[0m│\u001b[90m Continuous                       \u001b[0m│\u001b[90m Continuous          \u001b[0m│\u001b[90m Continuous            \u001b[0m│\u001b[90m Continuous                    \u001b[0m│\u001b[90m Continuous           \u001b[0m│\u001b[90m Continuous            \u001b[0m│\u001b[90m Continuous                             \u001b[0m│\u001b[90m Continuous             \u001b[0m│\u001b[90m Continuous       \u001b[0m│\u001b[90m Continuous                        \u001b[0m│\u001b[90m Continuous        \u001b[0m│\u001b[90m Continuous               \u001b[0m│\u001b[90m Continuous         \u001b[0m│\u001b[90m Continuous         \u001b[0m│\u001b[90m Continuous            \u001b[0m│\u001b[90m Continuous             \u001b[0m│\u001b[90m Continuous                                \u001b[0m│\u001b[90m Continuous                              \u001b[0m│\u001b[90m Continuous                       \u001b[0m│\u001b[90m Continuous                   \u001b[0m│\u001b[90m Continuous      \u001b[0m│\u001b[90m Continuous    \u001b[0m│\u001b[90m Continuous \u001b[0m│\n",
      "├────────────┼────────────┼────────────────┼──────────────┼────────────────────┼─────────────────────┼─────────────┼──────────────┼────────────────┼─────────────────┼───────────────┼───────────────────┼────────────────────┼────────────────────┼──────────────────────────────────┼─────────────────────┼───────────────────────┼───────────────────────────────┼──────────────────────┼───────────────────────┼────────────────────────────────────────┼────────────────────────┼──────────────────┼───────────────────────────────────┼───────────────────┼──────────────────────────┼────────────────────┼────────────────────┼───────────────────────┼────────────────────────┼───────────────────────────────────────────┼─────────────────────────────────────────┼──────────────────────────────────┼──────────────────────────────┼─────────────────┼───────────────┼────────────┤\n",
      "│ 0.0        │ -0.0       │ -0.0           │ 0.0          │ -0.0               │ 0.0                 │ -0.0        │ 0.0          │ 0.102583       │ -0.920167       │ -0.74406      │ 0.0               │ -0.0               │ -0.122891          │ 0.0                              │ 0.0                 │ -0.0                  │ 0.263016                      │ -0.0                 │ 0.0                   │ -0.0                                   │ -0.0                   │ 0.41024          │ -0.0                              │ -0.0              │ 0.823492                 │ 0.0                │ -0.00808394        │ -0.00211737           │ 0.219662               │ -0.0                                      │ -0.0                                    │ 0.459811                         │ -0.0                         │ 0.35513         │ 0.0           │ 0.0        │\n",
      "└────────────┴────────────┴────────────────┴──────────────┴────────────────────┴─────────────────────┴─────────────┴──────────────┴────────────────┴─────────────────┴───────────────┴───────────────────┴────────────────────┴────────────────────┴──────────────────────────────────┴─────────────────────┴───────────────────────┴───────────────────────────────┴──────────────────────┴───────────────────────┴────────────────────────────────────────┴────────────────────────┴──────────────────┴───────────────────────────────────┴───────────────────┴──────────────────────────┴────────────────────┴────────────────────┴───────────────────────┴────────────────────────┴───────────────────────────────────────────┴─────────────────────────────────────────┴──────────────────────────────────┴──────────────────────────────┴─────────────────┴───────────────┴────────────┘\n"
     ]
    }
   ],
   "source": [
    "params = tuned_params(X_train, y_train)\n",
    "\n",
    "best_lambda = params.best_model.lambda\n",
    "\n",
    "coefficients = params.best_fitted_params.coefs |> DataFrame |> pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.4270424258183725"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "intercept = params.best_fitted_params.intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the model with optimal hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trained Machine; caches model-specific representations of data\n",
       "  model: LogisticClassifier(lambda = 0.010000000000000004, …)\n",
       "  args: \n",
       "    1:\tSource @576 ⏎ Table{AbstractVector{Continuous}}\n",
       "    2:\tSource @820 ⏎ AbstractVector{Multiclass{2}}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the model with the best lambda\n",
    "best_model = LogisticClassifier(penalty=:l1, lambda=best_lambda)\n",
    "\n",
    "# Fit the best model\n",
    "best_model_fit = fit!(machine(best_model, X_train, y_train), verbosity=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>5×6 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">model</th><th style = \"text-align: left;\">accuracy</th><th style = \"text-align: left;\">precision</th><th style = \"text-align: left;\">recall</th><th style = \"text-align: left;\">f1</th><th style = \"text-align: left;\">auc</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"String\" style = \"text-align: left;\">String</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: left;\">GLM: Baseline</td><td style = \"text-align: right;\">0.817</td><td style = \"text-align: right;\">0.67</td><td style = \"text-align: right;\">0.597</td><td style = \"text-align: right;\">0.632</td><td style = \"text-align: right;\">0.859</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: left;\">GLM: Optimal Threshold</td><td style = \"text-align: right;\">0.8</td><td style = \"text-align: right;\">0.602</td><td style = \"text-align: right;\">0.699</td><td style = \"text-align: right;\">0.647</td><td style = \"text-align: right;\">0.859</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: left;\">GLM: Undersampling</td><td style = \"text-align: right;\">0.767</td><td style = \"text-align: right;\">0.752</td><td style = \"text-align: right;\">0.809</td><td style = \"text-align: right;\">0.78</td><td style = \"text-align: right;\">0.847</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: left;\">GLM: Oversampling</td><td style = \"text-align: right;\">0.787</td><td style = \"text-align: right;\">0.773</td><td style = \"text-align: right;\">0.827</td><td style = \"text-align: right;\">0.799</td><td style = \"text-align: right;\">0.87</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: left;\">GLM: L1 Reguralization</td><td style = \"text-align: right;\">0.79</td><td style = \"text-align: right;\">0.78</td><td style = \"text-align: right;\">0.822</td><td style = \"text-align: right;\">0.8</td><td style = \"text-align: right;\">0.867</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& model & accuracy & precision & recall & f1 & auc\\\\\n",
       "\t\\hline\n",
       "\t& String & Float64 & Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & GLM: Baseline & 0.817 & 0.67 & 0.597 & 0.632 & 0.859 \\\\\n",
       "\t2 & GLM: Optimal Threshold & 0.8 & 0.602 & 0.699 & 0.647 & 0.859 \\\\\n",
       "\t3 & GLM: Undersampling & 0.767 & 0.752 & 0.809 & 0.78 & 0.847 \\\\\n",
       "\t4 & GLM: Oversampling & 0.787 & 0.773 & 0.827 & 0.799 & 0.87 \\\\\n",
       "\t5 & GLM: L1 Reguralization & 0.79 & 0.78 & 0.822 & 0.8 & 0.867 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m5×6 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m model                  \u001b[0m\u001b[1m accuracy \u001b[0m\u001b[1m precision \u001b[0m\u001b[1m recall  \u001b[0m\u001b[1m f1      \u001b[0m\u001b[1m auc     \u001b[0m\n",
       "     │\u001b[90m String                 \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\n",
       "─────┼────────────────────────────────────────────────────────────────────────\n",
       "   1 │ GLM: Baseline              0.817      0.67     0.597    0.632    0.859\n",
       "   2 │ GLM: Optimal Threshold     0.8        0.602    0.699    0.647    0.859\n",
       "   3 │ GLM: Undersampling         0.767      0.752    0.809    0.78     0.847\n",
       "   4 │ GLM: Oversampling          0.787      0.773    0.827    0.799    0.87\n",
       "   5 │ GLM: L1 Reguralization     0.79       0.78     0.822    0.8      0.867"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = predict(best_model_fit, X_test)\n",
    "\n",
    "# Compute the metrics\n",
    "tuned_model_metrics = metrics(\n",
    "  \"GLM: L1 Reguralization\",\n",
    "  predictions,\n",
    "  y_test,\n",
    "  0.5\n",
    ")\n",
    "\n",
    "# Store the metrics in the DataFrame\n",
    "push!(all_model_metrics, tuned_model_metrics[1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5: Forward Stepwise Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───────────────────────────────────────────┬─────────────┬──────────────────────────┬────────────┬──────────────────────────┬──────────┬──────────┐\n",
      "│\u001b[1m variable                                  \u001b[0m│\u001b[1m mean        \u001b[0m│\u001b[1m min                      \u001b[0m│\u001b[1m median     \u001b[0m│\u001b[1m max                      \u001b[0m│\u001b[1m nmissing \u001b[0m│\u001b[1m eltype   \u001b[0m│\n",
      "│\u001b[90m Symbol                                    \u001b[0m│\u001b[90m Float64     \u001b[0m│\u001b[90m Real                     \u001b[0m│\u001b[90m Float64    \u001b[0m│\u001b[90m Real                     \u001b[0m│\u001b[90m Int64    \u001b[0m│\u001b[90m DataType \u001b[0m│\n",
      "│\u001b[90m Unknown                                   \u001b[0m│\u001b[90m Continuous  \u001b[0m│\u001b[90m Union{Continuous, Count} \u001b[0m│\u001b[90m Continuous \u001b[0m│\u001b[90m Union{Continuous, Count} \u001b[0m│\u001b[90m Count    \u001b[0m│\u001b[90m Unknown  \u001b[0m│\n",
      "├───────────────────────────────────────────┼─────────────┼──────────────────────────┼────────────┼──────────────────────────┼──────────┼──────────┤\n",
      "│ latitude                                  │ 0.000916545 │ -1.51763                 │ 0.0564237  │ 2.31211                  │ 0        │ Float64  │\n",
      "│ longitude                                 │ -0.00383142 │ -2.08666                 │ 0.0191435  │ 2.59842                  │ 0        │ Float64  │\n",
      "│ gender__Female                            │ 0.496143    │ 0.0                      │ 0.371577   │ 1.0                      │ 0        │ Float64  │\n",
      "│ gender__Male                              │ 0.503857    │ 0.0                      │ 0.628423   │ 1.0                      │ 0        │ Float64  │\n",
      "│ senior_citizen__No                        │ 0.812466    │ 0.0                      │ 1.0        │ 1.0                      │ 0        │ Float64  │\n",
      "│ senior_citizen__Yes                       │ 0.187534    │ 0.0                      │ 0.0        │ 1.0                      │ 0        │ Float64  │\n",
      "│ partner__No                               │ 0.558644    │ 0.0                      │ 1.0        │ 1.0                      │ 0        │ Float64  │\n",
      "│ partner__Yes                              │ 0.441356    │ 0.0                      │ 0.0        │ 1.0                      │ 0        │ Float64  │\n",
      "│ dependents__No                            │ 0.829716    │ 0.0                      │ 1.0        │ 1.0                      │ 0        │ Float64  │\n",
      "│ dependents__Yes                           │ 0.170284    │ 0.0                      │ 0.0        │ 1.0                      │ 0        │ Float64  │\n",
      "│ tenure_months                             │ -0.198359   │ -1.28016                 │ -0.496253  │ 1.61246                  │ 0        │ Float64  │\n",
      "│ phone_service__No                         │ 0.0953966   │ 0.0                      │ 0.0        │ 1.0                      │ 0        │ Float64  │\n",
      "│ phone_service__Yes                        │ 0.904603    │ 0.0                      │ 1.0        │ 1.0                      │ 0        │ Float64  │\n",
      "│ multiple_lines__No                        │ 0.474606    │ 0.0                      │ 0.0        │ 1.0                      │ 0        │ Float64  │\n",
      "│ multiple_lines__No phone service          │ 0.0953966   │ 0.0                      │ 0.0        │ 1.0                      │ 0        │ Float64  │\n",
      "│ multiple_lines__Yes                       │ 0.429998    │ 0.0                      │ 0.0        │ 1.0                      │ 0        │ Float64  │\n",
      "│ internet_service__DSL                     │ 0.309916    │ 0.0                      │ 0.0        │ 1.0                      │ 0        │ Float64  │\n",
      "│ internet_service__Fiber optic             │ 0.523277    │ 0.0                      │ 1.0        │ 1.0                      │ 0        │ Float64  │\n",
      "│ internet_service__No                      │ 0.166807    │ 0.0                      │ 0.0        │ 1.0                      │ 0        │ Float64  │\n",
      "│ device_protection__No                     │ 0.510762    │ 0.0                      │ 0.906031   │ 1.0                      │ 0        │ Float64  │\n",
      "│ device_protection__No internet service    │ 0.166807    │ 0.0                      │ 0.0        │ 1.0                      │ 0        │ Float64  │\n",
      "│ device_protection__Yes                    │ 0.322431    │ 0.0                      │ 0.0        │ 1.0                      │ 0        │ Float64  │\n",
      "│ tech_support__No                          │ 0.590473    │ 0.0                      │ 1.0        │ 1.0                      │ 0        │ Float64  │\n",
      "│ tech_support__No internet service         │ 0.166807    │ 0.0                      │ 0.0        │ 1.0                      │ 0        │ Float64  │\n",
      "│ tech_support__Yes                         │ 0.24272     │ 0.0                      │ 0.0        │ 1.0                      │ 0        │ Float64  │\n",
      "│ contract__Month-to-month                  │ 0.663712    │ 0.0                      │ 1.0        │ 1.0                      │ 0        │ Float64  │\n",
      "│ contract__One year                        │ 0.165942    │ 0.0                      │ 0.0        │ 1.0                      │ 0        │ Float64  │\n",
      "│ contract__Two year                        │ 0.170346    │ 0.0                      │ 0.0        │ 1.0                      │ 0        │ Float64  │\n",
      "│ paperless_billing__No                     │ 0.347847    │ 0.0                      │ 0.0        │ 1.0                      │ 0        │ Float64  │\n",
      "│ paperless_billing__Yes                    │ 0.652153    │ 0.0                      │ 1.0        │ 1.0                      │ 0        │ Float64  │\n",
      "│ payment_method__Bank transfer (automatic) │ 0.184772    │ 0.0                      │ 0.0        │ 1.0                      │ 0        │ Float64  │\n",
      "│ payment_method__Credit card (automatic)   │ 0.182147    │ 0.0                      │ 0.0        │ 1.0                      │ 0        │ Float64  │\n",
      "│ payment_method__Electronic check          │ 0.429058    │ 0.0                      │ 0.0        │ 1.0                      │ 0        │ Float64  │\n",
      "│ payment_method__Mailed check              │ 0.204023    │ 0.0                      │ 0.0        │ 1.0                      │ 0        │ Float64  │\n",
      "│ monthly_charges                           │ 0.101051    │ -1.54717                 │ 0.335641   │ 1.79325                  │ 0        │ Float64  │\n",
      "│ total_charges                             │ -0.115977   │ -0.998998                │ -0.511092  │ 2.82406                  │ 0        │ Float64  │\n",
      "│ cltv                                      │ -0.0651389  │ -2.02843                 │ 0.0470057  │ 1.77481                  │ 0        │ Float64  │\n",
      "│ churn_value                               │ 0.5         │ 0                        │ 0.5        │ 1                        │ 0        │ Int64    │\n",
      "└───────────────────────────────────────────┴─────────────┴──────────────────────────┴────────────┴──────────────────────────┴──────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "describe(telco_balanced_oversampling) |> pretty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stepwise Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stepwise_selection (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function stepwise_selection(df::DataFrame, verbose::Bool = true)\n",
    "  # Split the data\n",
    "  X_train, y_train, X_test, y_test = split_data(telco_balanced_oversampling)\n",
    "\n",
    "  # Initialize an empty model\n",
    "  remaining_features = names(X_train) # Assuming X_train is a DataFrame\n",
    "  selected_features = []\n",
    "\n",
    "  subset_selection_results = DataFrame(\n",
    "    step = Int[],\n",
    "    feature = String[],\n",
    "    train_accuracy = Float64[],\n",
    "    test_accuracy = Float64[]\n",
    "  )\n",
    "\n",
    "  for step in 1:length(remaining_features)\n",
    "\n",
    "    if verbose\n",
    "      println(\"Step: $step\")\n",
    "    end\n",
    "\n",
    "    best_feature = nothing\n",
    "    best_train_accuracy = 0.0\n",
    "    best_test_accuracy = 0.0\n",
    "\n",
    "    for feature in remaining_features\n",
    "        # Update the model to include the new feature\n",
    "        current_features = [selected_features; feature]\n",
    "\n",
    "        # Fit the model\n",
    "        model_fit = fit!(machine(LogisticClassifier(), X_train[:, current_features], y_train), verbosity=0)\n",
    "\n",
    "        # Training set results\n",
    "        train_predictions = predict(model_fit, X_train[:, current_features])\n",
    "        train_metrics = metrics(\"dummy\", train_predictions, y_train, 0.5)\n",
    "        train_accuracy = train_metrics.accuracy[1]\n",
    "\n",
    "        # Testing set results\n",
    "        test_predictions = predict(model_fit, X_test[:, current_features])\n",
    "        test_metrics = metrics(\"dummy\", test_predictions, y_test, 0.5)\n",
    "        test_accuracy = test_metrics.accuracy[1]\n",
    "\n",
    "        if test_accuracy > best_test_accuracy\n",
    "            best_feature = feature\n",
    "            best_test_accuracy = test_accuracy\n",
    "            best_train_accuracy = train_accuracy\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # Break if no improvement\n",
    "    if isnothing(best_feature)\n",
    "        println(\"No improvement, stopping.\")\n",
    "        break\n",
    "    end\n",
    "\n",
    "    # Update selected and remaining features\n",
    "    push!(selected_features, best_feature)\n",
    "    push!(subset_selection_results, (step, best_feature, best_train_accuracy, best_test_accuracy))\n",
    "    filter!(feature -> feature != best_feature, remaining_features)\n",
    "\n",
    "    if verbose\n",
    "        println(\"Selected feature: $best_feature, with Test Accuracy: $best_test_accuracy\")\n",
    "    end\n",
    "  end\n",
    "\n",
    "  return subset_selection_results\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stepwise Plotting Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "plot_subset_selection_results (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function plot_subset_selection_results(results::DataFrame)\n",
    "  # Plot the accuracy of the model with the selected features\n",
    "  subset_selection_plot = plot(\n",
    "    xlabel=\"Number of Features\",\n",
    "    ylabel=\"Accuracy\",\n",
    "    legend=:outerright,\n",
    "    bg=false\n",
    "  )\n",
    "\n",
    "  # Add the training accuracy to the plot\n",
    "  subset_selection_plot = plot!(\n",
    "    results.step,\n",
    "    results.train_accuracy,\n",
    "    label=\"Train Accuracy\",\n",
    "    color=primary_color,\n",
    "    linewidth=3,\n",
    "  )\n",
    "\n",
    "  # Add the testing accuracy to the plot\n",
    "  subset_selection_plot = plot!(\n",
    "    results.step,\n",
    "    results.test_accuracy,\n",
    "    label=\"Test Accuracy\",\n",
    "    color=secondary_color,\n",
    "    linewidth=3,\n",
    "  )\n",
    "\n",
    "  return subset_selection_plot\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing Forward Stepwise Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───────┬───────────────────────────────────────────┬────────────────┬───────────────┐\n",
      "│\u001b[1m step  \u001b[0m│\u001b[1m feature                                   \u001b[0m│\u001b[1m train_accuracy \u001b[0m│\u001b[1m test_accuracy \u001b[0m│\n",
      "│\u001b[90m Int64 \u001b[0m│\u001b[90m String                                    \u001b[0m│\u001b[90m Float64        \u001b[0m│\u001b[90m Float64       \u001b[0m│\n",
      "│\u001b[90m Count \u001b[0m│\u001b[90m Textual                                   \u001b[0m│\u001b[90m Continuous     \u001b[0m│\u001b[90m Continuous    \u001b[0m│\n",
      "├───────┼───────────────────────────────────────────┼────────────────┼───────────────┤\n",
      "│ 1     │ contract__Month-to-month                  │ 0.73           │ 0.731         │\n",
      "│ 2     │ dependents__No                            │ 0.756          │ 0.766         │\n",
      "│ 3     │ internet_service__No                      │ 0.767          │ 0.774         │\n",
      "│ 4     │ tenure_months                             │ 0.767          │ 0.776         │\n",
      "│ 5     │ internet_service__DSL                     │ 0.77           │ 0.78          │\n",
      "│ 6     │ payment_method__Electronic check          │ 0.773          │ 0.786         │\n",
      "│ 7     │ phone_service__No                         │ 0.776          │ 0.789         │\n",
      "│ 8     │ paperless_billing__No                     │ 0.777          │ 0.791         │\n",
      "│ 9     │ cltv                                      │ 0.777          │ 0.792         │\n",
      "│ 10    │ senior_citizen__Yes                       │ 0.777          │ 0.793         │\n",
      "│ 11    │ senior_citizen__No                        │ 0.777          │ 0.793         │\n",
      "│ 12    │ gender__Female                            │ 0.777          │ 0.793         │\n",
      "│ 13    │ payment_method__Mailed check              │ 0.777          │ 0.793         │\n",
      "│ 14    │ latitude                                  │ 0.778          │ 0.793         │\n",
      "│ 15    │ internet_service__Fiber optic             │ 0.777          │ 0.793         │\n",
      "│ 16    │ phone_service__Yes                        │ 0.777          │ 0.793         │\n",
      "│ 17    │ gender__Male                              │ 0.777          │ 0.792         │\n",
      "│ 18    │ dependents__Yes                           │ 0.776          │ 0.792         │\n",
      "│ 19    │ device_protection__No                     │ 0.776          │ 0.793         │\n",
      "│ 20    │ device_protection__Yes                    │ 0.777          │ 0.793         │\n",
      "│ 21    │ multiple_lines__No phone service          │ 0.777          │ 0.792         │\n",
      "│ 22    │ device_protection__No internet service    │ 0.776          │ 0.792         │\n",
      "│ 23    │ tech_support__No internet service         │ 0.776          │ 0.792         │\n",
      "│ 24    │ paperless_billing__Yes                    │ 0.777          │ 0.792         │\n",
      "│ 25    │ payment_method__Bank transfer (automatic) │ 0.777          │ 0.792         │\n",
      "│ 26    │ payment_method__Credit card (automatic)   │ 0.776          │ 0.792         │\n",
      "│ 27    │ longitude                                 │ 0.776          │ 0.791         │\n",
      "│ 28    │ contract__One year                        │ 0.775          │ 0.791         │\n",
      "│ 29    │ contract__Two year                        │ 0.775          │ 0.791         │\n",
      "│ 30    │ total_charges                             │ 0.78           │ 0.791         │\n",
      "│ 31    │ tech_support__No                          │ 0.78           │ 0.792         │\n",
      "│ 32    │ tech_support__Yes                         │ 0.781          │ 0.793         │\n",
      "│ 33    │ monthly_charges                           │ 0.781          │ 0.791         │\n",
      "│ 34    │ multiple_lines__Yes                       │ 0.782          │ 0.79          │\n",
      "│ 35    │ multiple_lines__No                        │ 0.781          │ 0.788         │\n",
      "│ 36    │ partner__No                               │ 0.782          │ 0.787         │\n",
      "│ 37    │ partner__Yes                              │ 0.782          │ 0.787         │\n",
      "└───────┴───────────────────────────────────────────┴────────────────┴───────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Run the stepwise selection\n",
    "subset_selection_result = stepwise_selection(telco_balanced_oversampling, false)\n",
    "\n",
    "# Display the results\n",
    "subset_selection_result |> pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/Users/redx/Library/CloudStorage/OneDrive-Personal/UCR/academics/winter_24/STAT206/project/images/subset_selection_plot.png\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "subset_selection_plot = subset_selection_result |> plot_subset_selection_results\n",
    "savefig(subset_selection_plot, \"images/subset_selection_plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the Model with only the Selected Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{String}:\n",
       " \"contract__Month-to-month\"\n",
       " \"dependents__No\"\n",
       " \"internet_service__No\"\n",
       " \"tenure_months\"\n",
       " \"internet_service__DSL\"\n",
       " \"payment_method__Electronic check\"\n",
       " \"phone_service__No\"\n",
       " \"paperless_billing__No\"\n",
       " \"cltv\"\n",
       " \"senior_citizen__Yes\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Join the first 10 feautres in a string array\n",
    "first_10_features = subset_selection_result[1:10, 2]\n",
    "\n",
    "# Split the data\n",
    "X_train, y_train, X_test, y_test = split_data(\n",
    "  DataFrames.select(\n",
    "    telco_balanced_oversampling,\n",
    "    [first_10_features; \"churn_value\"]\n",
    "  )\n",
    ")\n",
    "\n",
    "first_10_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trained Machine; caches model-specific representations of data\n",
       "  model: LogisticClassifier(lambda = 2.220446049250313e-16, …)\n",
       "  args: \n",
       "    1:\tSource @915 ⏎ Table{AbstractVector{Continuous}}\n",
       "    2:\tSource @535 ⏎ AbstractVector{Multiclass{2}}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_subset = LogisticClassifier()\n",
    "model_subset_fit = fit!(\n",
    "  machine(\n",
    "    model_subset,\n",
    "    X_train,\n",
    "    y_train\n",
    "  ),\n",
    "  verbosity = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>6×6 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">model</th><th style = \"text-align: left;\">accuracy</th><th style = \"text-align: left;\">precision</th><th style = \"text-align: left;\">recall</th><th style = \"text-align: left;\">f1</th><th style = \"text-align: left;\">auc</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"String\" style = \"text-align: left;\">String</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: left;\">GLM: Baseline</td><td style = \"text-align: right;\">0.817</td><td style = \"text-align: right;\">0.67</td><td style = \"text-align: right;\">0.597</td><td style = \"text-align: right;\">0.632</td><td style = \"text-align: right;\">0.859</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: left;\">GLM: Optimal Threshold</td><td style = \"text-align: right;\">0.8</td><td style = \"text-align: right;\">0.602</td><td style = \"text-align: right;\">0.699</td><td style = \"text-align: right;\">0.647</td><td style = \"text-align: right;\">0.859</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: left;\">GLM: Undersampling</td><td style = \"text-align: right;\">0.767</td><td style = \"text-align: right;\">0.752</td><td style = \"text-align: right;\">0.809</td><td style = \"text-align: right;\">0.78</td><td style = \"text-align: right;\">0.847</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: left;\">GLM: Oversampling</td><td style = \"text-align: right;\">0.787</td><td style = \"text-align: right;\">0.773</td><td style = \"text-align: right;\">0.827</td><td style = \"text-align: right;\">0.799</td><td style = \"text-align: right;\">0.87</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: left;\">GLM: L1 Reguralization</td><td style = \"text-align: right;\">0.79</td><td style = \"text-align: right;\">0.78</td><td style = \"text-align: right;\">0.822</td><td style = \"text-align: right;\">0.8</td><td style = \"text-align: right;\">0.867</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: left;\">GLM: Subset Selection</td><td style = \"text-align: right;\">0.793</td><td style = \"text-align: right;\">0.783</td><td style = \"text-align: right;\">0.825</td><td style = \"text-align: right;\">0.804</td><td style = \"text-align: right;\">0.868</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& model & accuracy & precision & recall & f1 & auc\\\\\n",
       "\t\\hline\n",
       "\t& String & Float64 & Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & GLM: Baseline & 0.817 & 0.67 & 0.597 & 0.632 & 0.859 \\\\\n",
       "\t2 & GLM: Optimal Threshold & 0.8 & 0.602 & 0.699 & 0.647 & 0.859 \\\\\n",
       "\t3 & GLM: Undersampling & 0.767 & 0.752 & 0.809 & 0.78 & 0.847 \\\\\n",
       "\t4 & GLM: Oversampling & 0.787 & 0.773 & 0.827 & 0.799 & 0.87 \\\\\n",
       "\t5 & GLM: L1 Reguralization & 0.79 & 0.78 & 0.822 & 0.8 & 0.867 \\\\\n",
       "\t6 & GLM: Subset Selection & 0.793 & 0.783 & 0.825 & 0.804 & 0.868 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m6×6 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m model                  \u001b[0m\u001b[1m accuracy \u001b[0m\u001b[1m precision \u001b[0m\u001b[1m recall  \u001b[0m\u001b[1m f1      \u001b[0m\u001b[1m auc     \u001b[0m\n",
       "     │\u001b[90m String                 \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\n",
       "─────┼────────────────────────────────────────────────────────────────────────\n",
       "   1 │ GLM: Baseline              0.817      0.67     0.597    0.632    0.859\n",
       "   2 │ GLM: Optimal Threshold     0.8        0.602    0.699    0.647    0.859\n",
       "   3 │ GLM: Undersampling         0.767      0.752    0.809    0.78     0.847\n",
       "   4 │ GLM: Oversampling          0.787      0.773    0.827    0.799    0.87\n",
       "   5 │ GLM: L1 Reguralization     0.79       0.78     0.822    0.8      0.867\n",
       "   6 │ GLM: Subset Selection      0.793      0.783    0.825    0.804    0.868"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = predict(model_subset_fit, X_test)\n",
    "\n",
    "# Compute the metrics\n",
    "model_subset_metrics = metrics(\n",
    "  \"GLM: Subset Selection\",\n",
    "  predictions,\n",
    "  y_test,\n",
    "  0.5\n",
    ")\n",
    "\n",
    "# Store the metrics in the DataFrame\n",
    "push!(all_model_metrics, model_subset_metrics[1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: The Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trained Machine; caches model-specific representations of data\n",
       "  model: RandomForestClassifier(max_depth = -1, …)\n",
       "  args: \n",
       "    1:\tSource @186 ⏎ Table{AbstractVector{Continuous}}\n",
       "    2:\tSource @685 ⏎ AbstractVector{Multiclass{2}}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split the data\n",
    "X_train, y_train, X_test, y_test = split_data(telco)\n",
    "\n",
    "# Build the machine with Random Forest Classifier\n",
    "model_rf_baseline = RandomForestClassifier()\n",
    "\n",
    "model_rf_baseline_fit = fit!(machine(model_rf_baseline, X_train, y_train), verbosity=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>7×6 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">model</th><th style = \"text-align: left;\">accuracy</th><th style = \"text-align: left;\">precision</th><th style = \"text-align: left;\">recall</th><th style = \"text-align: left;\">f1</th><th style = \"text-align: left;\">auc</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"String\" style = \"text-align: left;\">String</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: left;\">GLM: Baseline</td><td style = \"text-align: right;\">0.817</td><td style = \"text-align: right;\">0.67</td><td style = \"text-align: right;\">0.597</td><td style = \"text-align: right;\">0.632</td><td style = \"text-align: right;\">0.859</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: left;\">GLM: Optimal Threshold</td><td style = \"text-align: right;\">0.8</td><td style = \"text-align: right;\">0.602</td><td style = \"text-align: right;\">0.699</td><td style = \"text-align: right;\">0.647</td><td style = \"text-align: right;\">0.859</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: left;\">GLM: Undersampling</td><td style = \"text-align: right;\">0.767</td><td style = \"text-align: right;\">0.752</td><td style = \"text-align: right;\">0.809</td><td style = \"text-align: right;\">0.78</td><td style = \"text-align: right;\">0.847</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: left;\">GLM: Oversampling</td><td style = \"text-align: right;\">0.787</td><td style = \"text-align: right;\">0.773</td><td style = \"text-align: right;\">0.827</td><td style = \"text-align: right;\">0.799</td><td style = \"text-align: right;\">0.87</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: left;\">GLM: L1 Reguralization</td><td style = \"text-align: right;\">0.79</td><td style = \"text-align: right;\">0.78</td><td style = \"text-align: right;\">0.822</td><td style = \"text-align: right;\">0.8</td><td style = \"text-align: right;\">0.867</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: left;\">GLM: Subset Selection</td><td style = \"text-align: right;\">0.793</td><td style = \"text-align: right;\">0.783</td><td style = \"text-align: right;\">0.825</td><td style = \"text-align: right;\">0.804</td><td style = \"text-align: right;\">0.868</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: left;\">Random Forest: Baseline</td><td style = \"text-align: right;\">0.804</td><td style = \"text-align: right;\">0.661</td><td style = \"text-align: right;\">0.523</td><td style = \"text-align: right;\">0.584</td><td style = \"text-align: right;\">0.853</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& model & accuracy & precision & recall & f1 & auc\\\\\n",
       "\t\\hline\n",
       "\t& String & Float64 & Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & GLM: Baseline & 0.817 & 0.67 & 0.597 & 0.632 & 0.859 \\\\\n",
       "\t2 & GLM: Optimal Threshold & 0.8 & 0.602 & 0.699 & 0.647 & 0.859 \\\\\n",
       "\t3 & GLM: Undersampling & 0.767 & 0.752 & 0.809 & 0.78 & 0.847 \\\\\n",
       "\t4 & GLM: Oversampling & 0.787 & 0.773 & 0.827 & 0.799 & 0.87 \\\\\n",
       "\t5 & GLM: L1 Reguralization & 0.79 & 0.78 & 0.822 & 0.8 & 0.867 \\\\\n",
       "\t6 & GLM: Subset Selection & 0.793 & 0.783 & 0.825 & 0.804 & 0.868 \\\\\n",
       "\t7 & Random Forest: Baseline & 0.804 & 0.661 & 0.523 & 0.584 & 0.853 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m7×6 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m model                   \u001b[0m\u001b[1m accuracy \u001b[0m\u001b[1m precision \u001b[0m\u001b[1m recall  \u001b[0m\u001b[1m f1      \u001b[0m\u001b[1m auc    \u001b[0m ⋯\n",
       "     │\u001b[90m String                  \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64\u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │ GLM: Baseline               0.817      0.67     0.597    0.632    0.859 ⋯\n",
       "   2 │ GLM: Optimal Threshold      0.8        0.602    0.699    0.647    0.859\n",
       "   3 │ GLM: Undersampling          0.767      0.752    0.809    0.78     0.847\n",
       "   4 │ GLM: Oversampling           0.787      0.773    0.827    0.799    0.87\n",
       "   5 │ GLM: L1 Reguralization      0.79       0.78     0.822    0.8      0.867 ⋯\n",
       "   6 │ GLM: Subset Selection       0.793      0.783    0.825    0.804    0.868\n",
       "   7 │ Random Forest: Baseline     0.804      0.661    0.523    0.584    0.853"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = predict(model_rf_baseline_fit, X_test)\n",
    "\n",
    "# Compute the metrics\n",
    "model_rf_baseline_metrics = metrics(\n",
    "  \"Random Forest: Baseline\",\n",
    "  predictions,\n",
    "  y_test,\n",
    "  0.5\n",
    ")\n",
    "\n",
    "# Store the metrics in the DataFrame\n",
    "push!(all_model_metrics, model_rf_baseline_metrics[1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Addressing Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trained Machine; caches model-specific representations of data\n",
       "  model: RandomForestClassifier(max_depth = -1, …)\n",
       "  args: \n",
       "    1:\tSource @519 ⏎ Table{AbstractVector{Continuous}}\n",
       "    2:\tSource @101 ⏎ AbstractVector{Multiclass{2}}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split the data\n",
    "X_train, y_train, X_test, y_test = split_data(telco_balanced_oversampling)\n",
    "\n",
    "# Build the machine with Random Forest Classifier\n",
    "model_rf_balanced = RandomForestClassifier()\n",
    "\n",
    "model_rf_baseline_fit = fit!(machine(model_rf_balanced, X_train, y_train), verbosity=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>8×6 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">model</th><th style = \"text-align: left;\">accuracy</th><th style = \"text-align: left;\">precision</th><th style = \"text-align: left;\">recall</th><th style = \"text-align: left;\">f1</th><th style = \"text-align: left;\">auc</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"String\" style = \"text-align: left;\">String</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: left;\">GLM: Baseline</td><td style = \"text-align: right;\">0.817</td><td style = \"text-align: right;\">0.67</td><td style = \"text-align: right;\">0.597</td><td style = \"text-align: right;\">0.632</td><td style = \"text-align: right;\">0.859</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: left;\">GLM: Optimal Threshold</td><td style = \"text-align: right;\">0.8</td><td style = \"text-align: right;\">0.602</td><td style = \"text-align: right;\">0.699</td><td style = \"text-align: right;\">0.647</td><td style = \"text-align: right;\">0.859</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: left;\">GLM: Undersampling</td><td style = \"text-align: right;\">0.767</td><td style = \"text-align: right;\">0.752</td><td style = \"text-align: right;\">0.809</td><td style = \"text-align: right;\">0.78</td><td style = \"text-align: right;\">0.847</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: left;\">GLM: Oversampling</td><td style = \"text-align: right;\">0.787</td><td style = \"text-align: right;\">0.773</td><td style = \"text-align: right;\">0.827</td><td style = \"text-align: right;\">0.799</td><td style = \"text-align: right;\">0.87</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: left;\">GLM: L1 Reguralization</td><td style = \"text-align: right;\">0.79</td><td style = \"text-align: right;\">0.78</td><td style = \"text-align: right;\">0.822</td><td style = \"text-align: right;\">0.8</td><td style = \"text-align: right;\">0.867</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: left;\">GLM: Subset Selection</td><td style = \"text-align: right;\">0.793</td><td style = \"text-align: right;\">0.783</td><td style = \"text-align: right;\">0.825</td><td style = \"text-align: right;\">0.804</td><td style = \"text-align: right;\">0.868</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: left;\">Random Forest: Baseline</td><td style = \"text-align: right;\">0.804</td><td style = \"text-align: right;\">0.661</td><td style = \"text-align: right;\">0.523</td><td style = \"text-align: right;\">0.584</td><td style = \"text-align: right;\">0.853</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: left;\">Random Forest: Oversampling</td><td style = \"text-align: right;\">0.865</td><td style = \"text-align: right;\">0.856</td><td style = \"text-align: right;\">0.885</td><td style = \"text-align: right;\">0.87</td><td style = \"text-align: right;\">0.942</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& model & accuracy & precision & recall & f1 & auc\\\\\n",
       "\t\\hline\n",
       "\t& String & Float64 & Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & GLM: Baseline & 0.817 & 0.67 & 0.597 & 0.632 & 0.859 \\\\\n",
       "\t2 & GLM: Optimal Threshold & 0.8 & 0.602 & 0.699 & 0.647 & 0.859 \\\\\n",
       "\t3 & GLM: Undersampling & 0.767 & 0.752 & 0.809 & 0.78 & 0.847 \\\\\n",
       "\t4 & GLM: Oversampling & 0.787 & 0.773 & 0.827 & 0.799 & 0.87 \\\\\n",
       "\t5 & GLM: L1 Reguralization & 0.79 & 0.78 & 0.822 & 0.8 & 0.867 \\\\\n",
       "\t6 & GLM: Subset Selection & 0.793 & 0.783 & 0.825 & 0.804 & 0.868 \\\\\n",
       "\t7 & Random Forest: Baseline & 0.804 & 0.661 & 0.523 & 0.584 & 0.853 \\\\\n",
       "\t8 & Random Forest: Oversampling & 0.865 & 0.856 & 0.885 & 0.87 & 0.942 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m8×6 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m model                       \u001b[0m\u001b[1m accuracy \u001b[0m\u001b[1m precision \u001b[0m\u001b[1m recall  \u001b[0m\u001b[1m f1      \u001b[0m\u001b[1m auc\u001b[0m ⋯\n",
       "     │\u001b[90m String                      \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Flo\u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │ GLM: Baseline                   0.817      0.67     0.597    0.632    0 ⋯\n",
       "   2 │ GLM: Optimal Threshold          0.8        0.602    0.699    0.647    0\n",
       "   3 │ GLM: Undersampling              0.767      0.752    0.809    0.78     0\n",
       "   4 │ GLM: Oversampling               0.787      0.773    0.827    0.799    0\n",
       "   5 │ GLM: L1 Reguralization          0.79       0.78     0.822    0.8      0 ⋯\n",
       "   6 │ GLM: Subset Selection           0.793      0.783    0.825    0.804    0\n",
       "   7 │ Random Forest: Baseline         0.804      0.661    0.523    0.584    0\n",
       "   8 │ Random Forest: Oversampling     0.865      0.856    0.885    0.87     0\n",
       "\u001b[36m                                                                1 column omitted\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = predict(model_rf_baseline_fit, X_test)\n",
    "\n",
    "# Compute the metrics\n",
    "model_rf_baseline_metrics = metrics(\n",
    "  \"Random Forest: Oversampling\",\n",
    "  predictions,\n",
    "  y_test,\n",
    "  0.5\n",
    ")\n",
    "\n",
    "# Store the metrics in the DataFrame\n",
    "push!(all_model_metrics, model_rf_baseline_metrics[1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProbabilisticTunedModel(\n",
       "  model = RandomForestClassifier(\n",
       "        max_depth = -1, \n",
       "        min_samples_leaf = 1, \n",
       "        min_samples_split = 2, \n",
       "        min_purity_increase = 0.0, \n",
       "        n_subfeatures = -1, \n",
       "        n_trees = 100, \n",
       "        sampling_fraction = 0.7, \n",
       "        feature_importance = :impurity, \n",
       "        rng = Random._GLOBAL_RNG()), \n",
       "  tuning = RandomSearch(\n",
       "        bounded = Distributions.Uniform, \n",
       "        positive_unbounded = Distributions.Gamma, \n",
       "        other = Distributions.Normal, \n",
       "        rng = Random._GLOBAL_RNG()), \n",
       "  resampling = CV(\n",
       "        nfolds = 5, \n",
       "        shuffle = false, \n",
       "        rng = Random._GLOBAL_RNG()), \n",
       "  measure = Accuracy(), \n",
       "  weights = nothing, \n",
       "  class_weights = nothing, \n",
       "  operation = nothing, \n",
       "  range = NumericRange{Int64, MLJBase.Bounded, Symbol}[NumericRange(10 ≤ n_trees ≤ 100; origin=55.0, unit=45.0), NumericRange(1 ≤ max_depth ≤ 30; origin=15.5, unit=14.5), NumericRange(2 ≤ min_samples_split ≤ 20; origin=11.0, unit=9.0)], \n",
       "  selection_heuristic = NaiveSelection(nothing), \n",
       "  train_best = true, \n",
       "  repeats = 1, \n",
       "  n = 50, \n",
       "  acceleration = CPU1{Nothing}(nothing), \n",
       "  acceleration_resampling = CPU1{Nothing}(nothing), \n",
       "  check_measure = true, \n",
       "  cache = true)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split the data\n",
    "X_train, y_train, X_test, y_test = split_data(telco_balanced_oversampling)\n",
    "\n",
    "# Define the model\n",
    "model_rf_tuned = RandomForestClassifier()\n",
    "\n",
    "# Define the range for the number of trees\n",
    "n_trees_range = range(model_rf_tuned, :n_trees, lower=10, upper=100)\n",
    "max_depth_range = range(model_rf_tuned, :max_depth, lower=1, upper=30)\n",
    "min_samples_split_range = range(model_rf_tuned, :min_samples_split, lower=2, upper=20)\n",
    "\n",
    "tuned_model = TunedModel(\n",
    "    model=model_rf_tuned,\n",
    "    resampling=CV(nfolds=5),\n",
    "    ranges=[n_trees_range, max_depth_range, min_samples_split_range],\n",
    "    measure=accuracy,\n",
    "    n=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(best_model = RandomForestClassifier(max_depth = 28, …),\n",
       " best_fitted_params = (forest = Ensemble of Decision Trees\n",
       "Trees:      97\n",
       "Avg Leaves: 699.8144329896908\n",
       "Avg Depth:  26.47422680412371,),)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming X_train, y_train from your data preparation step\n",
    "model_rf_tuned_fit = fit!(machine(tuned_model, X_train, y_train), verbosity = 0)\n",
    "\n",
    "# Params\n",
    "params = fitted_params(model_rf_tuned_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trained Machine; caches model-specific representations of data\n",
       "  model: RandomForestClassifier(max_depth = 28, …)\n",
       "  args: \n",
       "    1:\tSource @955 ⏎ Table{AbstractVector{Continuous}}\n",
       "    2:\tSource @052 ⏎ AbstractVector{Multiclass{2}}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_rf_tuned_best_model = params.best_model\n",
    "\n",
    "# Fit the best model\n",
    "model_rf_tuned_best_model_fit = fit!(machine(model_rf_tuned_best_model, X_train, y_train), verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>9×6 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">model</th><th style = \"text-align: left;\">accuracy</th><th style = \"text-align: left;\">precision</th><th style = \"text-align: left;\">recall</th><th style = \"text-align: left;\">f1</th><th style = \"text-align: left;\">auc</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"String\" style = \"text-align: left;\">String</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: left;\">GLM: Baseline</td><td style = \"text-align: right;\">0.817</td><td style = \"text-align: right;\">0.67</td><td style = \"text-align: right;\">0.597</td><td style = \"text-align: right;\">0.632</td><td style = \"text-align: right;\">0.859</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: left;\">GLM: Optimal Threshold</td><td style = \"text-align: right;\">0.8</td><td style = \"text-align: right;\">0.602</td><td style = \"text-align: right;\">0.699</td><td style = \"text-align: right;\">0.647</td><td style = \"text-align: right;\">0.859</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: left;\">GLM: Undersampling</td><td style = \"text-align: right;\">0.767</td><td style = \"text-align: right;\">0.752</td><td style = \"text-align: right;\">0.809</td><td style = \"text-align: right;\">0.78</td><td style = \"text-align: right;\">0.847</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: left;\">GLM: Oversampling</td><td style = \"text-align: right;\">0.787</td><td style = \"text-align: right;\">0.773</td><td style = \"text-align: right;\">0.827</td><td style = \"text-align: right;\">0.799</td><td style = \"text-align: right;\">0.87</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: left;\">GLM: L1 Reguralization</td><td style = \"text-align: right;\">0.79</td><td style = \"text-align: right;\">0.78</td><td style = \"text-align: right;\">0.822</td><td style = \"text-align: right;\">0.8</td><td style = \"text-align: right;\">0.867</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: left;\">GLM: Subset Selection</td><td style = \"text-align: right;\">0.793</td><td style = \"text-align: right;\">0.783</td><td style = \"text-align: right;\">0.825</td><td style = \"text-align: right;\">0.804</td><td style = \"text-align: right;\">0.868</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: left;\">Random Forest: Baseline</td><td style = \"text-align: right;\">0.804</td><td style = \"text-align: right;\">0.661</td><td style = \"text-align: right;\">0.523</td><td style = \"text-align: right;\">0.584</td><td style = \"text-align: right;\">0.853</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: left;\">Random Forest: Oversampling</td><td style = \"text-align: right;\">0.865</td><td style = \"text-align: right;\">0.856</td><td style = \"text-align: right;\">0.885</td><td style = \"text-align: right;\">0.87</td><td style = \"text-align: right;\">0.942</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: left;\">Random Forest: Hyperparameter Tuning</td><td style = \"text-align: right;\">0.865</td><td style = \"text-align: right;\">0.847</td><td style = \"text-align: right;\">0.899</td><td style = \"text-align: right;\">0.873</td><td style = \"text-align: right;\">0.94</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& model & accuracy & precision & recall & f1 & auc\\\\\n",
       "\t\\hline\n",
       "\t& String & Float64 & Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & GLM: Baseline & 0.817 & 0.67 & 0.597 & 0.632 & 0.859 \\\\\n",
       "\t2 & GLM: Optimal Threshold & 0.8 & 0.602 & 0.699 & 0.647 & 0.859 \\\\\n",
       "\t3 & GLM: Undersampling & 0.767 & 0.752 & 0.809 & 0.78 & 0.847 \\\\\n",
       "\t4 & GLM: Oversampling & 0.787 & 0.773 & 0.827 & 0.799 & 0.87 \\\\\n",
       "\t5 & GLM: L1 Reguralization & 0.79 & 0.78 & 0.822 & 0.8 & 0.867 \\\\\n",
       "\t6 & GLM: Subset Selection & 0.793 & 0.783 & 0.825 & 0.804 & 0.868 \\\\\n",
       "\t7 & Random Forest: Baseline & 0.804 & 0.661 & 0.523 & 0.584 & 0.853 \\\\\n",
       "\t8 & Random Forest: Oversampling & 0.865 & 0.856 & 0.885 & 0.87 & 0.942 \\\\\n",
       "\t9 & Random Forest: Hyperparameter Tuning & 0.865 & 0.847 & 0.899 & 0.873 & 0.94 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m9×6 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m model                             \u001b[0m\u001b[1m accuracy \u001b[0m\u001b[1m precision \u001b[0m\u001b[1m recall  \u001b[0m\u001b[1m f1    \u001b[0m ⋯\n",
       "     │\u001b[90m String                            \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float6\u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │ GLM: Baseline                         0.817      0.67     0.597    0.63 ⋯\n",
       "   2 │ GLM: Optimal Threshold                0.8        0.602    0.699    0.64\n",
       "   3 │ GLM: Undersampling                    0.767      0.752    0.809    0.78\n",
       "   4 │ GLM: Oversampling                     0.787      0.773    0.827    0.79\n",
       "   5 │ GLM: L1 Reguralization                0.79       0.78     0.822    0.8  ⋯\n",
       "   6 │ GLM: Subset Selection                 0.793      0.783    0.825    0.80\n",
       "   7 │ Random Forest: Baseline               0.804      0.661    0.523    0.58\n",
       "   8 │ Random Forest: Oversampling           0.865      0.856    0.885    0.87\n",
       "   9 │ Random Forest: Hyperparameter Tu…     0.865      0.847    0.899    0.87 ⋯\n",
       "\u001b[36m                                                               2 columns omitted\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = predict(model_rf_tuned_best_model_fit, X_test)\n",
    "\n",
    "# Compute the metrics\n",
    "model_rf_tuned_metrics = metrics(\n",
    "  \"Random Forest: Hyperparameter Tuning\",\n",
    "  predictions,\n",
    "  y_test,\n",
    "  0.5\n",
    ")\n",
    "\n",
    "# Store the metrics in the DataFrame\n",
    "push!(all_model_metrics, model_rf_tuned_metrics[1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: The Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trained Machine; caches model-specific representations of data\n",
       "  model: KNNClassifier(K = 5, …)\n",
       "  args: \n",
       "    1:\tSource @711 ⏎ Table{AbstractVector{Continuous}}\n",
       "    2:\tSource @696 ⏎ AbstractVector{Multiclass{2}}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split the data\n",
    "X_train, y_train, X_test, y_test = split_data(telco_balanced_oversampling)\n",
    "\n",
    "# Define the model\n",
    "model_knn_baseline = NearestNeighborModels.KNNClassifier()\n",
    "\n",
    "# Fit the model\n",
    "model_knn_baseline_fit = fit!(machine(model_knn_baseline, X_train, y_train), verbosity=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>10×6 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">model</th><th style = \"text-align: left;\">accuracy</th><th style = \"text-align: left;\">precision</th><th style = \"text-align: left;\">recall</th><th style = \"text-align: left;\">f1</th><th style = \"text-align: left;\">auc</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"String\" style = \"text-align: left;\">String</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: left;\">GLM: Baseline</td><td style = \"text-align: right;\">0.817</td><td style = \"text-align: right;\">0.67</td><td style = \"text-align: right;\">0.597</td><td style = \"text-align: right;\">0.632</td><td style = \"text-align: right;\">0.859</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: left;\">GLM: Optimal Threshold</td><td style = \"text-align: right;\">0.8</td><td style = \"text-align: right;\">0.602</td><td style = \"text-align: right;\">0.699</td><td style = \"text-align: right;\">0.647</td><td style = \"text-align: right;\">0.859</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: left;\">GLM: Undersampling</td><td style = \"text-align: right;\">0.767</td><td style = \"text-align: right;\">0.752</td><td style = \"text-align: right;\">0.809</td><td style = \"text-align: right;\">0.78</td><td style = \"text-align: right;\">0.847</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: left;\">GLM: Oversampling</td><td style = \"text-align: right;\">0.787</td><td style = \"text-align: right;\">0.773</td><td style = \"text-align: right;\">0.827</td><td style = \"text-align: right;\">0.799</td><td style = \"text-align: right;\">0.87</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: left;\">GLM: L1 Reguralization</td><td style = \"text-align: right;\">0.79</td><td style = \"text-align: right;\">0.78</td><td style = \"text-align: right;\">0.822</td><td style = \"text-align: right;\">0.8</td><td style = \"text-align: right;\">0.867</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: left;\">GLM: Subset Selection</td><td style = \"text-align: right;\">0.793</td><td style = \"text-align: right;\">0.783</td><td style = \"text-align: right;\">0.825</td><td style = \"text-align: right;\">0.804</td><td style = \"text-align: right;\">0.868</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: left;\">Random Forest: Baseline</td><td style = \"text-align: right;\">0.804</td><td style = \"text-align: right;\">0.661</td><td style = \"text-align: right;\">0.523</td><td style = \"text-align: right;\">0.584</td><td style = \"text-align: right;\">0.853</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: left;\">Random Forest: Oversampling</td><td style = \"text-align: right;\">0.865</td><td style = \"text-align: right;\">0.856</td><td style = \"text-align: right;\">0.885</td><td style = \"text-align: right;\">0.87</td><td style = \"text-align: right;\">0.942</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: left;\">Random Forest: Hyperparameter Tuning</td><td style = \"text-align: right;\">0.865</td><td style = \"text-align: right;\">0.847</td><td style = \"text-align: right;\">0.899</td><td style = \"text-align: right;\">0.873</td><td style = \"text-align: right;\">0.94</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">10</td><td style = \"text-align: left;\">K-Nearest Neighbors: Baseline</td><td style = \"text-align: right;\">0.805</td><td style = \"text-align: right;\">0.752</td><td style = \"text-align: right;\">0.926</td><td style = \"text-align: right;\">0.83</td><td style = \"text-align: right;\">0.877</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& model & accuracy & precision & recall & f1 & auc\\\\\n",
       "\t\\hline\n",
       "\t& String & Float64 & Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & GLM: Baseline & 0.817 & 0.67 & 0.597 & 0.632 & 0.859 \\\\\n",
       "\t2 & GLM: Optimal Threshold & 0.8 & 0.602 & 0.699 & 0.647 & 0.859 \\\\\n",
       "\t3 & GLM: Undersampling & 0.767 & 0.752 & 0.809 & 0.78 & 0.847 \\\\\n",
       "\t4 & GLM: Oversampling & 0.787 & 0.773 & 0.827 & 0.799 & 0.87 \\\\\n",
       "\t5 & GLM: L1 Reguralization & 0.79 & 0.78 & 0.822 & 0.8 & 0.867 \\\\\n",
       "\t6 & GLM: Subset Selection & 0.793 & 0.783 & 0.825 & 0.804 & 0.868 \\\\\n",
       "\t7 & Random Forest: Baseline & 0.804 & 0.661 & 0.523 & 0.584 & 0.853 \\\\\n",
       "\t8 & Random Forest: Oversampling & 0.865 & 0.856 & 0.885 & 0.87 & 0.942 \\\\\n",
       "\t9 & Random Forest: Hyperparameter Tuning & 0.865 & 0.847 & 0.899 & 0.873 & 0.94 \\\\\n",
       "\t10 & K-Nearest Neighbors: Baseline & 0.805 & 0.752 & 0.926 & 0.83 & 0.877 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m10×6 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m model                             \u001b[0m\u001b[1m accuracy \u001b[0m\u001b[1m precision \u001b[0m\u001b[1m recall  \u001b[0m\u001b[1m f1    \u001b[0m ⋯\n",
       "     │\u001b[90m String                            \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float6\u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │ GLM: Baseline                         0.817      0.67     0.597    0.63 ⋯\n",
       "   2 │ GLM: Optimal Threshold                0.8        0.602    0.699    0.64\n",
       "   3 │ GLM: Undersampling                    0.767      0.752    0.809    0.78\n",
       "   4 │ GLM: Oversampling                     0.787      0.773    0.827    0.79\n",
       "   5 │ GLM: L1 Reguralization                0.79       0.78     0.822    0.8  ⋯\n",
       "   6 │ GLM: Subset Selection                 0.793      0.783    0.825    0.80\n",
       "   7 │ Random Forest: Baseline               0.804      0.661    0.523    0.58\n",
       "   8 │ Random Forest: Oversampling           0.865      0.856    0.885    0.87\n",
       "   9 │ Random Forest: Hyperparameter Tu…     0.865      0.847    0.899    0.87 ⋯\n",
       "  10 │ K-Nearest Neighbors: Baseline         0.805      0.752    0.926    0.83\n",
       "\u001b[36m                                                               2 columns omitted\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = predict(model_knn_baseline_fit, X_test)\n",
    "\n",
    "# Compute the metrics\n",
    "model_knn_baseline_metrics = metrics(\n",
    "  \"K-Nearest Neighbors: Baseline\",\n",
    "  predictions,\n",
    "  y_test,\n",
    "  0.5\n",
    ")\n",
    "\n",
    "# Store the metrics in the DataFrame\n",
    "push!(all_model_metrics, model_knn_baseline_metrics[1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Balancing Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trained Machine; caches model-specific representations of data\n",
       "  model: KNNClassifier(K = 5, …)\n",
       "  args: \n",
       "    1:\tSource @342 ⏎ Table{AbstractVector{Continuous}}\n",
       "    2:\tSource @518 ⏎ AbstractVector{Multiclass{2}}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split the data\n",
    "X_train, y_train, X_test, y_test = split_data(telco_balanced_oversampling)\n",
    "\n",
    "# Define the model\n",
    "model_knn_oversampling = NearestNeighborModels.KNNClassifier()\n",
    "\n",
    "# Fit the model\n",
    "model_knn_oversampling_fit = fit!(machine(model_knn_oversampling, X_train, y_train), verbosity=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>11×6 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">model</th><th style = \"text-align: left;\">accuracy</th><th style = \"text-align: left;\">precision</th><th style = \"text-align: left;\">recall</th><th style = \"text-align: left;\">f1</th><th style = \"text-align: left;\">auc</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"String\" style = \"text-align: left;\">String</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: left;\">GLM: Baseline</td><td style = \"text-align: right;\">0.817</td><td style = \"text-align: right;\">0.67</td><td style = \"text-align: right;\">0.597</td><td style = \"text-align: right;\">0.632</td><td style = \"text-align: right;\">0.859</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: left;\">GLM: Optimal Threshold</td><td style = \"text-align: right;\">0.8</td><td style = \"text-align: right;\">0.602</td><td style = \"text-align: right;\">0.699</td><td style = \"text-align: right;\">0.647</td><td style = \"text-align: right;\">0.859</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: left;\">GLM: Undersampling</td><td style = \"text-align: right;\">0.767</td><td style = \"text-align: right;\">0.752</td><td style = \"text-align: right;\">0.809</td><td style = \"text-align: right;\">0.78</td><td style = \"text-align: right;\">0.847</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: left;\">GLM: Oversampling</td><td style = \"text-align: right;\">0.787</td><td style = \"text-align: right;\">0.773</td><td style = \"text-align: right;\">0.827</td><td style = \"text-align: right;\">0.799</td><td style = \"text-align: right;\">0.87</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: left;\">GLM: L1 Reguralization</td><td style = \"text-align: right;\">0.79</td><td style = \"text-align: right;\">0.78</td><td style = \"text-align: right;\">0.822</td><td style = \"text-align: right;\">0.8</td><td style = \"text-align: right;\">0.867</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: left;\">GLM: Subset Selection</td><td style = \"text-align: right;\">0.793</td><td style = \"text-align: right;\">0.783</td><td style = \"text-align: right;\">0.825</td><td style = \"text-align: right;\">0.804</td><td style = \"text-align: right;\">0.868</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: left;\">Random Forest: Baseline</td><td style = \"text-align: right;\">0.804</td><td style = \"text-align: right;\">0.661</td><td style = \"text-align: right;\">0.523</td><td style = \"text-align: right;\">0.584</td><td style = \"text-align: right;\">0.853</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: left;\">Random Forest: Oversampling</td><td style = \"text-align: right;\">0.865</td><td style = \"text-align: right;\">0.856</td><td style = \"text-align: right;\">0.885</td><td style = \"text-align: right;\">0.87</td><td style = \"text-align: right;\">0.942</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: left;\">Random Forest: Hyperparameter Tuning</td><td style = \"text-align: right;\">0.865</td><td style = \"text-align: right;\">0.847</td><td style = \"text-align: right;\">0.899</td><td style = \"text-align: right;\">0.873</td><td style = \"text-align: right;\">0.94</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">10</td><td style = \"text-align: left;\">K-Nearest Neighbors: Baseline</td><td style = \"text-align: right;\">0.805</td><td style = \"text-align: right;\">0.752</td><td style = \"text-align: right;\">0.926</td><td style = \"text-align: right;\">0.83</td><td style = \"text-align: right;\">0.877</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">11</td><td style = \"text-align: left;\">K-Nearest Neighbors: Oversampling</td><td style = \"text-align: right;\">0.805</td><td style = \"text-align: right;\">0.752</td><td style = \"text-align: right;\">0.926</td><td style = \"text-align: right;\">0.83</td><td style = \"text-align: right;\">0.877</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& model & accuracy & precision & recall & f1 & auc\\\\\n",
       "\t\\hline\n",
       "\t& String & Float64 & Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & GLM: Baseline & 0.817 & 0.67 & 0.597 & 0.632 & 0.859 \\\\\n",
       "\t2 & GLM: Optimal Threshold & 0.8 & 0.602 & 0.699 & 0.647 & 0.859 \\\\\n",
       "\t3 & GLM: Undersampling & 0.767 & 0.752 & 0.809 & 0.78 & 0.847 \\\\\n",
       "\t4 & GLM: Oversampling & 0.787 & 0.773 & 0.827 & 0.799 & 0.87 \\\\\n",
       "\t5 & GLM: L1 Reguralization & 0.79 & 0.78 & 0.822 & 0.8 & 0.867 \\\\\n",
       "\t6 & GLM: Subset Selection & 0.793 & 0.783 & 0.825 & 0.804 & 0.868 \\\\\n",
       "\t7 & Random Forest: Baseline & 0.804 & 0.661 & 0.523 & 0.584 & 0.853 \\\\\n",
       "\t8 & Random Forest: Oversampling & 0.865 & 0.856 & 0.885 & 0.87 & 0.942 \\\\\n",
       "\t9 & Random Forest: Hyperparameter Tuning & 0.865 & 0.847 & 0.899 & 0.873 & 0.94 \\\\\n",
       "\t10 & K-Nearest Neighbors: Baseline & 0.805 & 0.752 & 0.926 & 0.83 & 0.877 \\\\\n",
       "\t11 & K-Nearest Neighbors: Oversampling & 0.805 & 0.752 & 0.926 & 0.83 & 0.877 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m11×6 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m model                             \u001b[0m\u001b[1m accuracy \u001b[0m\u001b[1m precision \u001b[0m\u001b[1m recall  \u001b[0m\u001b[1m f1    \u001b[0m ⋯\n",
       "     │\u001b[90m String                            \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float6\u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │ GLM: Baseline                         0.817      0.67     0.597    0.63 ⋯\n",
       "   2 │ GLM: Optimal Threshold                0.8        0.602    0.699    0.64\n",
       "   3 │ GLM: Undersampling                    0.767      0.752    0.809    0.78\n",
       "   4 │ GLM: Oversampling                     0.787      0.773    0.827    0.79\n",
       "   5 │ GLM: L1 Reguralization                0.79       0.78     0.822    0.8  ⋯\n",
       "   6 │ GLM: Subset Selection                 0.793      0.783    0.825    0.80\n",
       "   7 │ Random Forest: Baseline               0.804      0.661    0.523    0.58\n",
       "   8 │ Random Forest: Oversampling           0.865      0.856    0.885    0.87\n",
       "   9 │ Random Forest: Hyperparameter Tu…     0.865      0.847    0.899    0.87 ⋯\n",
       "  10 │ K-Nearest Neighbors: Baseline         0.805      0.752    0.926    0.83\n",
       "  11 │ K-Nearest Neighbors: Oversampling     0.805      0.752    0.926    0.83\n",
       "\u001b[36m                                                               2 columns omitted\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = predict(model_knn_oversampling_fit, X_test)\n",
    "\n",
    "# Compute the metrics\n",
    "model_knn_oversampling_metrics = metrics(\n",
    "  \"K-Nearest Neighbors: Oversampling\",\n",
    "  predictions,\n",
    "  y_test,\n",
    "  0.5\n",
    ")\n",
    "\n",
    "# Store the metrics in the DataFrame\n",
    "push!(all_model_metrics, model_knn_oversampling_metrics[1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProbabilisticTunedModel(\n",
       "  model = KNNClassifier(\n",
       "        K = 5, \n",
       "        algorithm = :kdtree, \n",
       "        metric = Euclidean(0.0), \n",
       "        leafsize = 10, \n",
       "        reorder = true, \n",
       "        weights = Uniform()), \n",
       "  tuning = Grid(\n",
       "        goal = nothing, \n",
       "        resolution = 20, \n",
       "        shuffle = true, \n",
       "        rng = Random._GLOBAL_RNG()), \n",
       "  resampling = CV(\n",
       "        nfolds = 5, \n",
       "        shuffle = false, \n",
       "        rng = Random._GLOBAL_RNG()), \n",
       "  measure = Accuracy(), \n",
       "  weights = nothing, \n",
       "  class_weights = nothing, \n",
       "  operation = MLJModelInterface.predict_mode, \n",
       "  range = NumericRange{Int64, MLJBase.Bounded, Symbol}[NumericRange(1 ≤ K ≤ 20; origin=10.5, unit=9.5)], \n",
       "  selection_heuristic = NaiveSelection(nothing), \n",
       "  train_best = true, \n",
       "  repeats = 1, \n",
       "  n = 20, \n",
       "  acceleration = CPU1{Nothing}(nothing), \n",
       "  acceleration_resampling = CPU1{Nothing}(nothing), \n",
       "  check_measure = true, \n",
       "  cache = true)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split the data\n",
    "X_train, y_train, X_test, y_test = split_data(telco_balanced_oversampling)\n",
    "\n",
    "# Define the model\n",
    "model_knn = NearestNeighborModels.KNNClassifier()\n",
    "\n",
    "# We try to find the best k up to 20\n",
    "k_range = range(model_knn, :K, lower=1, upper=20)\n",
    "\n",
    "# Defining the grid\n",
    "tuning_strategy = Grid(resolution=20)\n",
    "\n",
    "# Define the tuning strategy\n",
    "model_knn_tuned = TunedModel(\n",
    "    model=model_knn,\n",
    "    tuning=tuning_strategy,\n",
    "    resampling=CV(nfolds=5),\n",
    "    ranges=[k_range],\n",
    "    measure=accuracy,\n",
    "    operation=predict_mode,\n",
    "    n=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training machine(ProbabilisticTunedModel(model = KNNClassifier(K = 5, …), …), …).\n",
      "└ @ MLJBase /Users/redx/.julia/packages/MLJBase/eCnWm/src/machines.jl:493\n",
      "┌ Info: Attempting to evaluate 20 models.\n",
      "└ @ MLJTuning /Users/redx/.julia/packages/MLJTuning/CLXum/src/tuned_models.jl:729\n",
      "\u001b[33mEvaluating over 20 metamodels: 100%[=========================] Time: 0:00:09\u001b[39m\u001b[K\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trained Machine; does not cache data\n",
       "  model: ProbabilisticTunedModel(model = KNNClassifier(K = 5, …), …)\n",
       "  args: \n",
       "    1:\tSource @619 ⏎ Table{AbstractVector{Continuous}}\n",
       "    2:\tSource @321 ⏎ AbstractVector{Multiclass{2}}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_knn_tuned_fit = fit!(machine(model_knn_tuned, X_train, y_train), verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the best k\n",
    "params = fitted_params(model_knn_tuned_fit)\n",
    "best_k = params.best_model.K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>12×6 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">model</th><th style = \"text-align: left;\">accuracy</th><th style = \"text-align: left;\">precision</th><th style = \"text-align: left;\">recall</th><th style = \"text-align: left;\">f1</th><th style = \"text-align: left;\">auc</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"String\" style = \"text-align: left;\">String</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: left;\">GLM: Baseline</td><td style = \"text-align: right;\">0.817</td><td style = \"text-align: right;\">0.67</td><td style = \"text-align: right;\">0.597</td><td style = \"text-align: right;\">0.632</td><td style = \"text-align: right;\">0.859</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: left;\">GLM: Optimal Threshold</td><td style = \"text-align: right;\">0.8</td><td style = \"text-align: right;\">0.602</td><td style = \"text-align: right;\">0.699</td><td style = \"text-align: right;\">0.647</td><td style = \"text-align: right;\">0.859</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: left;\">GLM: Undersampling</td><td style = \"text-align: right;\">0.767</td><td style = \"text-align: right;\">0.752</td><td style = \"text-align: right;\">0.809</td><td style = \"text-align: right;\">0.78</td><td style = \"text-align: right;\">0.847</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: left;\">GLM: Oversampling</td><td style = \"text-align: right;\">0.787</td><td style = \"text-align: right;\">0.773</td><td style = \"text-align: right;\">0.827</td><td style = \"text-align: right;\">0.799</td><td style = \"text-align: right;\">0.87</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: left;\">GLM: L1 Reguralization</td><td style = \"text-align: right;\">0.79</td><td style = \"text-align: right;\">0.78</td><td style = \"text-align: right;\">0.822</td><td style = \"text-align: right;\">0.8</td><td style = \"text-align: right;\">0.867</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: left;\">GLM: Subset Selection</td><td style = \"text-align: right;\">0.793</td><td style = \"text-align: right;\">0.783</td><td style = \"text-align: right;\">0.825</td><td style = \"text-align: right;\">0.804</td><td style = \"text-align: right;\">0.868</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: left;\">Random Forest: Baseline</td><td style = \"text-align: right;\">0.804</td><td style = \"text-align: right;\">0.661</td><td style = \"text-align: right;\">0.523</td><td style = \"text-align: right;\">0.584</td><td style = \"text-align: right;\">0.853</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: left;\">Random Forest: Oversampling</td><td style = \"text-align: right;\">0.865</td><td style = \"text-align: right;\">0.856</td><td style = \"text-align: right;\">0.885</td><td style = \"text-align: right;\">0.87</td><td style = \"text-align: right;\">0.942</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: left;\">Random Forest: Hyperparameter Tuning</td><td style = \"text-align: right;\">0.865</td><td style = \"text-align: right;\">0.847</td><td style = \"text-align: right;\">0.899</td><td style = \"text-align: right;\">0.873</td><td style = \"text-align: right;\">0.94</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">10</td><td style = \"text-align: left;\">K-Nearest Neighbors: Baseline</td><td style = \"text-align: right;\">0.805</td><td style = \"text-align: right;\">0.752</td><td style = \"text-align: right;\">0.926</td><td style = \"text-align: right;\">0.83</td><td style = \"text-align: right;\">0.877</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">11</td><td style = \"text-align: left;\">K-Nearest Neighbors: Oversampling</td><td style = \"text-align: right;\">0.805</td><td style = \"text-align: right;\">0.752</td><td style = \"text-align: right;\">0.926</td><td style = \"text-align: right;\">0.83</td><td style = \"text-align: right;\">0.877</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">12</td><td style = \"text-align: left;\">K-Nearest Neighbors: Hyperparameter Tuning</td><td style = \"text-align: right;\">0.848</td><td style = \"text-align: right;\">0.801</td><td style = \"text-align: right;\">0.937</td><td style = \"text-align: right;\">0.864</td><td style = \"text-align: right;\">0.845</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& model & accuracy & precision & recall & f1 & \\\\\n",
       "\t\\hline\n",
       "\t& String & Float64 & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & GLM: Baseline & 0.817 & 0.67 & 0.597 & 0.632 & $\\dots$ \\\\\n",
       "\t2 & GLM: Optimal Threshold & 0.8 & 0.602 & 0.699 & 0.647 & $\\dots$ \\\\\n",
       "\t3 & GLM: Undersampling & 0.767 & 0.752 & 0.809 & 0.78 & $\\dots$ \\\\\n",
       "\t4 & GLM: Oversampling & 0.787 & 0.773 & 0.827 & 0.799 & $\\dots$ \\\\\n",
       "\t5 & GLM: L1 Reguralization & 0.79 & 0.78 & 0.822 & 0.8 & $\\dots$ \\\\\n",
       "\t6 & GLM: Subset Selection & 0.793 & 0.783 & 0.825 & 0.804 & $\\dots$ \\\\\n",
       "\t7 & Random Forest: Baseline & 0.804 & 0.661 & 0.523 & 0.584 & $\\dots$ \\\\\n",
       "\t8 & Random Forest: Oversampling & 0.865 & 0.856 & 0.885 & 0.87 & $\\dots$ \\\\\n",
       "\t9 & Random Forest: Hyperparameter Tuning & 0.865 & 0.847 & 0.899 & 0.873 & $\\dots$ \\\\\n",
       "\t10 & K-Nearest Neighbors: Baseline & 0.805 & 0.752 & 0.926 & 0.83 & $\\dots$ \\\\\n",
       "\t11 & K-Nearest Neighbors: Oversampling & 0.805 & 0.752 & 0.926 & 0.83 & $\\dots$ \\\\\n",
       "\t12 & K-Nearest Neighbors: Hyperparameter Tuning & 0.848 & 0.801 & 0.937 & 0.864 & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m12×6 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m model                             \u001b[0m\u001b[1m accuracy \u001b[0m\u001b[1m precision \u001b[0m\u001b[1m recall  \u001b[0m\u001b[1m f1    \u001b[0m ⋯\n",
       "     │\u001b[90m String                            \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float6\u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │ GLM: Baseline                         0.817      0.67     0.597    0.63 ⋯\n",
       "   2 │ GLM: Optimal Threshold                0.8        0.602    0.699    0.64\n",
       "   3 │ GLM: Undersampling                    0.767      0.752    0.809    0.78\n",
       "   4 │ GLM: Oversampling                     0.787      0.773    0.827    0.79\n",
       "   5 │ GLM: L1 Reguralization                0.79       0.78     0.822    0.8  ⋯\n",
       "   6 │ GLM: Subset Selection                 0.793      0.783    0.825    0.80\n",
       "   7 │ Random Forest: Baseline               0.804      0.661    0.523    0.58\n",
       "   8 │ Random Forest: Oversampling           0.865      0.856    0.885    0.87\n",
       "   9 │ Random Forest: Hyperparameter Tu…     0.865      0.847    0.899    0.87 ⋯\n",
       "  10 │ K-Nearest Neighbors: Baseline         0.805      0.752    0.926    0.83\n",
       "  11 │ K-Nearest Neighbors: Oversampling     0.805      0.752    0.926    0.83\n",
       "  12 │ K-Nearest Neighbors: Hyperparame…     0.848      0.801    0.937    0.86\n",
       "\u001b[36m                                                               2 columns omitted\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = predict(model_knn_tuned_fit, X_test)\n",
    "\n",
    "# Compute the metrics\n",
    "model_knn_tuned_metrics = metrics(\n",
    "  \"K-Nearest Neighbors: Hyperparameter Tuning\",\n",
    "  predictions,\n",
    "  y_test,\n",
    "  0.5\n",
    ")\n",
    "\n",
    "# Store the metrics in the DataFrame\n",
    "push!(all_model_metrics, model_knn_tuned_metrics[1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Balancing Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trained Machine; caches model-specific representations of data\n",
       "  model: XGBoostClassifier(test = 1, …)\n",
       "  args: \n",
       "    1:\tSource @419 ⏎ Table{AbstractVector{Continuous}}\n",
       "    2:\tSource @344 ⏎ AbstractVector{Multiclass{2}}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split the data\n",
    "X_train, y_train, X_test, y_test = split_data(telco_balanced_oversampling)\n",
    "\n",
    "# Define the model\n",
    "model_xgb_balanced = XGBoostClassifier()\n",
    "\n",
    "# Fit the model\n",
    "model_xgb_balanced_fit = fit!(machine(model_xgb_balanced, X_train, y_train), verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28-element Vector{Pair{Symbol, Float32}}:\n",
       "                Symbol(\"contract__Month-to-month\") => 117.64085\n",
       "           Symbol(\"internet_service__Fiber optic\") => 22.227396\n",
       "                                   :dependents__No => 22.098532\n",
       "        Symbol(\"payment_method__Electronic check\") => 13.937189\n",
       "                                 :tech_support__No => 9.693799\n",
       "                      Symbol(\"contract__Two year\") => 8.707777\n",
       "                            :paperless_billing__No => 7.596537\n",
       "                      Symbol(\"contract__One year\") => 6.2016234\n",
       "                               :multiple_lines__No => 5.3117175\n",
       " Symbol(\"payment_method__Credit card (automatic)\") => 5.0322046\n",
       "                                                   ⋮\n",
       "                           :device_protection__Yes => 2.8750942\n",
       "                            :internet_service__DSL => 2.6524317\n",
       "                                  :monthly_charges => 2.5880418\n",
       "                                        :longitude => 2.1900978\n",
       "                                         :latitude => 2.1414237\n",
       "                                             :cltv => 2.0649722\n",
       "                                    :total_charges => 1.9267629\n",
       "                                :tech_support__Yes => 1.3679583\n",
       "                             :internet_service__No => 0.5494163"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_features = feature_importances(model_xgb_balanced_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>13×6 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">model</th><th style = \"text-align: left;\">accuracy</th><th style = \"text-align: left;\">precision</th><th style = \"text-align: left;\">recall</th><th style = \"text-align: left;\">f1</th><th style = \"text-align: left;\">auc</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"String\" style = \"text-align: left;\">String</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: left;\">GLM: Baseline</td><td style = \"text-align: right;\">0.817</td><td style = \"text-align: right;\">0.67</td><td style = \"text-align: right;\">0.597</td><td style = \"text-align: right;\">0.632</td><td style = \"text-align: right;\">0.859</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: left;\">GLM: Optimal Threshold</td><td style = \"text-align: right;\">0.8</td><td style = \"text-align: right;\">0.602</td><td style = \"text-align: right;\">0.699</td><td style = \"text-align: right;\">0.647</td><td style = \"text-align: right;\">0.859</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: left;\">GLM: Undersampling</td><td style = \"text-align: right;\">0.767</td><td style = \"text-align: right;\">0.752</td><td style = \"text-align: right;\">0.809</td><td style = \"text-align: right;\">0.78</td><td style = \"text-align: right;\">0.847</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: left;\">GLM: Oversampling</td><td style = \"text-align: right;\">0.787</td><td style = \"text-align: right;\">0.773</td><td style = \"text-align: right;\">0.827</td><td style = \"text-align: right;\">0.799</td><td style = \"text-align: right;\">0.87</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: left;\">GLM: L1 Reguralization</td><td style = \"text-align: right;\">0.79</td><td style = \"text-align: right;\">0.78</td><td style = \"text-align: right;\">0.822</td><td style = \"text-align: right;\">0.8</td><td style = \"text-align: right;\">0.867</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: left;\">GLM: Subset Selection</td><td style = \"text-align: right;\">0.793</td><td style = \"text-align: right;\">0.783</td><td style = \"text-align: right;\">0.825</td><td style = \"text-align: right;\">0.804</td><td style = \"text-align: right;\">0.868</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: left;\">Random Forest: Baseline</td><td style = \"text-align: right;\">0.804</td><td style = \"text-align: right;\">0.661</td><td style = \"text-align: right;\">0.523</td><td style = \"text-align: right;\">0.584</td><td style = \"text-align: right;\">0.853</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: left;\">Random Forest: Oversampling</td><td style = \"text-align: right;\">0.865</td><td style = \"text-align: right;\">0.856</td><td style = \"text-align: right;\">0.885</td><td style = \"text-align: right;\">0.87</td><td style = \"text-align: right;\">0.942</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: left;\">Random Forest: Hyperparameter Tuning</td><td style = \"text-align: right;\">0.865</td><td style = \"text-align: right;\">0.847</td><td style = \"text-align: right;\">0.899</td><td style = \"text-align: right;\">0.873</td><td style = \"text-align: right;\">0.94</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">10</td><td style = \"text-align: left;\">K-Nearest Neighbors: Baseline</td><td style = \"text-align: right;\">0.805</td><td style = \"text-align: right;\">0.752</td><td style = \"text-align: right;\">0.926</td><td style = \"text-align: right;\">0.83</td><td style = \"text-align: right;\">0.877</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">11</td><td style = \"text-align: left;\">K-Nearest Neighbors: Oversampling</td><td style = \"text-align: right;\">0.805</td><td style = \"text-align: right;\">0.752</td><td style = \"text-align: right;\">0.926</td><td style = \"text-align: right;\">0.83</td><td style = \"text-align: right;\">0.877</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">12</td><td style = \"text-align: left;\">K-Nearest Neighbors: Hyperparameter Tuning</td><td style = \"text-align: right;\">0.848</td><td style = \"text-align: right;\">0.801</td><td style = \"text-align: right;\">0.937</td><td style = \"text-align: right;\">0.864</td><td style = \"text-align: right;\">0.845</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">13</td><td style = \"text-align: left;\">XGBoost: Oversampling</td><td style = \"text-align: right;\">0.858</td><td style = \"text-align: right;\">0.855</td><td style = \"text-align: right;\">0.871</td><td style = \"text-align: right;\">0.863</td><td style = \"text-align: right;\">0.938</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& model & accuracy & precision & recall & f1 & \\\\\n",
       "\t\\hline\n",
       "\t& String & Float64 & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & GLM: Baseline & 0.817 & 0.67 & 0.597 & 0.632 & $\\dots$ \\\\\n",
       "\t2 & GLM: Optimal Threshold & 0.8 & 0.602 & 0.699 & 0.647 & $\\dots$ \\\\\n",
       "\t3 & GLM: Undersampling & 0.767 & 0.752 & 0.809 & 0.78 & $\\dots$ \\\\\n",
       "\t4 & GLM: Oversampling & 0.787 & 0.773 & 0.827 & 0.799 & $\\dots$ \\\\\n",
       "\t5 & GLM: L1 Reguralization & 0.79 & 0.78 & 0.822 & 0.8 & $\\dots$ \\\\\n",
       "\t6 & GLM: Subset Selection & 0.793 & 0.783 & 0.825 & 0.804 & $\\dots$ \\\\\n",
       "\t7 & Random Forest: Baseline & 0.804 & 0.661 & 0.523 & 0.584 & $\\dots$ \\\\\n",
       "\t8 & Random Forest: Oversampling & 0.865 & 0.856 & 0.885 & 0.87 & $\\dots$ \\\\\n",
       "\t9 & Random Forest: Hyperparameter Tuning & 0.865 & 0.847 & 0.899 & 0.873 & $\\dots$ \\\\\n",
       "\t10 & K-Nearest Neighbors: Baseline & 0.805 & 0.752 & 0.926 & 0.83 & $\\dots$ \\\\\n",
       "\t11 & K-Nearest Neighbors: Oversampling & 0.805 & 0.752 & 0.926 & 0.83 & $\\dots$ \\\\\n",
       "\t12 & K-Nearest Neighbors: Hyperparameter Tuning & 0.848 & 0.801 & 0.937 & 0.864 & $\\dots$ \\\\\n",
       "\t13 & XGBoost: Oversampling & 0.858 & 0.855 & 0.871 & 0.863 & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m13×6 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m model                             \u001b[0m\u001b[1m accuracy \u001b[0m\u001b[1m precision \u001b[0m\u001b[1m recall  \u001b[0m\u001b[1m f1    \u001b[0m ⋯\n",
       "     │\u001b[90m String                            \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float6\u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │ GLM: Baseline                         0.817      0.67     0.597    0.63 ⋯\n",
       "   2 │ GLM: Optimal Threshold                0.8        0.602    0.699    0.64\n",
       "   3 │ GLM: Undersampling                    0.767      0.752    0.809    0.78\n",
       "   4 │ GLM: Oversampling                     0.787      0.773    0.827    0.79\n",
       "   5 │ GLM: L1 Reguralization                0.79       0.78     0.822    0.8  ⋯\n",
       "   6 │ GLM: Subset Selection                 0.793      0.783    0.825    0.80\n",
       "   7 │ Random Forest: Baseline               0.804      0.661    0.523    0.58\n",
       "   8 │ Random Forest: Oversampling           0.865      0.856    0.885    0.87\n",
       "   9 │ Random Forest: Hyperparameter Tu…     0.865      0.847    0.899    0.87 ⋯\n",
       "  10 │ K-Nearest Neighbors: Baseline         0.805      0.752    0.926    0.83\n",
       "  11 │ K-Nearest Neighbors: Oversampling     0.805      0.752    0.926    0.83\n",
       "  12 │ K-Nearest Neighbors: Hyperparame…     0.848      0.801    0.937    0.86\n",
       "  13 │ XGBoost: Oversampling                 0.858      0.855    0.871    0.86 ⋯\n",
       "\u001b[36m                                                               2 columns omitted\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = predict(model_xgb_balanced_fit, X_test)\n",
    "\n",
    "# Compute the metrics\n",
    "model_xgb_balanced_metrics = metrics(\n",
    "  \"XGBoost: Oversampling\",\n",
    "  predictions,\n",
    "  y_test,\n",
    "  0.5\n",
    ")\n",
    "\n",
    "# Store the metrics in the DataFrame\n",
    "push!(all_model_metrics, model_xgb_balanced_metrics[1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Hypertuning Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProbabilisticTunedModel(\n",
       "  model = XGBoostClassifier(\n",
       "        test = 1, \n",
       "        num_round = 100, \n",
       "        booster = \"gbtree\", \n",
       "        disable_default_eval_metric = 0, \n",
       "        eta = 0.3, \n",
       "        num_parallel_tree = 1, \n",
       "        gamma = 0.0, \n",
       "        max_depth = 6, \n",
       "        min_child_weight = 1.0, \n",
       "        max_delta_step = 0.0, \n",
       "        subsample = 1.0, \n",
       "        colsample_bytree = 1.0, \n",
       "        colsample_bylevel = 1.0, \n",
       "        colsample_bynode = 1.0, \n",
       "        lambda = 1.0, \n",
       "        alpha = 0.0, \n",
       "        tree_method = \"auto\", \n",
       "        sketch_eps = 0.03, \n",
       "        scale_pos_weight = 1.0, \n",
       "        updater = nothing, \n",
       "        refresh_leaf = 1, \n",
       "        process_type = \"default\", \n",
       "        grow_policy = \"depthwise\", \n",
       "        max_leaves = 0, \n",
       "        max_bin = 256, \n",
       "        predictor = \"cpu_predictor\", \n",
       "        sample_type = \"uniform\", \n",
       "        normalize_type = \"tree\", \n",
       "        rate_drop = 0.0, \n",
       "        one_drop = 0, \n",
       "        skip_drop = 0.0, \n",
       "        feature_selector = \"cyclic\", \n",
       "        top_k = 0, \n",
       "        tweedie_variance_power = 1.5, \n",
       "        objective = \"automatic\", \n",
       "        base_score = 0.5, \n",
       "        watchlist = nothing, \n",
       "        nthread = 1, \n",
       "        importance_type = \"gain\", \n",
       "        seed = nothing, \n",
       "        validate_parameters = false, \n",
       "        eval_metric = String[]), \n",
       "  tuning = RandomSearch(\n",
       "        bounded = Distributions.Uniform, \n",
       "        positive_unbounded = Distributions.Gamma, \n",
       "        other = Distributions.Normal, \n",
       "        rng = Random._GLOBAL_RNG()), \n",
       "  resampling = CV(\n",
       "        nfolds = 5, \n",
       "        shuffle = false, \n",
       "        rng = Random._GLOBAL_RNG()), \n",
       "  measure = Accuracy(), \n",
       "  weights = nothing, \n",
       "  class_weights = nothing, \n",
       "  operation = MLJModelInterface.predict_mode, \n",
       "  range = NumericRange{T, MLJBase.Bounded, Symbol} where T[NumericRange(0.01 ≤ eta ≤ 0.3; origin=0.155, unit=0.145), NumericRange(3 ≤ max_depth ≤ 10; origin=6.5, unit=3.5)], \n",
       "  selection_heuristic = NaiveSelection(nothing), \n",
       "  train_best = true, \n",
       "  repeats = 1, \n",
       "  n = 20, \n",
       "  acceleration = CPU1{Nothing}(nothing), \n",
       "  acceleration_resampling = CPU1{Nothing}(nothing), \n",
       "  check_measure = true, \n",
       "  cache = true)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split the data\n",
    "X_train, y_train, X_test, y_test = split_data(telco_balanced_oversampling)\n",
    "\n",
    "# Define the model\n",
    "model_xgb = XGBoostClassifier()\n",
    "\n",
    "# Define Ranges\n",
    "eta_range = range(model_xgb, :eta, lower=0.01, upper=0.3)\n",
    "max_depth_range = range(model_xgb, :max_depth, lower=3, upper=10)\n",
    "\n",
    "model_xgb_tuned = TunedModel(\n",
    "    model=model_xgb,\n",
    "    resampling=CV(nfolds=5),\n",
    "    ranges=[eta_range, max_depth_range],\n",
    "    measure=accuracy,\n",
    "    operation=predict_mode,\n",
    "    n=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trained Machine; does not cache data\n",
       "  model: ProbabilisticTunedModel(model = XGBoostClassifier(test = 1, …), …)\n",
       "  args: \n",
       "    1:\tSource @938 ⏎ Table{AbstractVector{Continuous}}\n",
       "    2:\tSource @749 ⏎ AbstractVector{Multiclass{2}}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_xgb_tuned_fit = fit!(machine(model_xgb_tuned, X_train, y_train), verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(best_model = XGBoostClassifier(test = 1, …),\n",
       " best_fitted_params = (fitresult = (XGBoost.Booster(), CategoricalValue{Int64, UInt32} 1),),)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the best parameters\n",
    "params = fitted_params(model_xgb_tuned_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ETA: 0.24753436556032415Best MAX DEPTH: 8\n"
     ]
    }
   ],
   "source": [
    "best_eta = params.best_model.eta\n",
    "best_max_depth = params.best_model.max_depth\n",
    "\n",
    "println(\"Best ETA: \", best_eta, \"Best MAX DEPTH: \", best_max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28-element Vector{Pair{Symbol, Float32}}:\n",
       "                Symbol(\"contract__Month-to-month\") => 117.64085\n",
       "           Symbol(\"internet_service__Fiber optic\") => 22.227396\n",
       "                                   :dependents__No => 22.098532\n",
       "        Symbol(\"payment_method__Electronic check\") => 13.937189\n",
       "                                 :tech_support__No => 9.693799\n",
       "                      Symbol(\"contract__Two year\") => 8.707777\n",
       "                            :paperless_billing__No => 7.596537\n",
       "                      Symbol(\"contract__One year\") => 6.2016234\n",
       "                               :multiple_lines__No => 5.3117175\n",
       " Symbol(\"payment_method__Credit card (automatic)\") => 5.0322046\n",
       "                                                   ⋮\n",
       "                           :device_protection__Yes => 2.8750942\n",
       "                            :internet_service__DSL => 2.6524317\n",
       "                                  :monthly_charges => 2.5880418\n",
       "                                        :longitude => 2.1900978\n",
       "                                         :latitude => 2.1414237\n",
       "                                             :cltv => 2.0649722\n",
       "                                    :total_charges => 1.9267629\n",
       "                                :tech_support__Yes => 1.3679583\n",
       "                             :internet_service__No => 0.5494163"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_features = feature_importances(model_xgb_balanced_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>14×6 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">model</th><th style = \"text-align: left;\">accuracy</th><th style = \"text-align: left;\">precision</th><th style = \"text-align: left;\">recall</th><th style = \"text-align: left;\">f1</th><th style = \"text-align: left;\">auc</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"String\" style = \"text-align: left;\">String</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: left;\">GLM: Baseline</td><td style = \"text-align: right;\">0.817</td><td style = \"text-align: right;\">0.67</td><td style = \"text-align: right;\">0.597</td><td style = \"text-align: right;\">0.632</td><td style = \"text-align: right;\">0.859</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: left;\">GLM: Optimal Threshold</td><td style = \"text-align: right;\">0.8</td><td style = \"text-align: right;\">0.602</td><td style = \"text-align: right;\">0.699</td><td style = \"text-align: right;\">0.647</td><td style = \"text-align: right;\">0.859</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: left;\">GLM: Undersampling</td><td style = \"text-align: right;\">0.767</td><td style = \"text-align: right;\">0.752</td><td style = \"text-align: right;\">0.809</td><td style = \"text-align: right;\">0.78</td><td style = \"text-align: right;\">0.847</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: left;\">GLM: Oversampling</td><td style = \"text-align: right;\">0.787</td><td style = \"text-align: right;\">0.773</td><td style = \"text-align: right;\">0.827</td><td style = \"text-align: right;\">0.799</td><td style = \"text-align: right;\">0.87</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: left;\">GLM: L1 Reguralization</td><td style = \"text-align: right;\">0.79</td><td style = \"text-align: right;\">0.78</td><td style = \"text-align: right;\">0.822</td><td style = \"text-align: right;\">0.8</td><td style = \"text-align: right;\">0.867</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: left;\">GLM: Subset Selection</td><td style = \"text-align: right;\">0.793</td><td style = \"text-align: right;\">0.783</td><td style = \"text-align: right;\">0.825</td><td style = \"text-align: right;\">0.804</td><td style = \"text-align: right;\">0.868</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: left;\">Random Forest: Baseline</td><td style = \"text-align: right;\">0.804</td><td style = \"text-align: right;\">0.661</td><td style = \"text-align: right;\">0.523</td><td style = \"text-align: right;\">0.584</td><td style = \"text-align: right;\">0.853</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: left;\">Random Forest: Oversampling</td><td style = \"text-align: right;\">0.865</td><td style = \"text-align: right;\">0.856</td><td style = \"text-align: right;\">0.885</td><td style = \"text-align: right;\">0.87</td><td style = \"text-align: right;\">0.942</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: left;\">Random Forest: Hyperparameter Tuning</td><td style = \"text-align: right;\">0.865</td><td style = \"text-align: right;\">0.847</td><td style = \"text-align: right;\">0.899</td><td style = \"text-align: right;\">0.873</td><td style = \"text-align: right;\">0.94</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">10</td><td style = \"text-align: left;\">K-Nearest Neighbors: Baseline</td><td style = \"text-align: right;\">0.805</td><td style = \"text-align: right;\">0.752</td><td style = \"text-align: right;\">0.926</td><td style = \"text-align: right;\">0.83</td><td style = \"text-align: right;\">0.877</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">11</td><td style = \"text-align: left;\">K-Nearest Neighbors: Oversampling</td><td style = \"text-align: right;\">0.805</td><td style = \"text-align: right;\">0.752</td><td style = \"text-align: right;\">0.926</td><td style = \"text-align: right;\">0.83</td><td style = \"text-align: right;\">0.877</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">12</td><td style = \"text-align: left;\">K-Nearest Neighbors: Hyperparameter Tuning</td><td style = \"text-align: right;\">0.848</td><td style = \"text-align: right;\">0.801</td><td style = \"text-align: right;\">0.937</td><td style = \"text-align: right;\">0.864</td><td style = \"text-align: right;\">0.845</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">13</td><td style = \"text-align: left;\">XGBoost: Oversampling</td><td style = \"text-align: right;\">0.858</td><td style = \"text-align: right;\">0.855</td><td style = \"text-align: right;\">0.871</td><td style = \"text-align: right;\">0.863</td><td style = \"text-align: right;\">0.938</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">14</td><td style = \"text-align: left;\">XGBoost: Hyperparameter Tuning</td><td style = \"text-align: right;\">0.859</td><td style = \"text-align: right;\">0.856</td><td style = \"text-align: right;\">0.871</td><td style = \"text-align: right;\">0.864</td><td style = \"text-align: right;\">0.943</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& model & accuracy & precision & recall & f1 & \\\\\n",
       "\t\\hline\n",
       "\t& String & Float64 & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & GLM: Baseline & 0.817 & 0.67 & 0.597 & 0.632 & $\\dots$ \\\\\n",
       "\t2 & GLM: Optimal Threshold & 0.8 & 0.602 & 0.699 & 0.647 & $\\dots$ \\\\\n",
       "\t3 & GLM: Undersampling & 0.767 & 0.752 & 0.809 & 0.78 & $\\dots$ \\\\\n",
       "\t4 & GLM: Oversampling & 0.787 & 0.773 & 0.827 & 0.799 & $\\dots$ \\\\\n",
       "\t5 & GLM: L1 Reguralization & 0.79 & 0.78 & 0.822 & 0.8 & $\\dots$ \\\\\n",
       "\t6 & GLM: Subset Selection & 0.793 & 0.783 & 0.825 & 0.804 & $\\dots$ \\\\\n",
       "\t7 & Random Forest: Baseline & 0.804 & 0.661 & 0.523 & 0.584 & $\\dots$ \\\\\n",
       "\t8 & Random Forest: Oversampling & 0.865 & 0.856 & 0.885 & 0.87 & $\\dots$ \\\\\n",
       "\t9 & Random Forest: Hyperparameter Tuning & 0.865 & 0.847 & 0.899 & 0.873 & $\\dots$ \\\\\n",
       "\t10 & K-Nearest Neighbors: Baseline & 0.805 & 0.752 & 0.926 & 0.83 & $\\dots$ \\\\\n",
       "\t11 & K-Nearest Neighbors: Oversampling & 0.805 & 0.752 & 0.926 & 0.83 & $\\dots$ \\\\\n",
       "\t12 & K-Nearest Neighbors: Hyperparameter Tuning & 0.848 & 0.801 & 0.937 & 0.864 & $\\dots$ \\\\\n",
       "\t13 & XGBoost: Oversampling & 0.858 & 0.855 & 0.871 & 0.863 & $\\dots$ \\\\\n",
       "\t14 & XGBoost: Hyperparameter Tuning & 0.859 & 0.856 & 0.871 & 0.864 & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m14×6 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m model                             \u001b[0m\u001b[1m accuracy \u001b[0m\u001b[1m precision \u001b[0m\u001b[1m recall  \u001b[0m\u001b[1m f1    \u001b[0m ⋯\n",
       "     │\u001b[90m String                            \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float6\u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │ GLM: Baseline                         0.817      0.67     0.597    0.63 ⋯\n",
       "   2 │ GLM: Optimal Threshold                0.8        0.602    0.699    0.64\n",
       "   3 │ GLM: Undersampling                    0.767      0.752    0.809    0.78\n",
       "   4 │ GLM: Oversampling                     0.787      0.773    0.827    0.79\n",
       "   5 │ GLM: L1 Reguralization                0.79       0.78     0.822    0.8  ⋯\n",
       "   6 │ GLM: Subset Selection                 0.793      0.783    0.825    0.80\n",
       "   7 │ Random Forest: Baseline               0.804      0.661    0.523    0.58\n",
       "   8 │ Random Forest: Oversampling           0.865      0.856    0.885    0.87\n",
       "   9 │ Random Forest: Hyperparameter Tu…     0.865      0.847    0.899    0.87 ⋯\n",
       "  10 │ K-Nearest Neighbors: Baseline         0.805      0.752    0.926    0.83\n",
       "  11 │ K-Nearest Neighbors: Oversampling     0.805      0.752    0.926    0.83\n",
       "  12 │ K-Nearest Neighbors: Hyperparame…     0.848      0.801    0.937    0.86\n",
       "  13 │ XGBoost: Oversampling                 0.858      0.855    0.871    0.86 ⋯\n",
       "  14 │ XGBoost: Hyperparameter Tuning        0.859      0.856    0.871    0.86\n",
       "\u001b[36m                                                               2 columns omitted\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = predict(model_xgb_tuned_fit, X_test)\n",
    "\n",
    "# Compute the metrics\n",
    "model_xgb_tuned_metrics = metrics(\n",
    "  \"XGBoost: Hyperparameter Tuning\",\n",
    "  predictions,\n",
    "  y_test,\n",
    "  0.5\n",
    ")\n",
    "\n",
    "# Store the metrics in the DataFrame\n",
    "push!(all_model_metrics, model_xgb_tuned_metrics[1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────────────────────────────────────────┬────────────┬────────────┬────────────┬────────────┬────────────┐\n",
      "│\u001b[1m model                                      \u001b[0m│\u001b[1m accuracy   \u001b[0m│\u001b[1m precision  \u001b[0m│\u001b[1m recall     \u001b[0m│\u001b[1m f1         \u001b[0m│\u001b[1m auc        \u001b[0m│\n",
      "│\u001b[90m String                                     \u001b[0m│\u001b[90m Float64    \u001b[0m│\u001b[90m Float64    \u001b[0m│\u001b[90m Float64    \u001b[0m│\u001b[90m Float64    \u001b[0m│\u001b[90m Float64    \u001b[0m│\n",
      "│\u001b[90m Textual                                    \u001b[0m│\u001b[90m Continuous \u001b[0m│\u001b[90m Continuous \u001b[0m│\u001b[90m Continuous \u001b[0m│\u001b[90m Continuous \u001b[0m│\u001b[90m Continuous \u001b[0m│\n",
      "├────────────────────────────────────────────┼────────────┼────────────┼────────────┼────────────┼────────────┤\n",
      "│ GLM: Baseline                              │ 0.817      │ 0.67       │ 0.597      │ 0.632      │ 0.859      │\n",
      "│ GLM: Optimal Threshold                     │ 0.8        │ 0.602      │ 0.699      │ 0.647      │ 0.859      │\n",
      "│ GLM: Undersampling                         │ 0.767      │ 0.752      │ 0.809      │ 0.78       │ 0.847      │\n",
      "│ GLM: Oversampling                          │ 0.787      │ 0.773      │ 0.827      │ 0.799      │ 0.87       │\n",
      "│ GLM: L1 Reguralization                     │ 0.79       │ 0.78       │ 0.822      │ 0.8        │ 0.867      │\n",
      "│ GLM: Subset Selection                      │ 0.793      │ 0.783      │ 0.825      │ 0.804      │ 0.868      │\n",
      "│ Random Forest: Baseline                    │ 0.804      │ 0.661      │ 0.523      │ 0.584      │ 0.853      │\n",
      "│ Random Forest: Oversampling                │ 0.865      │ 0.856      │ 0.885      │ 0.87       │ 0.942      │\n",
      "│ Random Forest: Hyperparameter Tuning       │ 0.865      │ 0.847      │ 0.899      │ 0.873      │ 0.94       │\n",
      "│ K-Nearest Neighbors: Baseline              │ 0.805      │ 0.752      │ 0.926      │ 0.83       │ 0.877      │\n",
      "│ K-Nearest Neighbors: Oversampling          │ 0.805      │ 0.752      │ 0.926      │ 0.83       │ 0.877      │\n",
      "│ K-Nearest Neighbors: Hyperparameter Tuning │ 0.848      │ 0.801      │ 0.937      │ 0.864      │ 0.845      │\n",
      "│ XGBoost: Oversampling                      │ 0.858      │ 0.855      │ 0.871      │ 0.863      │ 0.938      │\n",
      "│ XGBoost: Hyperparameter Tuning             │ 0.859      │ 0.856      │ 0.871      │ 0.864      │ 0.943      │\n",
      "└────────────────────────────────────────────┴────────────┴────────────┴────────────┴────────────┴────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "all_model_metrics |> pretty"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.0",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
